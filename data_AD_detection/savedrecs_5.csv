"Key","Item Type","Publication Year","Author","Title","Publication Title","ISBN","ISSN","DOI","Url","Abstract Note","Date","Date Added","Date Modified","Access Date","Pages","Num Pages","Issue","Volume","Number Of Volumes","Journal Abbreviation","Short Title","Series","Series Number","Series Text","Series Title","Publisher","Place","Language","Rights","Type","Archive","Archive Location","Library Catalog","Call Number","Extra","Notes","File Attachments","Link Attachments","Manual Tags","Automatic Tags","Editor","Series Editor","Translator","Contributor","Attorney Agent","Book Author","Cast Member","Commenter","Composer","Cosponsor","Counsel","Interviewer","Producer","Recipient","Reviewed Author","Scriptwriter","Words By","Guest","Number","Edition","Running Time","Scale","Medium","Artwork Size","Filing Date","Application Number","Assignee","Issuing Authority","Country","Meeting Name","Conference Name","Court","References","Reporter","Legal Status","Priority Numbers","Programming Language","Version","System","Code","Code Number","Section","Session","Committee","History","Legislative Body"
"IDPNX265","journalArticle","2023","Pfützner, H; Shilyashki, G; Bengtsson, C; Huber, E","Practical Aspects of Instantaneous Magnetization Power Functions of Silicon Iron Laminations","JOURNAL OF ELECTRICAL ENGINEERING & TECHNOLOGY","","1975-0102","10.1007/s42835-022-01265-2","","Magnetic energy loss P of SiFe steel represents a key factor for the efficiency of soft magnetic machine cores. Traditionally, they are operated with 50 Hz (or 60 Hz), a frequency value that yields rather balanced portions of hysteresis loss and eddy current loss. In equivalent circuits of transformers, P tends to be represented by a magnetic power resistance R-M, as a constant. For the most important case of sinusoidal induction B of 50 Hz, this would correspond to an instantaneous magnetization power function p(t) that is sinusoidal as well, however, with 100 Hz (or 120 Hz). On the other hand, from complex, non-linear mechanisms of hysteresis, it is obvious that p(t) should be strongly non-sinusoidal, even for exactly sinusoidal B(t). So far, almost all corresponding instantaneous investigations were restricted to calculated modelling of loss portions and transient modelling. On the other hand, for the first time, the present study was focussed on functions p(t) as measured at IEC-standardized samples of industrially relevant steel. Practical evaluations are discussed with respect to the revealed ""history"" of magnetization processes, as well as for product characterization. For these tasks, a novel digitized ""Low-mass Single Sheet Tester"" was developed that was applied for both non-oriented steel (NO) and grain-oriented steel (GO), for 50 Hz. Interpretations proved to be favoured by relating p(t) to total P, according to an instantaneous power ratio. As a result, both steel types revealed strongly non-sinusoidal power functions, with short durations of negative p. Negative p proved to be most pronounced for NO steel, as a measure for the onset of reversible turns of atomic moments. As a consequence, p(t) comprises strong upper harmonics of 200 Hz and even 300 Hz. Based on theoretical considerations, we split p(t) in a dissipative loss power function p(L)(t) and in a potential energy power function p(P)(t). Finally, we used p(t) to determine the corresponding power resistance R-M(t) that proves to be a distinctly nonlinear function as well. It resembles a rectified co-sinus, also exhibiting short negative spikes that reflect the crystallographic dis-orientation of the polycrystalline material.","2023-03","2025-02-26 20:45:05","2025-02-26 20:45:05","","1273-1282","","2","18","","","","","","","","","","English","","","","WOS:000868965600002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;30</p>","","","Eddy currents; Equivalent circuits; Hysteresis; Instantaneous magnetization power; Loss resistance; Silicon iron; TRANSFORMER MODEL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RXJ8EP4J","journalArticle","2025","Benarfa, G; Amamou, A; Kelouwani, S; Hébert, M; Zeghmi, L; Jemei, S","Online health-aware energy management strategy of a fuel cell hybrid autonomous mobile robot under startup-shutdown condition","EXPERT SYSTEMS WITH APPLICATIONS","","0957-4174","10.1016/j.eswa.2024.125943","","In the age of Industry 4.0, the automation of industrial processes is essential for enhancing efficiency, productivity, and flexibility. Autonomous mobile robots are pivotal in this transformation, particularly in material handling and logistics operations within complex industrial environments. Fuel cell hybrid autonomous mobile robots, a type of autonomous mobile robot that functions with hybridization of battery and fuel cell, offer significant advantages in operational efficiency and sustainability. However, the commercialization of these vehicles is impeded by the limited lifespan of fuel cells and the adverse effects of frequent startup-shutdown cycles, which lead to significant fuel cell degradation and reduced operational efficiency. This study addresses these challenges by presenting an innovative, health-aware energy management strategy tailored for fuel cell hybrid autonomous mobile robots. The proposed strategy aims to balance hydrogen consumption with fuel cell degradation through a comprehensive two-step approach. First, the offline module employs digital modeling combined with a Markov Decision Process to generate long-term power profiles. This step includes the use of Dynamic Programming to optimize power distribution, ensuring an efficient energy management strategy. Additionally, a transformer neural network is trained on this optimized data to accurately predict the fuel cell's power output. In the online step, a Model Predictive Control technique is utilized to dynamically track the fuel cell's power output based on real-time predictions from the trained transformer model. This enables the system to adapt to changing operational conditions, maintaining optimal performance and extending the fuel cell's lifespan. Our comparative analysis, based on simulations and experimental tests conducted in a controlled laboratory environment, demonstrates that this approach enhances both fuel cell lifespan and hydrogen efficiency. Specifically, our strategy extends the fuel cell's operational life by 9.5% and achieves a hydrogen consumption of 15.83 g over a 600-s operational cycle, compared to benchmark methods. The novelty of this research lies in its integration of advanced predictive models and control techniques, which collectively optimize the operational efficiency and durability of fuel cell hybrid autonomous mobile robots.","2025-03-25","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","266","","","","","","","","","","English","","","","WOS:001386242000001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;66</p>","","","Autonomous mobile robot; COST; DEGRADATION; Digital modeling; Energy management strategy; Industry 4.0; LOAD; Model predictive control; PREDICTION; Transformer neural network; VEHICLES","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"78Z3RM62","journalArticle","2022","Masubuchi, Y; Hashimoto, M; Otsuka, A","SIBYL: A Method for Detecting Similar Binary Functions Using Machine Learning","IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS","","0916-8532","10.1587/transinf.2021EDP7135","","Binary code similarity comparison methods are mainly used to find bugs in software, to detect software plagiarism, and to reduce the workload during malware analysis. In this paper, we propose a method to compare the binary code similarity of each function by using a combination of Control Flow Graphs (CFGs) and disassembled instruction sequences contained in each function, and to detect a function with high similarity to a specified function. One of the challenges in performing similarity comparisons is that different compile-time optimizations and different architectures produce different binary code. The main units for comparing code are instructions, basic blocks and functions. The challenge of functions is that they have a graph structure in which basic blocks are combined, making it relatively difficult to derive similarity. However, analysis tools such as IDA, display the disassembled instruction sequence in function units. Detecting similarity on a function basis has the advantage of facilitating simplified understanding by analysts. To solve the aforementioned challenges, we use machine learning methods in the field of natural language processing. In this field, there is a Transformer model, as of 2017, that updates each record for various language processing tasks, and as of 2021, Transformer is the basis for BERT, which updates each record for language processing tasks. There is also a method called node2vec, which uses machine learning techniques to capture the features of each node from the graph structure. In this paper, we propose SIBYL, a combination of Transformer and node2vec. In SIBYL, a method called Triplet-Loss is used during learning so that similar items are brought closer and dissimilar items are moved away. To evaluate SIBYL, we created a new dataset using open-source software widely used in the real world, and conducted training and evaluation experiments using the dataset. In the evaluation experiments, we evaluated the similarity of binary codes across different architectures using evaluation indices such as Rankl and MRR. The experimental results showed that SIBYL outperforms existing research. We believe that this is due to the fact that machine learning has been able to capture the features of the graph structure and the order of instructions on a function-by-function basis. The results of these experiments are presented in detail, followed by a discussion and conclusion.","2022-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","755-765","","4","E105D","","","","","","","","","","English","","","","WOS:000790987300002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;31</p>","","","binary code; function; machine learning; NLP; similarity","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"9BJKHGD4","journalArticle","2024","Buehler, MJ","Generative Retrieval-Augmented Ontologic Graph and Multiagent Strategies for Interpretive Large Language Model-Based Materials Design","ACS ENGINEERING AU","","2694-2488","10.1021/acsengineeringau.3c00058","","Transformer neural networks show promising capabilities, in particular for uses in materials analysis, design, and manufacturing, including their capacity to work effectively with human language, symbols, code, and numerical data. Here, we explore the use of large language models (LLMs) as a tool that can support engineering analysis of materials, applied to retrieving key information about subject areas, developing research hypotheses, discovery of mechanistic relationships across disparate areas of knowledge, and writing and executing simulation codes for active knowledge generation based on physical ground truths. Moreover, when used as sets of AI agents with specific features, capabilities, and instructions, LLMs can provide powerful problem-solution strategies for applications in analysis and design problems. Our experiments focus on using a fine-tuned model, MechGPT, developed based on training data in the mechanics of materials domain. We first affirm how fine-tuning endows LLMs with a reasonable understanding of subject area knowledge. However, when queried outside the context of learned matter, LLMs can have difficulty recalling correct information and may hallucinate. We show how this can be addressed using retrieval-augmented Ontological Knowledge Graph strategies. The graph-based strategy helps us not only to discern how the model understands what concepts are important but also how they are related, which significantly improves generative performance and also naturally allows for injection of new and augmented data sources into generative AI algorithms. We find that the additional feature of relatedness provides advantages over regular retrieval augmentation approaches and not only improves LLM performance but also provides mechanistic insights for exploration of a material design process. Illustrated for a use case of relating distinct areas of knowledge, here, music and proteins, such strategies can also provide an interpretable graph structure with rich information at the node, edge, and subgraph level that provides specific insights into mechanisms and relationships. We discuss other approaches to improve generative qualities, including nonlinear sampling strategies and agent-based modeling that offer enhancements over single-shot generations, whereby LLMs are used to both generate content and assess content against an objective target. Examples provided include complex question answering, code generation, and execution in the context of automated force-field development from actively learned density functional theory (DFT) modeling and data analysis.","2024-01-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","241-277","","2","4","","","","","","","","","","English","","","","WOS:001162334400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;11<br/>Total Times Cited:&nbsp;&nbsp;12<br/>Cited Reference Count:&nbsp;&nbsp;107</p>","","","biomaterials; FRACTURE; generative autoregressivetransformer; hierarchical; HYPERELASTICITY; language models; materials by design; multiscale modeling; TRANSFORMER MODEL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WHLE7UCT","journalArticle","2025","Randazzo, V; Caligari, S; Pasero, E; Giustetto, C; Saglietto, A; Bertarello, W; Averbuch, A; Marcus-Kalish, M; Zheludev, V; Gaita, F","A Vision Transformer Model for the Prediction of Fatal Arrhythmic Events in Patients with Brugada Syndrome","SENSORS","","1424-8220","10.3390/s25030824","","Brugada syndrome (BrS) is an inherited electrical cardiac disorder that is associated with a higher risk of ventricular fibrillation (VF) and sudden cardiac death (SCD) in patients without structural heart disease. The diagnosis is based on the documentation of the typical pattern in the electrocardiogram (ECG) characterized by a J-point elevation of >= 2 mm, coved-type ST-segment elevation, and negative T wave in one or more right precordial leads, called type 1 Brugada ECG. Risk stratification is particularly difficult in asymptomatic cases. Patients who have experienced documented VF are generally recommended to receive an implantable cardioverter defibrillator to lower the likelihood of sudden death due to recurrent episodes. However, for asymptomatic individuals, the most appropriate course of action remains uncertain. Accurate risk prediction is critical to avoiding premature deaths and unnecessary treatments. Due to the challenges associated with experimental research on human cardiac tissue, alternative techniques such as computational modeling and deep learning-based artificial intelligence (AI) are becoming increasingly important. This study introduces a vision transformer (ViT) model that leverages 12-lead ECG images to predict potentially fatal arrhythmic events in BrS patients. This dataset includes a total of 278 ECGs, belonging to 210 patients which have been diagnosed with Brugada syndrome, and it is split into two classes: event and no event. The event class contains 94 ECGs of patients with documented ventricular tachycardia, ventricular fibrillation, or sudden cardiac death, while the no event class is composed of 184 ECGs used as the control group. At first, the ViT is trained on a balanced dataset, achieving satisfactory results (89% accuracy, 94% specificity, 84% sensitivity, and 89% F1-score). Then, the discarded no event ECGs are attached to additional 30 event ECGs, extracted by a 24 h recording of a singular individual, composing a new test set. Finally, the use of an optimized classification threshold improves the predictions on an unbalanced set of data (74% accuracy, 95% negative predictive value, and 90% sensitivity), suggesting that the ECG signal can reveal key information for the risk stratification of patients with Brugada syndrome.","2025-02","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","3","25","","","","","","","","","","English","","","","WOS:001419421000001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;55</p>","","","Brugada syndrome; CONSENSUS CONFERENCE; deep learning; DIAGNOSIS; electrocardiogram; GAPS; PATTERN; PROGNOSIS; REPORT EMERGING CONCEPTS; risk stratification; RISK STRATIFICATION; sudden cardiac death; SUDDEN CARDIAC DEATH; vision transformer; WAVE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"NLM2F7GI","journalArticle","2024","Ma, HY; Ma, XB; Yang, CX; Niu, Q; Gao, T; Liu, CX; Chen, Y","Development and evaluation of a program based on a generative pre-trained transformer model from a public natural language processing platform for efficiency enhancement in post-procedural quality control of esophageal endoscopic submucosal dissection","SURGICAL ENDOSCOPY AND OTHER INTERVENTIONAL TECHNIQUES","","0930-2794","10.1007/s00464-023-10620-x","","Background Post-procedural quality control of endoscopic submucosal dissection (ESD) is emphasized in guidelines. However, this process can be tedious and time-consuming. Recently, a pre-training model called generative pre-trained transformer (GPT) on a public natural language processing platform has emerged and garnered significant attention, whose capabilities align well with the post-procedural quality control process and have the potential to streamline it. Therefore, we developed a simple program utilizing this platform and evaluated its performance. Methods Esophageal ESDs were retrospectively included. The manual quality control process was performed and act as reference standard. GPT's prompt was optimized through multiple iterations. A Python program was developed to automatically submit prompt with pathological report of each ESD procedure and collect quality control information provided by GPT. Its performance on quality control was evaluated with accuracy, precision, recall, and F-1 score. Results 165 cases were involved into the dataset, of which 5 were utilized as the prompt optimization dataset and 160 as the validation dataset. Definitive prompt was achieved through seven iterations. Time spent on the validation dataset by GPT was 13.47 +/- 2.43 min. Accuracies of pathological diagnosis, invasion depth, horizontal margin, vertical margin, vascular invasion, and lymphatic invasion of the quality control program were (0.940, 0.952) (95% CI), (0.925, 0.945) (95% CI), 0.931, 1.0, and 1.0, respectively. Precisions were (0.965, 0.969) (95% CI), (0.934, 0.954) (95% CI), and 0.957 for pathological diagnosis, invasion depth, and horizontal margin, respectively. Recalls were (0.940, 0.952) (95% CI), (0.925, 0.945) (95% CI), and 0.931 for factors as mentioned, respectively. F1-score were (0.945, 0.957) (95% CI), (0.928, 0.948) (95% CI), and 0.941 for factors as mentioned, respectively. Conclusions This quality control program was qualified of post-procedural quality control of esophageal ESDs. GPT can be easily applied to this quality control process and reduce workload of the endoscopists.","2024-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","1264-1272","","3","38","","","","","","","","","","English","","","","WOS:001342605500009","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;31</p>","","","Artificial Intelligence; ARTIFICIAL-INTELLIGENCE; Endoscopic submucosal dissection; Esophagus; GASTRIC-CANCER; Natural language processing; Quality control","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SHDPUHFP","journalArticle","2023","Alzahrani, SM","ConvAttenMixer: Brain tumor detection and type classification using convolutional mixer with external and self-attention mechanisms","JOURNAL OF KING SAUD UNIVERSITY-COMPUTER AND INFORMATION SCIENCES","","1319-1578","10.1016/j.jksuci.2023.101810","","Attention-based methods have recently demonstrated notable advancements in brain tumor classification. To further advance and strengthen this development, we have developed ConvAttenMixer, a transformer model that incorporates convolutional layers along with two attention mechanisms: self-attention and external attention. The proposed model utilizes two blocks of convolution mixers to effectively process and blend across patches, thereby enhancing the model's ability to capture spatial and channel-wise dependencies in MRI brain images. The self-attention block enables the model to prioritize important regions within the image and establish dependencies by assigning weights to each part based on their relevance to the task. This allows the model to emphasize crucial local features, disregard irrelevant ones, and capture interactions between different patches. On the other hand, the external attention block focuses more on significant global features and captures interactions among different images, enabling the model to establish dependencies and correlations across all samples. The classification head in the proposed model is a simple yet effective block designed to process the output feature maps using a squeeze-and-excitation mechanism, which in turn assigns higher weights to important channels and suppresses less-relevant channels. For experimentation, our ConvAttenMixer model was trained on a dataset consisting of 5712 MRI scans and subsequently tested on 1311 scans for classification into glioma, meningioma, pituitary tumor, and no-tumor images. Different variants of the proposed model were tested and evaluated. The optimally performing architecture was evaluated against the state-of-the-art baselines, namely self-attention MLP, external attention MLP, attention-based pooling convolutional net, and convolutional mixer net. Extensive experiments demonstrated that ConvAttenMixer outperformed the other baselines, which employed either self-attention or external attention mechanisms, while requiring significantly less computational memory. The suggested model exhibited higher precision, recall, and f-measure, achieving the highest accuracy of 0.9794 compared with the baselines' accuracy, which ranged from 0.87 to 0.93. The ConvAttenMixer model demonstrates the ability to operate locally on the patch level using self-attention and globally on the sample level using external attention, as well as prioritize important information on the spatial level and channel level using convolution mixers and the squeeze-and-excitation mechanism.","2023-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","10","35","","","","","","","","","","English","","","","WOS:001105442200001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;11<br/>Total Times Cited:&nbsp;&nbsp;11<br/>Cited Reference Count:&nbsp;&nbsp;69</p>","","","Brain tumor classification; Convolutional mixer; External attention; SEGMENTATION; Self-attention; Transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B9W958ZC","journalArticle","2021","Lin, Y; Yang, B; Li, LC; Guo, DY; Zhang, JW; Chen, H; Zhang, Y","ATCSpeechNet: A multilingual end-to-end speech recognition framework for air traffic control systems","APPLIED SOFT COMPUTING","","1568-4946","10.1016/j.asoc.2021.107847","","In this paper, a multilingual end-to-end framework, called ATCSpeechNet, is proposed to tackle the issue of translating communication speech into human-readable text in air traffic control (ATC) sys-tems. In the proposed framework, we focus on integrating multilingual automatic speech recognition (ASR) into one model, in which an end-to-end paradigm is developed to convert speech waveforms into text directly, without any feature engineering or lexicon. To compensate the deficiency of handcrafted feature engineering caused by ATC challenges, including multilingual, multispeaker dialog and unstable speech rates, a speech representation learning (SRL) network is proposed to capture robust and discriminative speech representations from raw waves. The self-supervised training strategy is adopted to optimize the SRL network from unlabeled data, and to further predict the speech features, i.e., wave-to-feature. An end-to-end architecture is improved to complete the ASR task, in which a grapheme-based modeling unit is applied to address the multilingual ASR issue. Facing the problem of small transcribed samples in the ATC domain, an unsupervised approach with mask prediction is applied to pretrain the backbone network of the ASR model on unlabeled data by a feature-to -feature process. Finally, by integrating the SRL with ASR, an end-to-end multilingual ASR framework is formulated in a supervised manner, which is able to translate the raw wave into text in one model, i.e., wave-to-text. Experimental results on the ATCSpeech corpus demonstrate that the proposed approach achieves high performance with a very small labeled corpus and less resource consumption, only a 4.20% label error rate on the 58-hour transcribed corpus. Compared to the baseline model, the proposed approach obtains over 100% relative performance improvement which can be further enhanced with increasing size of the transcribed samples. It is also confirmed that the proposed SRL and training strategies make significant contributions to improving the final performance. In addition, the effectiveness of the proposed framework is also validated on common corpora (AISHELL, LibriSpeech, and cv-fr). More importantly, the proposed multilingual framework not only reduces the system complexity but also obtains higher accuracy compared to that of the independent monolingual ASR models. The proposed approach can also greatly reduce the cost of annotating samples, which benefits to advance the ASR technique to industrial applications. (C) 2021 Published by Elsevier B.V.","2021-11","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","112","","","","","","","","","","English","","","","WOS:000724547700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;18<br/>Total Times Cited:&nbsp;&nbsp;19<br/>Cited Reference Count:&nbsp;&nbsp;47</p>","","","Air traffic control; End-to-end; KNOWLEDGE; MODELS; Multilingual; NEURAL-NETWORKS; Pretraining; Representation learning; Small samples; Speech recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"LPKH4EM6","journalArticle","2024","Sonmez, YÜ; Varol, A","In-depth investigation of speech emotion recognition studies from past to present -The importance of emotion recognition from speech signal for AI-","INTELLIGENT SYSTEMS WITH APPLICATIONS","","2667-3053","10.1016/j.iswa.2024.200351","","In the super smart society (Society 5.0), new and rapid methods are needed for speech recognition, emotion recognition, and speech emotion recognition areas to maximize human-machine or human-computer interaction and collaboration. Speech signal contains much information about the speaker, such as age, sex, ethnicity, health condition, emotion, and thoughts. The field of study which analyzes the mood of the person from the speech is called speech emotion recognition (SER). Classifying the emotions from the speech data is a complicated problem for artificial intelligence, and its sub-discipline, machine learning. Because it is hard to analyze the speech signal which contains various frequencies and characteristics. Speech data are digitized with signal processing methods and speech features are obtained. These features vary depending on the emotions such as sadness, fear, anger, happiness, boredom, confusion, etc. Even though different methods have been developed for determining the audio properties and emotion recognition, the success rate varies depending on the languages, cultures, emotions, and data sets. In speech emotion recognition, there is a need for new methods which can be applied in data sets with different sizes, which will increase classification success, in which best properties can be obtained, and which are affordable. The success rates are affected by many factors such as the methods used, lack of speech emotion datasets, the homogeneity of the database, the difficulty of the language (linguistic differences), the noise in audio data and the length of the audio data. Within the scope of this study, studies on emotion recognition from speech signals from past to present have been analyzed in detail. In this study, classification studies based on a discrete emotion model using speech data belonging to the Berlin emotional database (EMODB), Italian emotional speech database (EMOVO), The Surrey audio-visual expressed emotion database (SAVEE), Ryerson Audio-Visual Database of Emotional Speech and Song Database (RAVDESS), which are mostly independent of the speaker and content, are examined. The results of both classical classifiers and deep learning methods are compared. Deep learning results are more successful, but classical classification is more important in determining the defining features of speech, song or voice. So It develops feature extraction stage. This study will be able to contribute to the literature and help the researchers in the SER field.","2024-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","22","","","","","","","","","","English","","","","WOS:001306336500001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;112</p>","","","AMPLITUDE; Artificial intelligence; CLASSIFICATION; CLASSIFIERS; DEPRESSION; INDUCED STRESS; LEVEL; MACHINE; Machine-learning methods; SPECTRAL FEATURES; Speech emotion recognition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SQUBUYGU","journalArticle","2021","Pevy, N; Christensen, H; Walker, T; Reuber, M","Feasibility of using an automated analysis of formulation effort in patients' spoken seizure descriptions in the differential diagnosis of epileptic and nonepileptic seizures","SEIZURE-EUROPEAN JOURNAL OF EPILEPSY","","1059-1311","10.1016/j.seizure.2021.06.009","","Objective: There are three common causes of Transient Loss of Consciousness (TLOC), syncope, epileptic and psychogenic nonepileptic seizures (PNES). Many individuals who have experienced TLOC initially receive an incorrect diagnosis and inappropriate treatment. Whereas syncope can be distinguished relatively easily with a small number of ""yes""/""no"" questions, the differentiation of the other two causes of TLOC is more challenging. Previous qualitative research based on the methodology of Conversation Analysis has demonstrated that the descriptions of epileptic seizures contain more formulation effort than accounts of PNES. This research investigates whether features likely to reflect the level of formulation effort can be automatically elicited from audio recordings and transcripts of speech and used to differentiate between epileptic and nonepileptic seizures. Method: Verbatim transcripts of conversations between patients and neurologists were manually produced from video and audio recordings of 45 interactions (21 epilepsy and 24 PNES). The subsection of each transcript containing the person's account of their first seizure was manually extracted for the analysis. Seven automatically detectable features were designed as markers of formulation effort. These features were used to train a Random Forest machine learning classifier. Result: There were significantly more hesitations and repetitions in descriptions of epileptic than nonepileptic seizures. Using a nested leave-one-out cross validation approach, 71% of seizures were correctly classified by the Random Forest classifier. Discussion: This pilot study provides proof of principle that linguistic features that have been automatically extracted from audio recordings and transcripts could be used to distinguish between epileptic seizures and PNES and thereby contribute to the differential diagnosis of TLOC. Future research should explore whether additional observations can be incorporated into a diagnostic stratification tool and compare the performance of these features when they are combined with additional information provided by patients and witnesses about seizure manifestations and medical history.","2021-10","2025-02-26 20:45:06","2025-02-26 20:45:06","","141-145","","","91","","","","","","","","","","English","","","","WOS:000696706100023","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;14<br/>Total Times Cited:&nbsp;&nbsp;15<br/>Cited Reference Count:&nbsp;&nbsp;32</p>","","","Classification; CONVERSATION ANALYSIS; Diagnosis; DISORDERS; Epilepsy; Natural language processing; Nonepileptic seizures; PATIENTS ACCOUNTS; PNES; Speech analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VS2QENN5","journalArticle","2023","Hosokawa, K; Iwahashi, T; Iwahashi, M; Iwaki, S; Kato, C; Yoshida, M; Yoshida, D; Kitayama, I; Umatani, M; Matsushiro, N; Ogawa, M; Inohara, H","The Significant Influence of Hoarseness Levels in Connected Speech on the Voice-Related Disability Evaluated Using Voice Handicap Index-10","JOURNAL OF VOICE","","0892-1997","10.1016/j.jvoice.2020.11.024","","Objectives. This retrospective study examines the influence of voice quality in connected speech (CS) and sustained vowels (SV) on the voice-related disability in patients' daily living documented by Voice Handicap Index-10 (VHI-10).Methods. A total of 500 voice recordings of CS and SV samples from 338 patients with voice disturbances were included, along with the patients' age, diagnoses, maximum phonation time, and VHI-10. Dataset-1 comprised of 338 untreated patients, whereas Dataset-2 included 162 patients before and after phonosurgeries. As a prelimi-nary study, the concurrent and diagnostic validities based on auditory-perceptual judgments were examined for cepstral peak prominence (CPP) and CPP smoothed (CPPS) for CS and SV tasks. Next, simple correlations and multivariate regression analyses (MRA) were performed to identify which of the acoustic measures for the CS or SV tasks significantly influenced the total score or improvement of VHI-10.Results. The preliminary study confirmed high correlations with hoarseness levels as well as the excellent diagnostic accuracy of CPP and CPPS for both CS and SV tasks.In Dataset-1, the simple correlations and MRA results showed that cepstral measures in both tasks demonstrated moderate correlations with, and significant contribution to the total score of VHI-10, respectively. However, in Dataset-2, the changes of cepstral measures, as well as the median pitch after phonosurgeries in the CS tasks only, showed significant contributions to the improvement of VHI-10.Conclusion. The study demonstrated that the hoarseness levels in both the CS and SV tasks equivalently influ-enced the VHI-10 scores, and that the post-surgical change of voice quality only in the CS tasks influenced the of voice-related in","2023-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","290e7-290e16","","2","37","","","","","","","","","","English","","","","WOS:000958810900001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;59</p>","","","Acoustic measurement; ACOUSTIC MEASURES; Cepstral peak prominence; DYSPHONIA; LABORATORY MEASUREMENTS; PARAMETERS; PERCEPTUAL EVALUATION; QUALITY; Speech analysis; VALIDATION; Voice Handicap Index; Voice-related disability","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8LB736WS","journalArticle","2024","Patil, M; Naregalkar, PR; Tondare, S; Morbale, J; Patil, AS","Estimation of breathing signal and the respiratory parameters from the speech recordings using honey badger-based modular neural system","MULTIMEDIA TOOLS AND APPLICATIONS","","1380-7501","10.1007/s11042-024-18353-2","","Breathing is a necessary mechanism for speech production. Estimating breathing signal and respiratory parameters are the most important study in medical research. The breathing signal can be evaluated using a variety of models. However, the relevant result was not attained because of the poor quality of the model. Breathing is a vital bodily function, and changes in breathing patterns can indicate underlying health problems. For example, people with respiratory conditions such as asthma, Chronic Obstructive Pulmonary Disease (COPD), and heart failure may have abnormal breathing patterns. The speech data contains more noise content, which maximizes the model's complexity. Analyzing the speech recordings makes it difficult to develop a non-invasive and continuous monitoring system for respiration because the system failed to estimate the breathing signal using traditional methods. These problems resulted in poor prediction and less accuracy for prediction. So, this research introduces a novel Honey Badger-based Modular Neural System (HBMNS) to the challenging task of using voice recordings to estimate respiratory parameters and breathing signals. This study aims to bridge the gap between speech analysis and respiratory health monitoring by leveraging the adaptability and robustness observed in honey badger behavior. This indicates an interest in identifying patterns or features in speech that correlate with respiratory behavior. In addition to breathing signals, the system aims to estimate various respiratory parameters. These parameters might include breath rate, tidal volume, and breath event. Initially, data collection and importation into the Python environment involved speech recording data. The data was then pre-processed, and honey-badger optimization was used to extract the spectral features. The breathing signal and respiratory parameters are then estimated by a modular neural network. The provided model achieved a good exactness score in the estimation after the performance was measured.","2024-02-14","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","","","","","","","","","","","English","","","","WOS:001162161200007","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;38</p>","","","Breathing signal; Honey badger optimization; Modular neural network; Respiratory parameters; Spectral features","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZXNWHKRT","journalArticle","2022","Shellikeri, S; Cho, SHY; Cousins, KAQ; Liberman, M; Howard, E; Balganorth, Y; Weintraub, D; Spindler, M; Deik, A; Lee, EB; Trojanowski, JQ; Irwin, D; Wolk, D; Grossman, M; Nevler, N","Natural speech markers of Alzheimer's disease co-pathology in Lewy body dementias","PARKINSONISM & RELATED DISORDERS","","1353-8020","10.1016/j.parkreldis.2022.07.023","","Introduction: An estimated 50% of patients with Lewy body dementias (LBD), including Parkinson's disease dementia (PDD) and Dementia with Lewy bodies (DLB), have co-occurring Alzheimer's disease (AD) that is associated with worse prognosis. This study tests an automated analysis of natural speech as an inexpensive, non-invasive screening tool for AD co-pathology in biologically-confirmed cohorts of LBD patients with AD co -pathology (SYN + AD) and without (SYN-AD).Methods: We analyzed lexical-semantic and acoustic features of picture descriptions using automated methods in 22 SYN + AD and 38 SYN-AD patients stratified using AD CSF biomarkers or autopsy diagnosis. Speech markers of AD co-pathology were identified using best subset regression, and their diagnostic discrimination was tested using receiver operating characteristic. ANCOVAs compared measures between groups covarying for de-mographic differences and cognitive disease severity. We tested relations with CSF tau levels, and compared speech measures between PDD and DLB clinical disorders in the same cohort.Results: Age of acquisition of nouns (p = 0.034, |d| = 0.77) and lexical density (p = 0.0064, |d| = 0.72) were reduced in SYN + AD, and together showed excellent discrimination for SYN + AD vs. SYN-AD (95% sensitivity, 66% specificity; AUC = 0.82). Lower lexical density was related to higher CSF t-Tau levels (R =-0.41, p = 0.0021). Clinically-diagnosed PDD vs. DLB did not differ on any speech featuresConclusion: AD co-pathology may result in a deviant natural speech profile in LBD characterized by specific lexical-semantic impairments, not detectable by clinical disorder diagnosis. Our study demonstrates the potential of automated digital speech analytics as a screening tool for underlying AD co-pathology in LBD.","2022-09","2025-02-26 20:45:06","2025-02-26 20:45:06","","94-100","","","102","","","","","","","","","","English","","","","WOS:000862945100018","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;9<br/>Total Times Cited:&nbsp;&nbsp;9<br/>Cited Reference Count:&nbsp;&nbsp;30</p>","","","Alzheimer ?s disease; Automatic speech processing; BODIES; Digital biomarker; Lewy body dementia; Natural language processing; Parkinson disease dementia; Speech analysis; TAU","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RTLYL7MQ","journalArticle","2023","Butterworth, S; Fitzsimons, KJ; Medina, J; Britton, L; Van Eeden, S; Wahedally, H; Park, MH; van der Muelen, J; Russell, CJH","Investigating the Impact of Patient-Related Factors on Speech Outcomes at 5 Years of Age in Children With a Cleft Palate","CLEFT PALATE CRANIOFACIAL JOURNAL","","1055-6656","10.1177/10556656221110094","","Objective To investigate the relationship between patient-related factors (sex, cleft type, cleft extent, and Robin Sequence [RS]) and speech outcome at 5 years of age for children born with a cleft palate +/- lip (CP +/- L). Participants 3157 Children (1426 female:1731 male) with a nonsyndromic CP +/- L, born between 2006 and 2014 in England, Wales, and Northern Ireland. Outcome Measure Perceptual speech analysis utilized the Cleft Audit Protocol for Speech-Augmented (CAPS-A) rating and UK National Speech Outcome Standards: Speech Standard 1 (SS1)-speech within the normal range, SS2a-no structurally related speech difficulties or history of speech surgery, and SS3-speech without significant cleft-related articulation difficulties. Results Odds of achieving SS1 were lower among boys (aOR 0.771 [CI 0.660-0.901]), those with clefts involving the lip and palate (vs palate only) (UCLP-aOR 0.719 [CI 0.591-0.875]; BCLP-aOR 0.360 [CI 0.279-0.463]), and clefts involving the hard palate (incomplete-aOR 0.701 [CI 0.540-0.909]; complete-aOR 0.393 [CI 0.308-0.501]). Similar relationships with these patient factors were observed for SS3. SS2 was affected by the extent of hard palate involvement (complete; aOR 0.449 [CI 0.348-0.580]). Although those with CP and RS were less likely to meet all 3 standards than those without RS, odds ratios were not significant when adjusting for sex and cleft extent. Conclusion Sex, cleft type, and extent of hard palate involvement have a significant impact on speech outcome at 5 years of age. Incorporating these factors into risk-adjustment models for service-level outcome reporting is recommended.","2023-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","1578-1590","","12","60","","","","","","","","","","English","","","","WOS:000815035700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;14<br/>Total Times Cited:&nbsp;&nbsp;14<br/>Cited Reference Count:&nbsp;&nbsp;70</p>","","","- lip; AUDIT PROTOCOL; Cleft palate; Cleft palate plus; DELAYED HARD PALATE; FOLLOW-UP; LIP; MUSCLE RECONSTRUCTION; Patient-factors; PRIMARY SURGERY; PSYCHOLOGICAL ADJUSTMENT; ROBIN-SEQUENCE; SCANDCLEFT RANDOMIZED-TRIALS; Speech; VELOPHARYNGEAL FUNCTION","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CFRY6L7A","journalArticle","2022","Li, KY; Li, XX","AI driven human-computer interaction design framework of virtual environment based on comprehensive semantic data analysis with feature extraction","INTERNATIONAL JOURNAL OF SPEECH TECHNOLOGY","","1381-2416","10.1007/s10772-021-09954-5","","The most basic and important attributes of virtual reality (VR) are immersion, interactivity and creativity. For this young media, the continuous development and change and the exploration of content are the current situation of VR art. The arrival of 5G communication technology provides an important foundation for the popularization of VR technology. The rapid development of technology and hardware rewrites and expands the possibility of VR art design expression at any time. As the latest technology of natural human-computer interaction, somatosensory interaction technology enables users to interact with computers directly through body movements, gestures, etc., and control the environment at will. Its core value lies in that it not only enables the computer to have more accurate and effective ""eyes"" to observe the world, not only to understand human body language, but also to complete various instructions according to the body language, to achieve real-time interaction with people. Gesture, as a common communication mode with the ability of information expression, has the advantages of intuitionistic, natural and easy to understand. Therefore, the somatosensory interaction technology based on gesture recognition has become an effective means of natural human-computer interaction. With the rapid development of key technologies of VR, VR has been widely used in games, fitness, real estate, education, film and television and other industries. In recent years, VR systems are everywhere, including virtual home decoration system, virtual panoramic VR live broadcasting system, virtual fitness system, etc. The design and implementation of this VR system mainly includes four parts: scene modeling, rendering and rendering, performance and interaction, and network application. Based on this, this paper uses the comprehensive data analysis technology to build the human-computer interaction design model in the virtual environment. The AI and speech information are combined to perform the optimal analysis.","2022-02-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","","","","","","","","","","","English","","","","WOS:000752749100005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;28</p>","","","Data analysis; Eye movement system; Human-computer interaction; Object recognition; Speech analysis; Virtual reality","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XUMV9GUW","journalArticle","2024","Finze, N; Jechle, D; Fausser, S; Gewald, H","How are We Doing Today? Using Natural Speech Analysis to Assess Older Adults' Subjective Well-Being","BUSINESS & INFORMATION SYSTEMS ENGINEERING","","2363-7005","10.1007/s12599-024-00877-4","","The research presents the development and test of a machine learning (ML) model to assess the subjective well-being of older adults based solely on natural speech. The use of such technologies can have a positive impact on healthcare delivery: the proposed ML model is patient-centric and securely uses user-generated data to provide sustainable value not only in the healthcare context but also to address the global challenge of demographic change, especially with respect to healthy aging. The developed model unobtrusively analyzes the vocal characteristics of older adults by utilizing natural language processing but without using speech recognition capabilities and adhering to the highest privacy standards. It is based on theories of subjective well-being, acoustic phonetics, and prosodic theories. The ML models were trained with voice data from volunteer participants and calibrated through the World Health Organization Quality of Life Questionnaire (WHOQOL), a widely accepted tool for assessing the subjective well-being of human beings. Using WHOQOL scores as a proxy, the developed model provides accurate numerical estimates of individuals' subjective well-being.Different models were tested and compared. The regression model proves beneficial for detecting unexpected shifts in subjective well-being, whereas the support vector regression model performed best and achieved a mean absolute error of 10.90 with a standard deviation of 2.17. The results enhance the understanding of the subconscious information conveyed through natural speech. This offers multiple applications in healthcare and aging, as well as new ways to collect, analyze, and interpret self-reported user data. Practitioners can use these insights to develop a wealth of innovative products and services to help seniors maintain their independence longer, and physicians can gain much greater insight into changes in their patients' subjective well-being.","2024-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","321-334","","3","66","","","","","","","","","","English","","","","WOS:001242153500001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;81</p>","","","Artificial intelligence; HEALTH; Machine learning; Natural language processing; NEURAL-NETWORK; Older adult; RECOGNITION; REGRESSION SHRINKAGE; SELECTION; Subjective well-being assessment; SUPPORT; Voice analysis; WHOQOL; WHOQOL-OLD, WHOQOL-BREF","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PDGBI3KF","journalArticle","2022","Reddy, MK; Pohjalainen, H; Helkkula, P; Kaitue, K; Minkkinen, M; Tolppanen, H; Nieminen, T; Alku, P","Glottal flow characteristics in vowels produced by speakers with heart failure","SPEECH COMMUNICATION","","0167-6393","10.1016/j.specom.2021.12.001","","Heart failure (HF) is one of the most life-threatening diseases globally. HF is an under-diagnosed condition, and more screening tools are needed to detect it. A few recent studies have suggested that HF also affects the functioning of the speech production mechanism by causing generation of edema in the vocal folds and by impairing the lung function. It has not yet been studied whether these possible effects of HF on the speech production mechanism are large enough to cause acoustically measurable differences to distinguish speech produced in HF from that produced by healthy speakers. Therefore, the goal of the present study was to compare speech production between HF patients and healthy controls by focusing on the excitation signal generated at the level of the vocal folds, the glottal flow. The glottal flow was computed from speech using the quasi-closed phase glottal inverse filtering method and the estimated flow was parameterized with 12 glottal parameters. The sound pressure level (SPL) was measured from speech as an additional parameter. The statistical analyses conducted on the parameters indicated that most of the glottal parameters and SPL were significantly different between the HF patients and healthy controls. The results showed that the HF patients generally produced a more rounded glottal pulse and a lower SPL level compared to the healthy controls, indicating incomplete glottal closure and inappropriate leakage of air through the glottis. The results observed in this preliminary study indicate that glottal features are capable of distinguishing speakers with HF from healthy controls. Therefore, the study suggests that glottal features constitute a potential feature extraction approach which should be taken into account in future large-scale investigations in studying the automatic detection of HF from speech.","2022-02","2025-02-26 20:45:06","2025-02-26 20:45:06","","35-43","","","137","","","","","","","","","","English","","","","WOS:000793242300003","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;52</p>","","","AIR-FLOW; AMPLITUDE QUOTIENT; Bio-marker; FUNDAMENTAL-FREQUENCY; Glottal source parameters; Heart failure; PARKINSONS-DISEASE; PHONATION; PRESSURE; Sound pressure level; SPEECH ANALYSIS; VOICE SOURCE; Wilcoxon rank sum test","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YP3AITWJ","journalArticle","2023","Cascella, M; Vitale, VN; Mariani, F; Iuorio, M; Cutugno, F","Development of a binary classifier model from extended facial codes toward video-based pain recognition in cancer patients","SCANDINAVIAN JOURNAL OF PAIN","","1877-8860","10.1515/sjpain-2023-0011","","Objectives The Automatic Pain Assessment (APA) relies on the exploitation of objective methods to evaluate the severity of pain and other pain-related characteristics. Facial expressions are the most investigated pain behavior features for APA. We constructed a binary classifier model for discriminating between the absence and presence of pain through video analysis.Methods A brief interview lasting approximately two-minute was conducted with cancer patients, and video recordings were taken during the session. The Delaware Pain Database and UNBC-McMaster Shoulder Pain dataset were used for training. A set of 17 Action Units (AUs) was adopted. For each image, the OpenFace toolkit was used to extract the considered AUs. The collected data were grouped and split into train and test sets: 80 % of the data was used as a training set and the remaining 20 % as the validation set. For continuous estimation, the entire patient video with frame prediction values of 0 (no pain) or 1 (pain), was imported into an annotator (ELAN 6.4). The developed Neural Network classifier consists of two dense layers. The first layer contains 17 nodes associated with the facial AUs extracted by OpenFace for each image. The output layer is a classification label of ""pain"" (1) or ""no pain"" (0).Results The classifier obtained an accuracy of & SIM;94 % after about 400 training epochs. The Area Under the ROC curve (AUROC) value was approximately 0.98.Conclusions This study demonstrated that the use of a binary classifier model developed from selected AUs can be an effective tool for evaluating cancer pain. The implementation of an APA classifier can be useful for detecting potential pain fluctuations. In the context of APA research, further investigations are necessary to refine the process and particularly to combine this data with multi-parameter analyses such as speech analysis, text analysis, and data obtained from physiological parameters.","2023-10-26","2025-02-26 20:45:06","2025-02-26 20:45:06","","638-645","","4","23","","","","","","","","","","English","","","","WOS:001061741700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;9<br/>Total Times Cited:&nbsp;&nbsp;9<br/>Cited Reference Count:&nbsp;&nbsp;45</p>","","","artificial intelligence; automatic pain assessment; binary classifier model; cancer pain; EXPRESSION; pain","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IIWIRJVC","journalArticle","2023","Everaert, E; Selten, I; Boerma, T; Houben, M; Vorstman, J; de Wilde, H; Derksen, D; Haverkamp, S; Wijnen, F; Gerrits, E","The Language Profile of Preschool Children With 22q11.2 Deletion Syndrome and the Relationship With Speech Intelligibility","AMERICAN JOURNAL OF SPEECH-LANGUAGE PATHOLOGY","","1058-0360","10.1044/2022_AJSLP-21-00328","","Purpose: Young children with 22q11.2 deletion syndrome (22q11DS) often have impaired language development and poor speech intelligibility. Here, we report a comprehensive overview of standardized language assessment in a relatively large sample of preschool-aged children with 22q11DS. We furthermore explored whether speech ability explained variability in language skills. Method: Forty-four monolingual Dutch preschoolers (3-6 years) with a confirmed genetic 22q11DS diagnosis participated in this prospective cohort study. Standardized tests (Clinical Evaluation of Language Fundamentals Preschool-2-NL and Peabody Picture Vocabulary Test-III-NL) were administered. Speech intelligibility was rated by two expert speech and language therapists using a standardized procedure.Results: Most children had impaired language skills across all tested domains. The composite score for expressive language was significantly lower than that for receptive language, but the two were strongly correlated. Only small differ-ences between the mean scores on the various subtests were observed, with the lowest scores for expressive morphosyntactic skills. Language scores showed a moderate positive relation with speech intelligibility, but language abilities varied greatly among the children with intelligible speech. Conclusions: We show that the majority of preschool children with 22q11DS have a broad range of language problems. Other than the relatively larger impair-ment in expressive than in receptive language skills, our results do not show a clearly delineated language profile. As many of the children with intelligible speech still had below-average language scores, we highlight that language prob-lems require a broad assessment and care in all young children with 22q11DS. Future research using spontaneous language and detailed speech analysis is rec-ommended, to provide more in-depth understanding of children's language profile and the relationship between speech and language in 22q11DS.","2023-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","128-144","","1","32","","","","","","","","","","English","","","","WOS:000919666000009","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;6<br/>Total Times Cited:&nbsp;&nbsp;7<br/>Cited Reference Count:&nbsp;&nbsp;69</p>","","","ABILITIES; BEHAVIOR; CARDIO-FACIAL-SYNDROME; CLEFT-LIP; HISTORY; IMPAIRMENT; ISSUES; MICRODELETION; SKILLS; VELOCARDIOFACIAL SYNDROME","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SVZA6ZL2","journalArticle","2022","Bertini, F; Allevi, D; Lutero, G; Calzà, L; Montesi, D","An automatic Alzheimer's disease classifier based on spontaneous spoken English","COMPUTER SPEECH AND LANGUAGE","","0885-2308","10.1016/j.csl.2021.101298","","According to the World Health Organization, the number of people suffering from dementia worldwide will grow to 150 million by mid-century, and Alzheimer's disease is the most common form of dementia contributing to 60%-70% of cases. The problem is compounded by the fact that current pharmacologic treatments are only symptomatic, and therapies are ineffective in slow down or cure the degenerative process. An automatic and standardize classifier for Alzheimer's disease is thereby extremely important to rapidly respond and deliver as preventive as possible interventions. Speech alterations might be one of the earliest signs of cognitive defect and, recently, the researchers showed that they can be observable well in advance other cognitive deficits become manifest. In this paper, we propose a full automated method able to classify the spontaneous spoken production of the subjects. In particular, we trained an artificial neural network using the spectrogram of the audio signal, which is the visual representation of the speech of the subject. Moreover, to overcome the problem of the large amount of annotated data usually required for training deep learning models, we used a specific data augmentation approach that avoids distorting the original samples. We evaluated the proposed method using the English Pitt Corpus from DementiaBank. The used dataset consists of 180 subjects: 43 healthy controls and 137 Alzheimer's disease patients. The proposed method outperformed the other approaches in the literature based on manual and semi-automatic transcription and annotation of speech, improving the classification capability by 5.93%, and obtained good classification results compared to the state-of-the-art neuropsychological screening tests (i.e., the Mini-Mental State Examination and the Activities of Daily Living portion of the Blessed Dementia Rating Scale) exhibiting an accuracy of 93.30% and an F1 score of 88.50%.","2022-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","72","","","","","","","","","","English","","","","WOS:000728821200011","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;26<br/>Total Times Cited:&nbsp;&nbsp;27<br/>Cited Reference Count:&nbsp;&nbsp;51</p>","","","Alzheimer's disease; Autoencoder neural networks; Data augmentation; DEMENTIA; MILD COGNITIVE IMPAIRMENT; RECOGNITION; SPEECH; Speech analysis; Speech classification","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DIK7TQ9Z","journalArticle","2024","Alhumoud, SO","CNN-BiGRU-Focus: A Hybrid Deep Learning Classifier for Sentiment and Hate Speech Analysis of Ashura-Arabic Content for Policy Makers","INTERNATIONAL JOURNAL OF ADVANCED COMPUTER SCIENCE AND APPLICATIONS","","2158-107X","","","The rise of hate speech on social media during significant cultural and religious events, such as Ashura, poses serious challenges for content moderation, particularly in languages like Arabic, which present unique linguistic complexities. Most existing hate speech detection models, primarily developed for English text, fail to effectively handle the intricacies of Arabic, including its diverse dialects and rich morphology. This limitation underscores the need for specialized models tailored to the Arabic language. In response, the CNNBiGRU-Focus model proposed, a novel hybrid deep learning (DL) approach that combines Convolutional Neural Networks (CNN) to capture local linguistic patterns and Bidirectional Gated Recurrent Units (BiGRU) to manage long-term dependencies in sequential text. An attention mechanism is incorporated to enhance the model's ability to focus on the most relevant sections of the input, improving both the accuracy and interpretability of its predictions. In this paper, this model applied to a dataset of social media posts related to Ashura, revealing that 32% of the content comprised hate speech, with Shia users expressing more sentiments than Sunni users. Through extensive experiments, the CNN-BiGRU-Focus model demonstrated superior performance, significantly outperforming baseline models. It achieved an accuracy of 99.89% and AUC of 99, marking a substantial improvement in Ashura-Arabic hate speech detection. The model effectively addresses the linguistic challenges of Arabic, including dialect variations and nuanced contexts, making it highly suitable for content moderation tasks. To the best of author's knowledge, this study represents the first attempt to compile an Arabic-Ashura hate detection dataset from Twitter and apply CNN-BiGRU-Focus DL model to detect hate sentiment in Arabic social media posts. Dataset and source code can be downloaded from (https://github.com/imamu-asa).","2024-11","2025-02-26 20:45:06","2025-02-26 20:45:06","","1006-1020","","11","15","","","","","","","","","","English","","","","WOS:001378230200001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;38</p>","","","Arabic hate speech; Ashura content; attention mechanism; bidirectional gated recurrent unit; convolutional neural networks; deep learning; natural language processing; sentiment analysis; social media analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"3VJ3QM2W","journalArticle","2024","Bouvier, L; McKinlay, S; Truong, J; Genge, A; Dupre, N; Dionne, A; Kalra, S; Yunusova, Y","Speech timing and monosyllabic diadochokinesis measures in the assessment of amyotrophic lateral sclerosis in Canadian French","INTERNATIONAL JOURNAL OF SPEECH-LANGUAGE PATHOLOGY","","1754-9507","10.1080/17549507.2023.2214706","","PurposeThe primary objective of this study was to determine if speech and pause measures obtained using a passage reading task and timing measures from a monosyllabic diadochokinesis (DDK) task differ across speakers of Canadian French diagnosed with amyotrophic lateral sclerosis (ALS) presenting with and without bulbar symptoms, and healthy controls. The secondary objective was to determine if these measures can reflect the severity of bulbar symptoms.MethodA total of 29 Canadian French speakers with ALS (classified as bulbar symptomatic [n = 14] or pre-symptomatic [n = 15]) and 17 age-matched healthy controls completed a passage reading task and a monosyllabic DDK task (/pa/ and /ta/), for up to three follow-up visits. Measures of speaking rate, total duration, speech duration, and pause events were extracted from the passage reading recordings using a semi-automated speech and pause analysis procedure. Manual analysis of DDK recordings provided measures of DDK rate and variability.ResultGroup comparisons revealed significant differences (p = < .05) between the symptomatic group and the pre-symptomatic and control groups for all passage measures and DDK rates. Only the DDK rate in /ta/ differentiated the pre-symptomatic and control groups. Repeated measures correlations revealed moderate correlations (r(rm) = > 0.40; p = < 0.05) between passage measures of total duration, speaking rate, speech duration, and number of pauses, and ALSFRS-R total and bulbar scores, as well as between DDK rate and ALSFRS-R total score.ConclusionSpeech and pause measures in passage and timing measures in monosyllabic DDK tasks might be suitable for monitoring bulbar functional symptoms in French speakers with ALS, but more work is required to identify which measures are sensitive to the earliest stages of the disease.","2024-03-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","267-277","","2","26","","","","","","","","","","English","","","","WOS:000999741700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;63</p>","","","ALS; amyotrophic lateral sclerosis; bulbar ALS; BULBAR DYSFUNCTION; DETERIORATION; dysarthria; DYSARTHRIA; FEATURES; French speakers; MOTOR; motor speech disorders; ORAL DIADOCHOKINESIS; SPEAKING RATE; speech analysis; SYMPTOMS; TIME","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F7JY52K6","journalArticle","2022","Bazhenova, E","Margarita N. Kozhina's Way in Language Studies","QUAESTIO ROSSICA","","2311-911X","10.15826/qr.2022.2.697","","This article presents a scholarly biography of Margarita Kozhina (1925-2012), the founder of the well-known Perm School of Stylistics and professor at Perm State University. She developed the theory of functional stylistics, which revealed and described the regularities of stylistic differentiation in literary Russian. In the early 1960s, Kozhina was one of the first researchers in Russian linguistics to study issues of language functioning and form a new scholarly direction, namely speech studies. Based on an interdisciplinary approach, she defined the main categories of functional stylistics. The latter was regarded as the most important component of speech studies. Kozhina's papers organically entered the episteme of the second half of the twentieth and early twentyfirst centuries. They marked the turn of linguistics from the system-structural paradigm of studying language to the functional one. Kozhina developed the stylistic-statistical method of speech analysis. A statistical survey of the linguistic side of functional styles allowed her to make conclusions about the interaction of linguistic and extra-linguistic factors in speech. Kozhina authored more than 200 scholarly publications, including 8 monographs and the first Russian textbook on Russian-language stylistics. She was a member of the International Committee of Slavists and the editorial boards of several scholarly journals, including Stylistyka (Poland) and Styl (Serbia). The article describes Kozhina both as a prominent theorist and a remarkable organiser of scholarship. She initiated 20 collections of academic works on stylistics, some international conferences, a three-volume collective monograph on the history of Russian scholarly style from the eighteenth to the twentieth centuries, and the first Stylistic Encyclopaedic Dictionary of the Russian Language (a compendium of knowledge on functional stylistics). The author describes Margarita Kozhina as a talented researcher and an unusually strong personality, who managed, having overcome dramatic circumstances, to make an important contribution to European studies of language.","2022","2025-02-26 20:45:06","2025-02-26 20:45:06","","709-720","","2","10","","","","","","","","","","English","","","","WOS:000945575400020","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;21</p>","","","functional style of speech; functional stylistics; Margarita Kozhina; Perm School of Stylistics; scholarly biography; speech studies","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"365XHGRV","journalArticle","2023","Cavalcanti, JC; Eriksson, A; Barbosa, PA","On the speaker discriminatory power asymmetry regarding acoustic-phonetic parameters and the impact of speaking style","FRONTIERS IN PSYCHOLOGY","","1664-1078","10.3389/fpsyg.2023.1101187","","This study aimed to assess what we refer to as the speaker discriminatory power asymmetry and its forensic implications in comparisons performed in different speaking styles: spontaneous dialogues vs. interviews. We also addressed the impact of data sampling on the speaker's discriminatory performance concerning different acoustic-phonetic estimates. The participants were 20 male speakers, Brazilian Portuguese speakers from the same dialectal area. The speech material consisted of spontaneous telephone conversations between familiar individuals, and interviews conducted between each individual participant and the researcher. Nine acoustic-phonetic parameters were chosen for the comparisons, spanning from temporal and melodic to spectral acoustic-phonetic estimates. Ultimately, an analysis based on the combination of different parameters was also conducted. Two speaker discriminatory metrics were examined: Cost Log-likelihood-ratio (Cllr) and Equal Error Rate (EER) values. A general speaker discriminatory trend was suggested when assessing the parameters individually. Parameters pertaining to the temporal acoustic-phonetic class depicted the weakest performance in terms of speaker contrasting power as evidenced by the relatively higher Cllr and EER values. Moreover, from the set of acoustic parameters assessed, spectral parameters, mainly high formant frequencies, i.e., F3 and F4, were the best performing in terms of speaker discrimination, depicting the lowest EER and Cllr scores. The results appear to suggest a speaker discriminatory power asymmetry concerning parameters from different acoustic-phonetic classes, in which temporal parameters tended to present a lower discriminatory power. The speaking style mismatch also seemed to considerably impact the speaker comparison task, by undermining the overall discriminatory performance. A statistical model based on the combination of different acoustic-phonetic estimates was found to perform best in this case. Finally, data sampling has proven to be of crucial relevance for the reliability of discriminatory power assessment.","2023-04-17","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","14","","","","","","","","","","English","","","","WOS:000977779300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;33</p>","","","acoustic phonetics; forensic phonetics; MUSIC; phonetics; speaker comparison; speech analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RKMDFL93","journalArticle","2024","Butterworth, S; Fitzsimons, KJ; Britton, L; Van Eeden, S; Medina, J; van der Meulen, J; Russell, CJH","Investigating the Impact of Additional Congenital Malformations on Speech Outcomes at age Five in Children with a Cleft Palate","CLEFT PALATE CRANIOFACIAL JOURNAL","","1055-6656","10.1177/10556656241287759","","Objective To determine whether children born with a cleft palate +/- lip (CP +/- L) and additional congenital differences (ACDs - including 'Congenital malformations and deformations' as coded in ICD-10), are less likely to meet the three national speech outcome standards at age five compared to children with CP +/- L and no ACDs.Design An observational study, utilizing national data from the UK Cleft Registry and Audit NEtwork (CRANE) Database linked to national administrative data of hospital admissions.Setting National Health Service, England.Patients 2191 children (993 female, 1198 male) with CP +/- L, born 2006-2012 in England, with complete speech data and consent to data linkage.Outcome Measure Perceptual speech analysis utilised the Cleft Audit Protocol for Speech - Augmented (CAPS-A) rating and United Kingdom National Speech Outcome Standards (Speech Standard 1 (SS1), Speech Standard 2a (SS2a) and Speech Standard 3 (SS3)).Results Of 2191 children, 759 (35%) had at least one ACD. Presence of one ACD did not significantly impact speech outcomes but two or more ACDs reduced the odds of achieving all three speech standards: SS1 aOR 0.602 (CI 0.45-0.82, P = .002), SS2a aOR 0.563 (CI 0.41-0.77, P = .001), SS3 aOR 0.606 (0.43-0.84, P = .003). When exploring ACDs by ICD-10 groupings, congenital malformations of the 'Eye, ear, face and neck', 'Circulatory system', 'Digestive system', 'Musculoskeletal system' and 'Other congenital malformations' reduced a child's odds of achieving the speech standards.Conclusions ACDs, in the absence of a known syndrome, have a significant impact on speech outcome at age five. Incorporating these factors into risk-adjustment models for service level outcome reporting is recommended.","2024-10-09","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","","","","","","","","","","","English","","","","WOS:001329706100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;44</p>","","","5-YEAR-OLDS; ANOMALIES; AUDIT PROTOCOL; CARE; cleft palate; DEFECTS; INFANTS; LANGUAGE; LIP; outcomes; PRIMARY SURGERY; SCANDCLEFT RANDOMIZED-TRIALS; speech assessment; syndrome","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"39U2R2QR","journalArticle","2023","Saluja, H; Shah, S; Sachdeva, S; Dadhich, A","Comparative Evaluation of Swallowing and Tongue Movements Postoperatively in Carcinoma of Tongue Patients Reconstructed by Infrahyoid Myocutaneous Flap and Radial Forearm Flap: A Randomized Double-blind Clinical Study","BIOMEDICAL AND BIOTECHNOLOGY RESEARCH JOURNAL","","2588-9834","10.4103/bbrj.bbrj_129_23","","Background: Oral cancer is one of the most common cancers not only in India but also in South Asia. Treatment of oral cancer is not only limited to cure but also requires good reconstruction of the surgical defects for a better quality of life. There are many well-proven options for reconstruction purposes for the defects of the oral cavity, including local regional flaps to microvascular-free flaps. After resections of mobile parts of the tongue (up to 50%), speech and swallowing are impaired significantly; however, even after sophisticated plastic reconstruction, serious swallowing and speech problems persist. Methods: The clinical study was carried out to on 30 patients having carcinoma of the tongue. They were allocated into two treatment test group, one was the radial forearm flap and the other was infrahyoid myocutaneousflap (IHF) group to evaluate and compare the treatment outcome for reconstruction using two different treatment modalities. The prospective clinical trial was carried out after the allocation of patients with carcinoma of the tongue into two treatment test groups. The clinical parameters, likewise swallowing reflex, speech analysis, and tongue movements, were evaluated using IBM SPSS Statistics for Windows software, 20.0 (IBM Corp., Armonk, USA). Both descriptive statistics and the inferential statistics involving one-way ANOVA and Tukey's Post hoc test were analyzed to compare both intergroup and intragroup comparison at different time intervals. Results: The swallowing and aspiration analysis showed a highly statistically significant difference at 3 months and 6 months by Post hoc test between the groups. The results for tongue movements were statistically significant, henceforth better for the IHF group. Conclusion: The infrahyoid flap is a quick, easy, and reliable reconstructive method, which is cost-effective when used with knowledge of its clinical utility and limitations, the functional results are excellent with great patient satisfaction.","2023-07","2025-02-26 20:45:06","2025-02-26 20:45:06","","391-396","","3","7","","","","","","","","","","English","","","","WOS:001181767600011","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;14</p>","","","Carcinoma of the tongue; DEFECTS; infrahyoid flap; tongue reconstruction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PFH8LNNB","journalArticle","2023","Zhang, A; Pyon, RE; Chen, K; Lin, AY","Speech Analysis of Patients with Cleft Palate Using Artificial Intelligence Techniques: A Systematic Review","FACE","","2732-5016","10.1177/27325016231187985","","Introduction: This systematic review examines the use of machine learning (ML) algorithms to detect hypernasal speech in patients with cleft palate (CP), which can persist after primary repair surgery, and require revision. Due to a shortage of speech language pathologists (SLPs), automated detection tools could help improve access to care in underserved areas. The study compares the characteristics and accuracy of different types of machine learning algorithms.Methods: On August 18, 2021, searches were conducted across 8 databases: PubMed, SCOPUS, Cochrane, IEEE, ACM, L&LB, PsychInfo, and CINAHL. Search terms used were: (Artificial Intelligence OR Machine Learning OR Neural networks AND Cleft lip OR Cleft palate OR Hypernasality OR Velopharyngeal Insufficiency). To be included, papers needed to describe ML algorithms for CP speech detection and report concordance to human professional speech clinicians.Results: Database searches yielded 135 unique articles. Five articles met full inclusion criteria and 3 additional articles were identified by hand searching references of articles that passed initial screening. These algorithms were categorized as either Feature Dependent non-Deep learning (n = 5) or Feature Dependent deep learning (n = 2) algorithms or Feature Independent deep learning (n = 3) algorithms. Their pooled average concordance were 0.85, 0.93, and 0.91 respectively. Their average training database sizes were 3587, 3921, and 6306 speech samples respectively.Conclusion: Machine learning algorithms have been shown to be an effective tool for the evaluation of hypernasal speech. This systematic review has shown that ML algorithms are able to detect hypernasality with high concordance, consistent with professional speech language clinicians in a rapid, and autonomous manner. ML algorithms can extend the reach of speech language pathologists and complement their gold standard, this long-term outcome monitoring has great potential to improve treatment outcomes.","2023-09","2025-02-26 20:45:06","2025-02-26 20:45:06","","327-337","","3","4","","","","","","","","","","English","","","","WOS:001316077100003","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;52</p>","","","ARTICULATION; artificial intelligence; CHILDREN; CLASSIFICATION; cleft lip-palate; cleft palate; cleft speech; DISORDERS; HYPERNASALITY DETECTION; LANGUAGE; LIP; machine learning; PATHOLOGY; SERVICES; TELEHEALTH; velopharyngeal insufficiency","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IVMSSTZN","journalArticle","2022","Linda, GM; Lakshmi, NVSSR; Murugan, NS; Mahapatra, RP; Muthukumaran, V; Sivaram, M","Intelligent recognition system for viewpoint variations on gait and speech using CNN-CapsNet","INTERNATIONAL JOURNAL OF INTELLIGENT COMPUTING AND CYBERNETICS","","1756-378X","10.1108/IJICC-08-2021-0178","","Purpose The paper aims to introduce an intelligent recognition system for viewpoint variations of gait and speech. It proposes a convolutional neural network-based capsule network (CNN-CapsNet) model and outlining the performance of the system in recognition of gait and speech variations. The proposed intelligent system mainly focuses on relative spatial hierarchies between gait features in the entities of the image due to translational invariances in sub-sampling and speech variations. Design/methodology/approach This proposed work CNN-CapsNet is mainly used for automatic learning of feature representations based on CNN and used capsule vectors as neurons to encode all the spatial information of an image by adapting equal variances to change in viewpoint. The proposed study will resolve the discrepancies caused by cofactors and gait recognition between opinions based on a model of CNN-CapsNet. Findings This research work provides recognition of signal, biometric-based gait recognition and sound/speech analysis. Empirical evaluations are conducted on three aspects of scenarios, namely fixed-view, cross-view and multi-view conditions. The main parameters for recognition of gait are speed, change in clothes, subjects walking with carrying object and intensity of light. Research limitations/implications The proposed CNN-CapsNet has some limitations when considering for detecting the walking targets from surveillance videos considering multimodal fusion approaches using hardware sensor devices. It can also act as a pre-requisite tool to analyze, identify, detect and verify the malware practices. Practical implications This research work includes for detecting the walking targets from surveillance videos considering multimodal fusion approaches using hardware sensor devices. It can also act as a pre-requisite tool to analyze, identify, detect and verify the malware practices. Originality/value This proposed research work proves to be performing better for the recognition of gait and speech when compared with other techniques.","2022-07-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","363-382","","3","15","","","","","","","","","","English","","","","WOS:000717627000001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;28</p>","","","Capsule network; Convolutional neural network; Deep learning; Gait recognition; Intelligent system; MOTION; Viewpoint variations","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"N3XI5AXV","journalArticle","2024","Ferrari, A; Huichapa, T; Spoletini, P; Novielli, N; Fucci, D; Girardi, D","Using Voice and Biofeedback to Predict User Engagement during Product Feedback Interviews","ACM TRANSACTIONS ON SOFTWARE ENGINEERING AND METHODOLOGY","","1049-331X","10.1145/3635712","","Capturing users' engagement is crucial for gathering feedback about the features of a software product. In a market-driven context, current approaches to collecting and analyzing users' feedback are based on techniques leveraging information extracted from product reviews and social media. These approaches are hardly applicable in contexts where online feedback is limited, as for the majority of apps, and software in general. In such cases, companies need to resort to face-to-face interviews to get feedback on their products. In this article, we propose to utilize biometric data, in terms of physiological and voice features, to complement product feedback interviews with information about the engagement of the user on product-relevant topics. We evaluate our approach by interviewing users while gathering their physiological data (i.e., biofeedback) using an Empatica E4 wristband, and capturing their voice through the default audio-recorder of a common laptop. Our results show that we can predict users' engagement by training supervised machine learning algorithms on biofeedback and voice data, and that voice features alone can be sufficiently effective. The best configurations evaluated achieve an average F1 similar to 70% in terms of classification performance, and use voice features only. This work is one of the first studies in requirements engineering in which biometrics are used to identify emotions. Furthermore, this is one of the first studies in software engineering that considers voice analysis. The usage of voice features can be particularly helpful for emotion-aware feedback collection in remote communication, either performed by human analysts or voice-based chatbots, and can also be exploited to support the analysis of meetings in software engineering research.","2024-05","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","4","33","","","","","","","","","","English","","","","WOS:001208845200005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;138</p>","","","affective requirements engineering; biofeedback analysis; CLASSIFICATION; CROWD; DOMAIN KNOWLEDGE; emotion detection; EMOTION RECOGNITION; FEATURES; INVOLVEMENT; PULSE; REQUIREMENTS ELICITATION INTERVIEWS; requirements engineering; SENTIMENT; Software engineering; SPEECH; speech analysis; voice analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XP77V79Q","journalArticle","2021","Solomon, NP; Brungart, DS; Wince, JR; Abramowitz, JC; Eitel, MM; Cohen, J; Lippa, SM; Brickell, TA; French, LM; Lange, RT","Syllabic Diadochokinesis in Adults With and Without Traumatic Brain Injury: Severity, Stability, and Speech Considerations","AMERICAN JOURNAL OF SPEECH-LANGUAGE PATHOLOGY","","1058-0360","10.1044/2020_AJSLP-20-00158","","Purpose: Syllabic diadochokinesis (DDK) is a standard assessment task for motor speech disorders. This study aimed to compare rate and regularity of DDK according to the presence or absence of traumatic brain injury (TBI) and severity of TBI, examine the stability of DDK over time, and explore associations between DDK and extemporaneous speech. Method: Military service members and veterans were categorized into three groups: no history of TBI (control), uncomplicated mild TBI (mTBI), and moderate through severe (including penetrating) TBI (msTBl). Participants produced rapid alternating-motion and sequential-motion syllable repetitions during one or two sessions. A semi-automated protocol determined syllabic rate and regularity. Perceptual ratings of selected participants' connected speech samples were compared to DDK results. Results: Two hundred sixty-three service members and veterans provided data from one session and 69 from two sessions separated by 1.9 years (SD = 1.0). DDKs were significantly slower overall for mTBI and msTBI groups compared to controls. Regularity of productions did not differ significantly across groups. A significant Group x Task interaction revealed that the msTBI group produced sequential-motion syllable repetitions but not alternating-motion repetitions with greater regularity, whereas the opposite occurred for control and mTBI groups. DDK results did not differ significantly between sessions. Perceptual speech analysis for 30 participants, including 20 with atypical or questionable DDK performance, revealed two participants with mildly abnormal speech. Conclusions: Overall, DDK productions are slower than normal in adults with moderate, severe, and penetrating TBI and are stable over time. Regularity of productions did not differentiate groups, although this result differed according to task. There were surprisingly few people identified with disordered speech, making comparisons to DDK data tenuous, and indicating that dysarthria is a rare complication in a population of adults with mostly uncomplicated mTBI who are not selected from referrals to a speech-language pathology clinic.","2021-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","1400-1409","","3","30","","","","","","","","","","English","","","","WOS:000663328000008","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;8<br/>Total Times Cited:&nbsp;&nbsp;9<br/>Cited Reference Count:&nbsp;&nbsp;44</p>","","","ALTERNATING MOTION RATE; AMYOTROPHIC-LATERAL-SCLEROSIS; APRAXIA; DISORDERS; DYSARTHRIA; FIXED-TARGET TASK; IMPAIRMENT; MOTOR; ORAL DIADOCHOKINESIS; SPEAKING RATE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UYYSFT2L","journalArticle","2023","Tröger, J; Baltes, J; Baykara, E; Kasper, E; Kring, M; Linz, N; Robin, J; Schäfer, S; Schneider, A; Hermann, A","PROSA-a multicenter prospective observational study to develop low-burden digital speech biomarkers in ALS and FTD","AMYOTROPHIC LATERAL SCLEROSIS AND FRONTOTEMPORAL DEGENERATION","","2167-8421","10.1080/21678421.2023.2239312","","Objective: There is a need for novel biomarkers that can indicate disease state, project disease progression, or assess response to treatment for amyotrophic lateral sclerosis (ALS) and associated neurodegenerative diseases such as frontotemporal dementia (FTD). Digital biomarkers are especially promising as they can be collected non-invasively and at low burden for patients. Speech biomarkers have the potential to objectively measure cognitive, motor as well as respiratory symptoms at low-cost and in a remote fashion using widely available technology such as telephone calls. Methods: The PROSA study aims to develop and evaluate low-burden frequent prognostic digital speech biomarkers. The main goal is to create a single, easy-to-perform battery that serves as a valid and reliable proxy for cognitive, respiratory, and motor domains in ALS and FTD. The study will be a multicenter 12-months observational study aiming to include 75 ALS and 75 FTD patients as well as 50 healthy controls and build on three established longitudinal cohorts: DANCER, DESCRIBE-ALS and DESCRIBE-FTD. In addition to the extensive clinical phenotyping in DESCRIBE, PROSA collects a comprehensive speech protocol in fully remote and automated fashion over the telephone at four time points. This longitudinal speech data, together with gold standard measures, will allow advanced speech analysis using artificial intelligence for the development of speech-based phenotypes of ALS and FTD patients measuring cognitive, motor and respiratory symptoms. Conclusion: Speech-based phenotypes can be used to develop diagnostic and prognostic models predicting clinical change. Results are expected to have implications for future clinical trial stratification as well as supporting innovative trial designs in ALS and FTD.","2023-10-02","2025-02-26 20:45:06","2025-02-26 20:45:06","","589-598","","7-8","24","","","","","","","","","","English","","","","WOS:001039405000001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;55</p>","","","ALS; ALZHEIMERS-DISEASE; AMYOTROPHIC-LATERAL-SCLEROSIS; AUTOMATED-ANALYSIS; BEHAVIORAL VARIANT; DECLINE; DEFICITS; digital biomarkers; FRONTOTEMPORAL DEMENTIA; FTD; multicenter observational study; PRIMARY PROGRESSIVE APHASIA; RATING-SCALE; Speech biomarkers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DVEKJBQZ","journalArticle","2023","Barbon, S Jr; Guido, RC; Aguiar, GJ; Santana, EJ; Proenca, ML Jr; Patil, HA","Multiple voice disorders in the same individual: Investigating handcrafted features, multi-label classification algorithms, and base-learners","SPEECH COMMUNICATION","","0167-6393","10.1016/j.specom.2023.102952","","Non-invasive acoustic analyses of voice disorders have been at the forefront of current biomedical research. Usual strategies, essentially based on machine learning (ML) algorithms, commonly classify a subject as being either healthy or pathologically-affected. Nevertheless, the latter state is not always a result of a sole laryngeal issue, i.e., multiple disorders might exist, demanding multi-label classification procedures for effective diagnoses. Consequently, the objective of this paper is to investigate the application of five multi-label classification methods based on problem transformation to play the role of base-learners, i.e., Label Powerset, Binary Relevance, Nested Stacking, Classifier Chains, and Dependent Binary Relevance with Random Forest (RF) and Support Vector Machine (SVM), in addition to a Deep Neural Network (DNN) from an algorithm adaptation method, to detect multiple voice disorders, i.e., Dysphonia, Laryngitis, Reinke's Edema, Vox Senilis, and Central Laryngeal Motion Disorder. Receiving as input three handcrafted features, i.e., signal energy (SE), zero-crossing rates (ZCRs), and signal entropy (SH), which allow for interpretable descriptors in terms of speech analysis, production, and perception, we observed that the DNN-based approach powered with SE-based feature vectors presented the best values of F1-score among the tested methods, i.e., 0.943, as the averaged value from all the balancing scenarios, under Saarbrucken Voice Database (SVD) and considering 20% of balancing rate with Synthetic Minority Over-sampling Technique (SMOTE). Finally, our findings of most false negatives for laryngitis may explain the reason why its detection is a serious issue in speech technology. The results we report provide an original contribution, allowing for the consistent detection of multiple speech pathologies and advancing the state-of-the-art in the field of handcrafted acoustic-based non-invasive diagnosis of voice disorders.","2023-07","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","152","","","","","","","","","","English","","","","WOS:001036871600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;86</p>","","","Deep learning; ENSEMBLE; EXTRACTION; Handcrafted feature extraction; IDENTIFICATION; METHODOLOGY; Multi-label classification; Multiple voice disorders; SPEECH; SYSTEM; TUTORIAL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K53Y9J9W","journalArticle","2021","Guzmán-García, C; Gómez-Tome, M; Sánchez-González, P; Oropesa, I; Gómez, EJ","Speech-Based Surgical Phase Recognition for Non-Intrusive Surgical Skills' Assessment in Educational Contexts","SENSORS","","1424-8220","10.3390/s21041330","","Surgeons' procedural skills and intraoperative decision making are key elements of clinical practice. However, the objective assessment of these skills remains a challenge to this day. Surgical workflow analysis (SWA) is emerging as a powerful tool to solve this issue in surgical educational environments in real time. Typically, SWA makes use of video signals to automatically identify the surgical phase. We hypothesize that the analysis of surgeons' speech using natural language processing (NLP) can provide deeper insight into the surgical decision-making processes. As a preliminary step, this study proposes to use audio signals registered in the educational operating room (OR) to classify the phases of a laparoscopic cholecystectomy (LC). To do this, we firstly created a database with the transcriptions of audio recorded in surgical educational environments and their corresponding phase. Secondly, we compared the performance of four feature extraction techniques and four machine learning models to find the most appropriate model for phase recognition. The best resulting model was a support vector machine (SVM) coupled to a hidden-Markov model (HMM), trained with features obtained with Word2Vec (82.95% average accuracy). The analysis of this model's confusion matrix shows that some phrases are misplaced due to the similarity in the words used. The study of the model's temporal component suggests that further attention should be paid to accurately detect surgeons' normal conversation. This study proves that speech-based classification of LC phases can be effectively achieved. This lays the foundation for the use of audio signals for SWA, to create a framework of LC to be used in surgical training, especially for the training and assessment of procedural and decision-making skills (e.g., to assess residents' procedural knowledge and their ability to react to adverse situations).","2021-02","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","4","21","","","","","","","","","","English","","","","WOS:000624708100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;8<br/>Total Times Cited:&nbsp;&nbsp;8<br/>Cited Reference Count:&nbsp;&nbsp;58</p>","","","CLASSIFICATION; feature extraction; machine learning; natural language processing; REGRESSION; speech analysis; surgical process model; surgical workflow analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K9SLI38A","journalArticle","2024","Pevy, N; Christensen, H; Walker, T; Reuber, M","Predicting the cause of seizures using features extracted from interactions with a virtual agent","SEIZURE-EUROPEAN JOURNAL OF EPILEPSY","","1059-1311","10.1016/j.seizure.2023.11.022","","Objective: A clinical decision tool for Transient Loss of Consciousness (TLOC) could reduce currently high misdiagnosis rates and waiting times for specialist assessments. Most clinical decision tools based on patient reported symptom inventories only distinguish between two of the three most common causes of TLOC (epilepsy, functional /dissociative seizures, and syncope) or struggle with the particularly challenging differentiation between epilepsy and FDS. Based on previous research describing differences in spoken accounts of epileptic seizures and FDS seizures, this study explored the feasibility of predicting the cause of TLOC by combining the automated analysis of patient-reported symptoms and spoken TLOC descriptions.Method: Participants completed an online web application that consisted of a 34-item medical history and symptom questionnaire (iPEP) and spoken interaction with a virtual agent (VA) that asked eight questions about the most recent experience of TLOC. Support Vector Machines (SVM) were trained using different combinations of features and nested leave-one-out cross validation. The iPEP provided a baseline performance. Inspired by previous qualitative research three spoken language based feature sets were designed to assess: (1) formulation effort, (2) the proportion of words from different semantic categories, and (3) verb, adverb, and adjective usage.Results: 76 participants completed the application (Epilepsy = 24, FDS = 36, syncope = 16). Only 61 participants also completed the VA interaction (Epilepsy = 20, FDS = 29, syncope = 12). The iPEP model accurately predicted 65.8 % of all diagnoses, but the inclusion of the language features increased the accuracy to 85.5 % by improving the differential diagnosis between epilepsy and FDS.Conclusion: These findings suggest that an automated analysis of TLOC descriptions collected using an online web application and VA could improve the accuracy of current clinical decisions tools for TLOC and facilitate clinical stratification processes (such as ensuring appropriate referral to cardiological versus neurological investigation and management pathways).","2024-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","84-89","","","114","","","","","","","","","","English","","","","WOS:001137159700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;33</p>","","","ATTACK; Classification; DIFFERENTIAL-DIAGNOSIS; Epilepsy; Functional seizures; Natural language processing; PATIENTS ACCOUNTS; Speech analysis; Transient loss of consciousness","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IQCR28LY","journalArticle","2021","García, LS; Raman, S; Rioja, IH; Cordón, EN; Sanchez, J; Saratxaga, I","A Spanish multispeaker database of esophageal speech","COMPUTER SPEECH AND LANGUAGE","","0885-2308","10.1016/j.csl.2020.101168","","A laryngectomee is a person whose larynx has been removed by surgery, usually due to laryngeal cancer. After surgery, most laryngectomees are able to speak again, using techniques that are learned with the help of a speech therapist. This is termed as alaryngeal speech, and esophageal speech (ES) is one of the several alaryngeal speech production modes. A considerable amount of research has been dedicated to the study of alaryngeal speech, with a wide range of aims such as helping speech therapists with evaluation and diagnosis, and improving its quality and intelligibility using digital signal processing techniques. We present to you a database of Spanish ES voices, named AhoSLABI, which is designed to allow the development of new support technologies for this speech impairment. The database primarily consists of recordings of 31 laryngectomees (27 males and 4 females) pronouncing phonetically balanced sentences. Additionally, it includes parallel recordings of the sentences by 9 healthy speakers (6 males and 3 females) to facilitate speech processing tasks that require small parallel corpora, such as voice conversion or synthetic speech adaptation. Apart from the sentences, the database includes sustained vowels and a small set of isolated words, which can be valuable for research on ES analysis, diagnosis and evaluation. The paper describes the main contents of the database, the recording protocols and procedure, as well as the labeling process. The main acoustic characteristics of the voices, such as speaking rate, durations of the recordings, phones and silences, and other such characteristics are compared with those of a reduced set of healthy voices. In addition, we describe an experiment using the database to improve the performance of an ASR system for ES speakers. This new resource will be made available to the scientific community with the hope that it will be used to improve the quality of life of the laryngectomees. (C) 2020 Elsevier Ltd. All rights reserved.","2021-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","66","","","","","","","","","","English","","","","WOS:000596372000013","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;62</p>","","","ACCEPTABILITY; ACOUSTIC ANALYSIS; Esophageal speech; INTELLIGIBILITY; RATINGS; Speech analysis; Speech databases; Speech intelligibility; TRACHEOESOPHAGEAL SPEECH; Voice conversion; VOICE QUALITY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"HU4NKPZH","journalArticle","2023","Olah, J; Diederen, K; Gibbs-Dean, T; Kempton, MJ; Dobson, R; Spencer, T; Cummins, N","Online speech assessment of the psychotic spectrum: Exploring the relationship between overlapping acoustic markers of schizotypy, depression and anxiety","SCHIZOPHRENIA RESEARCH","","0920-9964","10.1016/j.schres.2023.03.044","","Background: Remote assessment of acoustic alterations in speech holds promise to increase scalability and validity in research across the psychosis spectrum. A feasible first step in establishing a procedure for online assessments is to assess acoustic alterations in psychometric schizotypy. However, to date, the complex relationship between alterations in speech related to schizotypy and those related to comorbid conditions such as symptoms of depression and anxiety has not been investigated. This study tested whether (1) depression, generalized anxiety and high psychometric schizotypy have similar voice characteristics, (2) which acoustic markers of online collected speech are the strongest predictors of psychometric schizotypy, (3) whether including generalized anxiety and depression symptoms in the model can improve the prediction of schizotypy. Methods: We collected cross-sectional, online-recorded speech data from 441 participants, assessing demographics, symptoms of depression, generalized anxiety and psychometric schizotypy. Results: Speech samples collected online could predict psychometric schizotypy, depression, and anxiety symptoms with weak to moderate predictive power, and with moderate and good predictive power when basic demographic variables were added to the models. Most influential features of these models largely overlapped. The predictive power of speech marker-based models of schizotypy significantly improved after including symptom scores of depression and generalized anxiety in the models (from R2 = 0.296 to R2 = 0. 436). Conclusions: Acoustic features of online collected speech are predictive of psychometric schizotypy as well as generalized anxiety and depression symptoms. The acoustic characteristics of schizotypy, depression and anxiety symptoms significantly overlap. Speech models that are designed to predict schizotypy or symptoms of the schizophrenia spectrum might therefore benefit from controlling for symptoms of depression and anxiety.","2023-09","2025-02-26 20:45:06","2025-02-26 20:45:06","","11-19","","","259","","","","","","","","","","English","","","","WOS:001103570600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;65</p>","","","ABNORMALITIES; Biomarkers; DISORDER; General population; GENERALIZED ANXIETY; LANGUAGE; NEGATIVE SYMPTOM SEVERITY; PEOPLE; PERFORMANCE; Psychometric schizotypy; PSYCHOMOTOR RETARDATION; SCHIZOPHRENIA; Speech analysis; VALIDITY; Voice abnormalities","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"FQ8CTQYX","journalArticle","2024","Hasimoglu, O; Altinkaya, A; Tuysuz, O; Hanoglu, T; Karacoban, TO; Geylan, NB; Barut, O; Basaran, R; Erkan, B; Guclu, O; Koksal, A; Kocak, B; Tugcu, B","The effect of subthalamic nucleus deep brain stimulation on speech performance: a prospective clinical and tractography study","ACTA NEUROCHIRURGICA","","0001-6268","10.1007/s00701-024-06257-x","","Background Speech changes significantly impact the quality of life for Parkinson's disease (PD) patients. Deep Brain Stimulation (DBS) of the Subthalamic Nucleus (STN) is a standard treatment for advanced PD, but its effects on speech remain unclear. This study aimed to investigate the relationship between STN-DBS and speech changes in PD patients using comprehensive clinical assessments and tractography. Methods Forty-seven PD patients underwent STN-DBS, with preoperative and 3-month postoperative assessments. Speech analyses included acoustic measurements, auditory-perceptual evaluations, and fluency-intelligibility tests. On the other hand, structures within the volume tissue activated (VTA) were identified using MRI and DTI. The clinical and demographic data and structures associated with VTA (Corticospinal tract, Internal capsule, Dentato-rubro-thalamic tract, Medial forebrain bundle, Medial lemniscus, Substantia nigra, Red nucleus) were compared with speech analyses. Results The majority of patients (36.2-55.4% good, 29.7-53.1% same) exhibited either improved or unchanged speech quality following STN-DBS. Only a small percentage (8.5-14.9%) experienced deterioration. Older patients and those with worsened motor symptoms postoperatively were more likely to experience negative speech changes (p < 0.05). Interestingly, stimulation of the right Substantia Nigra correlated with improved speech quality (p < 0.05). No significant relationship was found between other structures affected by VTA and speech changes. Conclusions This study suggests that STN-DBS does not predominantly negatively impact speech in PD patients, with potential benefits observed, especially in younger patients. These findings underscore the importance of individualized treatment approaches and highlight the need for further long-term studies to optimize therapeutic outcomes and better understand the effects of STN-DBS on speech.","2024-09-16","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","166","","","","","","","","","","English","","","","WOS:001314720600002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;37</p>","","","Diffusion Tensor Imaging; DYSARTHRIA; Hypokinetic Dysarthria; LEVODOPA; METAANALYSIS; Neuromodulation; Outcome; Parkinson's disease; PARKINSONS-DISEASE; SCALE; Speech Analysis; TRACT","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"CITVH8P7","journalArticle","2023","Kim, AY; Jang, EH; Lee, SH; Choi, KY; Park, JG; Shin, HC","Automatic Depression Detection Using Smartphone-Based Text-Dependent Speech Signals: Deep Convolutional Neural Network Approach","JOURNAL OF MEDICAL INTERNET RESEARCH","","1438-8871","10.2196/34474","","Background: Automatic diagnosis of depression based on speech can complement mental health treatment methods in the future. Previous studies have reported that acoustic properties can be used to identify depression. However, few studies have attempted a large-scale differential diagnosis of patients with depressive disorders using acoustic characteristics of non-English speakers. Objective: This study proposes a framework for automatic depression detection using large-scale acoustic characteristics based on the Korean language. Methods: We recruited 153 patients who met the criteria for major depressive disorder and 165 healthy controls without current or past mental illness. Participants' voices were recorded on a smartphone while performing the task of reading predefined text-based sentences. Three approaches were evaluated and compared to detect depression using data sets with text-dependent read speech tasks: conventional machine learning models based on acoustic features, a proposed model that trains and classifies log-Mel spectrograms by applying a deep convolutional neural network (CNN) with a relatively small number of parameters, and models that train and classify log-Mel spectrograms by applying well-known pretrained networks. Results: The acoustic characteristics of the predefined text-based sentence reading automatically detected depression using the proposed CNN model. The highest accuracy achieved with the proposed CNN on the speech data was 78.14%. Our results show that the deep-learned acoustic characteristics lead to better performance than those obtained using the conventional approach and pretrained models. Conclusions: Checking the mood of patients with major depressive disorder and detecting the consistency of objective descriptions are very important research topics. This study suggests that the analysis of speech data recorded while reading text-dependent sentences could help predict depression status automatically by capturing the characteristics of depression. Our method is smartphone based, is easily accessible, and can contribute to the automatic identification of depressive states.","2023-01-25","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","25","","","","","","","","","","English","","","","WOS:001009139500001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;13<br/>Total Times Cited:&nbsp;&nbsp;14<br/>Cited Reference Count:&nbsp;&nbsp;63</p>","","","acoustic; ADD; automatic depression detection; deep learning; depression; DIAGNOSIS; major depressive disorder; MDD; mobile health; mobile phone; RECOGNITION; smartphone; speech analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5RLKGC8W","journalArticle","2023","León, M; Washington, KN; McKenna, VS; Crowe, K; Fritz, K; Boyce, S","Characterizing Speech Sound Productions in Bilingual Speakers of Jamaican Creole and English : Application of Durational Acoustic Methods","JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH","","1092-4388","10.1044/2022_JSLHR-22-00304","","Purpose: This study examined the speech acoustic characteristics of Jamaican Creole (JC) and English in bilingual preschoolers and adults using acoustic duration measures. The aims were to determine if, for JC and English, (a) child and adult acoustic duration characteristics differ, (b) differences occur in pre-schoolers' duration patterns based on the language spoken, and (c) relationships exist between the preschoolers' personal contextual factors (i.e., age, sex, and percentage of language [%language] exposure and use) and acoustic duration.Method: Data for this cross-sectional study were collected in Kingston, Jamaica, and New York City, New York, United States, during 2013-2019. Par-ticipants included typically developing simultaneous bilingual preschoolers (n = 120, ages 3;4-5;11 [years;months]) and adults (n = 15, ages 19;0-54;4) from the same linguistic community. Audio recordings of single-word productions of JC and English were collected through elicited picture-based tasks and used for acoustic analysis. Durational features (voice onset time [VOT], vowel duration, whole-word duration, and the proportion of vowel to whole-word duration) were measured using Praat, a speech analysis software program.Results: JC-English-speaking children demonstrated developing speech motor control through differences in durational patterns compared with adults, includ-ing VOT for voiced plosives. Children's VOT, vowel duration, and whole-word duration were produced similarly across JC and English. The contextual factor %language use was predictive of vowel and whole-word duration in English.Conclusions: The findings from this study contribute to a foundation of under-standing typical bilingual speech characteristics and motor development as well as schema in JC-English speakers. In particular, minimal acoustic duration dif-ferences were observed across the post-Creole continuum, a feature that may be attributed to the JC-English bilingual environment.","2023-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","61-83","","1","66","","","","","","","","","","English","","","","WOS:000925631800005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;84</p>","","","ACQUISITION; CHILDREN; CONTEXT; CONTRAST; GENDER; INITIAL STOPS; LANGUAGE; VARIABILITY; VOICE ONSET TIME; VOWEL DURATION","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"76FFQK5T","journalArticle","2021","Verde, L; De Pietro, G; Ghoneim, A; Alrashoud, M; Al-Mutib, KN; Sannino, G","Exploring the Use of Artificial Intelligence Techniques to Detect the Presence of Coronavirus Covid-19 Through Speech and Voice Analysis","IEEE ACCESS","","2169-3536","10.1109/ACCESS.2021.3075571","","The Covid-19 pandemic represents one of the greatest global health emergencies of the last few decades with indelible consequences for all societies throughout the world. The cost in terms of human lives lost is devastating on account of the high contagiousness and mortality rate of the virus. Millions of people have been infected, frequently requiring continuous assistance and monitoring. Smart healthcare technologies and Artificial Intelligence algorithms constitute promising solutions useful not only for the monitoring of patient care but also in order to support the early diagnosis, prevention and evaluation of Covid-19 in a faster and more accurate way. On the other hand, the necessity to realise reliable and precise smart healthcare solutions, able to acquire and process voice signals by means of appropriate Internet of Things devices in real-time, requires the identification of algorithms able to discriminate accurately between pathological and healthy subjects. In this paper, we explore and compare the performance of the main machine learning techniques in terms of their ability to correctly detect Covid-19 disorders through voice analysis. Several studies report, in fact, significant effects of this virus on voice production due to the considerable impairment of the respiratory apparatus. Vocal folds oscillations that are more asynchronous, asymmetrical and restricted are observed during phonation in Covid-19 patients. Voice sounds selected by the Coswara database, an available crowd-sourced database, have been e analysed and processed to evaluate the capacity of the main ML techniques to distinguish between healthy and pathological voices. All the analyses have been evaluated in terms of accuracy, sensitivity, specificity, F1-score and Receiver Operating Characteristic area. These show the reliability of the Support Vector Machine algorithm to detect the Covid-19 infections, achieving an accuracy equal to about 97%.","2021","2025-02-26 20:45:06","2025-02-26 20:45:06","","65750-65757","","","9","","","","","","","","","","English","","","","WOS:000647306600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;38<br/>Total Times Cited:&nbsp;&nbsp;39<br/>Cited Reference Count:&nbsp;&nbsp;61</p>","","","AI; Artificial intelligence techniques; BIG DATA ANALYTICS; COVID-19; Covid-19~detection; Databases; Feature extraction; FRAMEWORK; FUSION; IDENTIFICATION; Machine learning algorithms; Medical services; Pathology; Reliability; speech analysis; voice analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RY3NNNXL","journalArticle","2022","van Noort-van der Spek, IL; Dudink, J; Reiss, IK; Franken, MCJP","Early Speech Sound Production and Its Trajectories in Very Preterm Children From 2 to 4 Years of Age","JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH","","1092-4388","10.1044/2021_JSLHR-21-00388","","Purpose: Very preterm (VPT) children are at risk for speech and language problems throughout school age. However, little is known about early speech sound production in these children. This study aims to present a detailed description of early speech sound production and its trajectories in VPT children from 2 to 4 years of age. In addition, this study aimed to determine if early speech sound production is associated with speech production and expressive language function at 4 years of age. Method: In 63 VPT children (< 32 weeks of gestation, 41 boys, mean gestational age = 28.8 weeks, mean birth weight = 1,135 g), speech sound production was assessed by naturalistic speech analysis at 2 years of corrected age and speech and language function by standardized tests at 4 years of age. Results: Speech sound production was found to be abnormal in 49% of the VPT children at 2 years of age and in 19% at 4 years of age. Four different speech production trajectories from 2 to 4 years of age could be identified: a normal trajectory, an abnormal trajectory, a catch-up trajectory, and a growing-into-deficit trajectory. Early speech production, defined by the number of acquired consonants at 2 years of age, significantly predicted the word production score at 4 years of age and the sentence production score at 4 years of age. Conclusions: Compared to the general population, an alarmingly high proportion of VPT children showed speech production problems at 2 years of age. About half of these children showed persistent speech problems at 4 years of age. Moreover, these problems were associated with expressive language problems at the age of 4 years.","2022-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","1294-1310","","4","65","","","","","","","","","","English","","","","WOS:000830953900005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;6<br/>Total Times Cited:&nbsp;&nbsp;6<br/>Cited Reference Count:&nbsp;&nbsp;65</p>","","","BORN; COGNITIVE-DEVELOPMENT; DELAY; EARLY VOCALIZATION; INFANTS; LANGUAGE-DEVELOPMENT; LOW-BIRTH-WEIGHT; NEONATAL INTENSIVE-CARE; PHONOLOGICAL DEVELOPMENT; RELIABILITY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCEB22HN","journalArticle","2023","Miled, ZB; Dexter, PR; Grout, RW; Boustani, M","Feature engineering from medical notes: A case study of dementia detection","HELIYON","","2405-8440","10.1016/j.heliyon.2023.e14636","","Background and objectives: Medical notes are narratives that describe the health of the patient in free text format. These notes can be more informative than structured data such as the history of medications or disease conditions. They are routinely collected and can be used to evaluate the patient's risk for developing chronic diseases such as dementia. This study investigates different methodologies for transforming routine care notes into dementia risk classifiers and evaluates the generalizability of these classifiers to new patients and new health care institutions.Methods: The notes collected over the relevant history of the patient are lengthy. In this study, TF-ICF is used to select keywords with the highest discriminative ability between at risk dementia patients and healthy controls. The medical notes are then summarized in the form of occurrences of the selected keywords. Two different encodings of the summary are compared. The first encoding consists of the average of the vector embedding of each keyword occurrence as pro-duced by the BERT or Clinical BERT pre-trained language models. The second encoding aggre-gates the keywords according to UMLS concepts and uses each concept as an exposure variable. For both encodings, misspellings of the selected keywords are also considered in an effort to improve the predictive performance of the classifiers. A neural network is developed over the first encoding and a gradient boosted trees model is applied to the second encoding. Patients from a single health care institution are used to develop all the classifiers which are then evaluated on held-out patients from the same health care institution as well as test patients from two other health care institutions.Results: The results indicate that it is possible to identify patients at risk for dementia one year ahead of the onset of the disease using medical notes with an AUC of 75% when a gradient boosted trees model is used in conjunction with exposure variables derived from UMLS concepts. However, this performance is not maintained with an embedded feature space and when the classifier is applied to patients from other health care institutions. Moreover, an analysis of the top predictors of the gradient boosted trees model indicates that different features inform the classification depending on whether or not spelling variants of the keywords are included.Conclusion: The present study demonstrates that medical notes can enable risk prediction models for complex chronic diseases such as dementia. However, additional research efforts are needed to improve the generalizability of these models. These efforts should take into consideration the","2023-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","3","9","","","","","","","","","","English","","","","WOS:000969231500001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;31</p>","","","BERT; Clinical BERT; COGNITIVE IMPAIRMENT; Dementia; EMR; Medical notes; UMLS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6UKIRZSC","journalArticle","2021","Petersen, JD; Wehberg, S; Packness, A; Svensson, NH; Hyldig, N; Raunsgaard, S; Andersen, MK; Ryg, J; Mercer, SW; Sondergaard, J; Waldorff, FB","Association of Socioeconomic Status With Dementia Diagnosis Among Older Adults in Denmark","JAMA NETWORK OPEN","","2574-3805","10.1001/jamanetworkopen.2021.10432","","IMPORTANCE Low socioeconomic status (SES) has been identified as a risk factor for the development of dementia. However, few studies have focused on the association between SES and dementia diagnostic evaluation on a population level. OBJECTIVE To investigate whether household income (HHI) is associated with dementia diagnosis and cognitive severity at the time of diagnosis. DESIGN, SETTING, AND PARTICIPANTS This population- and register-based cross-sectional study analyzed health, social, and economic data obtained from various Danish national registers. The study population comprised individuals who received a first-time referral for a diagnostic evaluation for dementia to the secondary health care sector of Denmark between January 1, 2017, and December 17, 2018. Dementia-related health data were retrieved from the Danish Quality Database for Dementia. Data analysis was conducted from October 2019 to December 2020. EXPOSURES Annual HHI (used as a proxy for SES) for 2015 and 2016 was obtained from Statistics Denmark and categorized into upper, middle, and lower tertiles within 5-year interval age groups. MAIN OUTCOMES AND MEASURES Dementia diagnoses (Alzheimer disease, vascular dementia, mixed dementia, dementia with Lewy bodies, Parkinson disease dementia, or other) and cognitive stages at diagnosis (cognitively intact; mild cognitive impairment but not dementia; or mild, moderate, or severe dementia) were retrieved from the database. Univariable and multivariable logistic and linear regressions adjusted for age group, sex, region of residence, household type, period (2017 and 2018), medication type, and medical conditions were analyzed for a possible association between HHI and receipt of dementia diagnosis. RESULTS Among the 10 191 individuals (mean [SD] age, 75 [10] years; 5476 women [53.7%]) included in the study, 8844 (86.8%) were diagnosed with dementia. Individuals with HHI in the upper tertile compared with those with lower-tertile HHI were less likely to receive a dementia diagnosis after referral (odds ratio, 0.65; 95% CI, 0.55-0.78) and, if diagnosed with dementia, had less severe cognitive stage (beta, -0.16; 95% CI, -0.21 to -0.10). Individuals with middle-tertile HHI did not significantly differ from those with lower-tertile HHI in terms of dementia diagnosis (odds ratio, 0.92; 95% CI, 0.77-1.09) and cognitive stage at diagnosis (beta, 0.01; 95% CI, -0.04 to 0.06). CONCLUSIONS AND RELEVANCE The results of this study revealed a social inequality in dementia diagnostic evaluation: in Denmark, people with higher income seem to receive an earlier diagnosis. Public health strategies should target people with lower SES for earlier dementia detection and intervention.","2021-05-18","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","5","4","","","","","","","","","","English","","","","WOS:000652671000007","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;59<br/>Total Times Cited:&nbsp;&nbsp;63<br/>Cited Reference Count:&nbsp;&nbsp;67</p>","","","ALZHEIMERS-DISEASE; CARE; DETERMINANTS; EDUCATION; ENVIRONMENTAL RISK-FACTORS; HEALTH; IMPACT; LIFE; METAANALYSIS; PREVALENCE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4TD5582R","journalArticle","2023","Cartz-Piver, L; Calvet, B; Mehrabian-Spassova, S; Raycheva, M; Rejdak, K; Papuk, E; Leperre-Desplanques, A; Krolak-Salmon, P","Empowering general practitioners in dementia care: The ANTISTIGMA education intervention in Europe","INTERNATIONAL JOURNAL OF GERIATRIC PSYCHIATRY","","0885-6230","10.1002/gps.5958","","Background: There is a well-documented lack or delay of diagnosis of dementia in all countries, including in Europe. Most general practitioners (GPs) have acquired adequate academic and scientific information about dementia but avoid using it in practice because of stigma. Objectives: To persuade GPs of their role in dementia detection, an Antistigma education intervention was designed, with teaching objectives focusing on ""Why"" and ""How"" to diagnosis and manage dementia, based on ethical and practical content, as opposed to classical training centered on teaching ""What"", with mainly academic content. Methods: During the European Joint Action ""ACT ON DEMENTIA"", the Antistigma education intervention was implemented in four Universities: Lyon and Limoges (France), Sofia (Bulgaria) and Lublin (Poland). General data, including information about training and experience in dementia, was collected. Specific scales measured Dementia Negative Stereotypes DNS and Dementia Clinical Confidence D-CO before and after training. Results: 134 GPs and 58 residents R completed the training. The participants were mainly women (74%), and the mean age was 42.8 +/- 13.2. Before training, participants expressed difficulties in defining GPs role and worries about inflicting Stigma, Risks of diagnosis, Lack of benefit and Communication difficulties. Participants' D-CO was significantly higher for Diagnosis process (64%) than for other clinical situations. After training, total NS was reduced from 34.2% to 29.9% (p < 0.001), and stereotypes were improved: GPs' role (40.1% reduced to 35.9%; p < 0.001), Stigma (38.7% reduced to 35.5%; p < 0.001), Risks of diagnosis (39.0% reduced to 33.3%; p < 0.001), Lack of Benefit (29.3% reduced to 24.6%; p < 0.001) and Communication difficulties (19.9% reduced to 16.9%; p < 0.001). After training, D-CO was significantly increased in all the clinical situations (p < 0.001), but stayed highest for Diagnosis Process. There was no significant difference between the universities. Participants who benefited best from the Antistigma education intervention were those without training in Geriatrics and those working in nursing homes (who reduced the most D-NS), as well younger participants and those who managed less than five people living with dementia per week (who increased the most D-CO). Conclusion: The Antistigma program is based on the idea that most often GPs and R have acquired adequate academic and scientific information about dementia but avoid using it in practice because of stigma. These results outline the importance of addressing ethical issues and practical management situations in dementia education, to empower GPs in dementia care.","2023-07","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","7","38","","","","","","","","","","English","","","","WOS:001021812700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;44</p>","","","ALZHEIMERS-DISEASE; ATTITUDES; dementia; DIAGNOSING DEMENTIA; diagnosis disclosure; education; MANAGEMENT; PERCEPTIONS; PHYSICIANS; PREVALENCE; primary care; stereotypes; stigma; STIGMA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R4RYU8CX","journalArticle","2022","Behrendt, T; Bielitzki, R; Behrens, M; Glazachev, OS; Schega, L","Effects of Intermittent Hypoxia-Hyperoxia Exposure Prior to Aerobic Cycling Exercise on Physical and Cognitive Performance in Geriatric Patients-A Randomized Controlled Trial","FRONTIERS IN PHYSIOLOGY","","1664-042X","10.3389/fphys.2022.899096","","Background: It was recently shown that intermittent hypoxic-hyperoxic exposure (IHHE) applied prior to a multimodal training program promoted additional improvements in cognitive and physical performance in geriatric patients compared to physical training only. However, there is a gap in the literature to which extent the addition of IHHE can enhance the effects of an aerobic training. Therefore, the aim of this study was to investigate the efficacy of IHHE applied prior to aerobic cycling exercise on cognitive and physical performance in geriatric patients.Methods: In a randomized, two-armed, controlled, and single-blinded trial, 25 geriatric patients (77-94 years) were assigned to two groups: intervention group (IG) and sham control group (CG). Both groups completed 6 weeks of aerobic training using a motorized cycle ergometer, three times a week for 20 min per day. The IG was additionally exposed to intermittent hypoxic and hyperoxic periods for 30 min prior to exercise. The CG followed the similar procedure breathing sham hypoxia and hyperoxia (i.e., normoxia). Within 1 week before and after the interventions, cognitive performance was assessed with the Dementia-Detection Test (DemTect) and the Clock Drawing Test (CDT), while physical performance was measured using the Timed ""Up and Go"" Test (TUG) and the Short-Physical-Performance-Battery (SPPB).Results: No interaction effect was found with respect to the DemTect (eta(2)(p) = 0.02). An interaction effect with medium effect size (eta(2)(p) = 0.08) was found for CDT performance with a higher change over time for IG (d = 0.57) compared to CG (d = 0.05). The ANCOVA with baseline-adjustment indicated between-group differences with a large and medium effect size at post-test for the TUG (eta(2)(p) = 0.29) and SPPB (eta(2)(p) = 0.06) performance, respectively, in favour of the IG. Within-group post-hoc analysis showed that the TUG performance was worsened in the CG (d = 0.65) and remained unchanged in the IG (d = 0.19). Furthermore, SPPB performance was increased (d = 0.58) in IG, but no relevant change over time was found for CG (d = 0.00).Conclusion: The current study suggests that an additional IHHE prior to aerobic cycling exercise seems to be more effective to increase global cognitive functions as well as physical performance and to preserve functional mobility in geriatric patients in comparison to aerobic exercise alone after a 6-week intervention period.","2022-05-26","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","13","","","","","","","","","","English","","","","WOS:000808249300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;13<br/>Total Times Cited:&nbsp;&nbsp;15<br/>Cited Reference Count:&nbsp;&nbsp;164</p>","","","aging; ALZHEIMERS-DISEASE; CARDIORESPIRATORY FITNESS; cognitive impaiment; dementia; elderly; EXECUTIVE FUNCTION; GAIT SPEED; hypoxic conditioning; LOWER-EXTREMITY; MINI-MENTAL-STATE; MUSCLE STRENGTH; OBSTRUCTIVE SLEEP-APNEA; OLDER-ADULTS; QUALITY-OF-LIFE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XF934X3M","journalArticle","2023","Estrada-Rojas, K; Ortiz, NPC","Increased Improvement in Speech-Language Skills After Transcranial Photobiomodulation Plus Speech-Language Therapy, Compared to Speech-Language Therapy Alone: Case Report with Aphasia","PHOTOBIOMODULATION PHOTOMEDICINE AND LASER SURGERY","","2578-5478","10.1089/photob.2022.0024","","Objective: This is a case report showing that transcranial photobiomodulation (tPBM) combined with traditional, speech-language therapy improved and accelerated the results from speech-language therapy, in a stroke person with aphasia (PWA).Background: tPBM is a safe, noninvasive technique using red and near-infrared light to improve the metabolism of cells. tPBM helps by promoting neuromodulation, while decreasing neuroinflammation and promoting vasodilation. Several studies have shown that tPBM can help individuals with stroke or traumatic brain injury achieve significant cognitive improvements.Methods: A 38-year-old female, who sustained an ischemic stroke on the left side of the brain, received two, 5-month series of treatments. The first series of treatments included traditional speech-language therapy, for the first 5 months poststroke. The second series of treatments included tPBM in combination with speech-language therapy, for the next 5 months. The tPBM treatments included application of red (630 and 660 nm) and near-infrared (850 nm) wavelengths of photons applied to left hemisphere scalp areas. The major cortical language areas were subjacent to the scalp placements along the line of the Sylvian fissure. At each session, first a light-emitting diode (LED) cluster head with red (630 and 660 nm) and near-infrared (850 nm) wavelengths, with an irradiance (power density) of 200 mW/cm(2), a beam size of 4.9 cm(2), and a fluence (energy density) of 12 J/cm(2) per minute, was applied to the left side of the scalp/brain, along the Sylvian fissure for 60 sec at each at the following eight, language network target areas: frontal pole, prefrontal cortex, and inferior frontal gyrus (Broca's area); supramarginal gyrus and angular gyrus in the parietal lobe; inferior motor/sensory cortex (mouth area); and posterior superior temporal gyrus (Wernicke's area) and superior temporal sulcus in the temporal lobe, for a total of 8 min. Second, for the next 20 min (1200 sec), simultaneous with speech-language therapy, an LED PBM helmet was applied to the scalp/head. This helmet contained 256 separate LED lights, near-infrared (810 nm) wavelength, 60 mW power per LED light, total power, 15 W; energy, 72 Joules; fluence, 28.8 J/cm(2); and irradiance, 24 mW/cm(2).Results and Conclusion: During the initial, 5-month treatment series with traditional speech-language therapy only, there was little to no improvement in dysarthria and expressive language. During the second, 5-month treatment series, however, with tPBM applied first, to the left hemisphere only, and second, to both hemispheres during each session plus simultaneous speech-language therapy, there was marked improvement in the dysarthria and expressive language. After the first 5-month series, this PWA had utilized a slow rate of speech with a production of similar to 25 to 30 words-per-minute during conversations and spontaneous speech. Utterance length was only 4-6 words with simple, grammatical structure. After the second, 5-month series of treatment combining tPBM plus speech-language therapy, the rate of speech increased to 80+ words-per-minute and utterance length was increased to 9-10 words, with more complex grammatical structure.","2023-05-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","234-240","","5","41","","","","","","","","","","English","","","","WOS:000960541300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;21</p>","","","aphasia; LASER THERAPY; low-level laser therapy (LLLT); photobiomodulation (PBM); SAFETY; speech-language therapy; TRAUMATIC BRAIN-INJURY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"T6MFRQSR","journalArticle","2024","Monti, E; D'Andrea, W; Carroll, LM; Norton, K; Miron, N; Resto, O; Toscano, K; Williams, J; Harris, D; Irene, L; Maass, A","Can perpetrators discern survivors from voice?","EUROPEAN JOURNAL OF PSYCHOTRAUMATOLOGY","","2000-8198","10.1080/20008066.2024.2358681","","Background: Research has shown that potential perpetrators and individuals high in psychopathic traits tend to body language cues to target a potential new victim. However, whether targeting occurs also by tending to vocal cues has not been examined. Thus, the role of voice in interpersonal violence merits investigation.Objective: In two studies, we examined whether perpetrators could differentiate female speakers with and without sexual and physical assault histories (presented as rating the degree of 'vulnerability' to victimization).Methods: Two samples of male listeners (sample one N = 105, sample two, N = 109) participated. Each sample rated 18 voices (9 survivors and 9 controls). Listener sample one heard spontaneous speech, and listener sample two heard the second sentence of a standardized passage. Listeners' self-reported psychopathic traits and history of previous perpetration were measured.Results: Across both samples, history of perpetration (but not psychopathy) predicted accuracy in distinguishing survivors of assault.Conclusions: These findings highlight the potential role of voice in prevention and intervention. Gaining a further understanding of what voice cues are associated with accuracy in discerning survivors can also help us understand whether or not specialized voice training could have a role in self-defense practices. We examined whether listeners with history of perpetration could differentiate female speakers with and without assault histories (presented as rating the degree of 'vulnerability' to victimization).Listeners' higher history of perpetration was associated with higher accuracy in differentiating survivors of assault from non-survivors.These findings highlight that voice could have a crucial role in prevention and intervention. Antecedentes: Las investigaciones han mostrado que los perpetradores potenciales y las personas con altos rasgos psicop & aacute;ticos tienden a detectar se & ntilde;ales del lenguaje corporal para apuntar o identificar a una nueva v & iacute;ctima potencial. Sin embargo, no se ha examinado si esta identificaci & oacute;n se produce tambi & eacute;n considerando se & ntilde;ales vocales. Por tanto, el papel de la voz en la violencia interpersonal amerita investigaci & oacute;n.Objetivo: En dos estudios, examinamos si los perpetradores pod & iacute;an diferenciar a las hablantes mujeres con y sin antecedentes de agresi & oacute;n f & iacute;sica y sexual (presentado como una calificaci & oacute;n del grado de 'vulnerabilidad' a la victimizaci & oacute;n).M & eacute;todos: Participaron dos muestras de oyentes hombres (muestra uno N = 105, muestra dos, N = 109). Cada muestra calific & oacute; 18 voces (9 supervivientes y 9 controles). El oyente de la muestra uno escuch & oacute; un discurso espont & aacute;neo y el oyente de la muestra dos escuch & oacute; la segunda oraci & oacute;n de un pasaje estandarizado. Se midieron los rasgos psicop & aacute;ticos autoinformados de los oyentes y el historial de perpetraci & oacute;n previa.Resultados: En ambas muestras, los antecedentes de perpetraci & oacute;n (pero no la psicopat & iacute;a) predijeron la precisi & oacute;n para distinguir a los sobrevivientes de agresi & oacute;n.Conclusiones: Estos hallazgos resaltan el papel potencial de la voz en la prevenci & oacute;n y la intervenci & oacute;n. Comprender mejor qu & eacute; se & ntilde;ales de voz est & aacute;n asociadas, con precisi & oacute;n, a la hora de discernir a los supervivientes tambi & eacute;n puede ayudarnos a comprender si el entrenamiento especializado de la voz podr & iacute;a tener o no un papel en las pr & aacute;cticas de autodefensa.","2024-12-31","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","15","","","","","","","","","","English","","","","WOS:001249816100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;54</p>","","","agresi & oacute; assault; ASSAULT; CHILDHOOD TRAUMA; DISSOCIATION; n; perpetraci & oacute; perpetration; PSYCHOPATHIC TRAITS; PTSD SYMPTOMS; RELIABILITY; REVICTIMIZATION; RISK; trauma; victimizaci & oacute; victimization; VICTIMS; VIOLENCE; Voice; Voz","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"YR5R8943","journalArticle","2023","Wang, YR; Wang, PC; Adams, LC; Sheybani, ND; Qu, LQ; Sarrami, AH; Theruvath, AJ; Gatidis, S; Ho, TA; Zhou, Q; Pribnow, A; Thakor, AS; Rubin, D; Daldrup-Link, HE","Low-count whole-body PET/MRI restoration: an evaluation of dose reduction spectrum and five state-of-the-art artificial intelligence models","EUROPEAN JOURNAL OF NUCLEAR MEDICINE AND MOLECULAR IMAGING","","1619-7070","10.1007/s00259-022-06097-w","","Purpose To provide a holistic and complete comparison of the five most advanced AI models in the augmentation of low-dose F-18-FDG PET data over the entire dose reduction spectrum.Methods In this multicenter study, five AI models were investigated for restoring low-count whole-body PET/MRI, covering convolutional benchmarks - U-Net, enhanced deep super-resolution network (EDSR), generative adversarial network (GAN) - and the most cutting-edge image reconstruction transformer models in computer vision to date - Swin transformer image restoration network (SwinIR) and EDSR-ViT (vision transformer). The models were evaluated against six groups of count levels representing the simulated 75%, 50%, 25%, 12.5%, 6.25%, and 1% (extremely ultra-low-count) of the clinical standard 3 MBq/kg F-18-FDG dose. The comparisons were performed upon two independent cohorts - (1) a primary cohort from Stanford University and (2) a cross-continental external validation cohort from Tubingen University - in order to ensure the findings are generalizable. A total of 476 original count and simulated low-count whole-body PET/MRI scans were incorporated into this analysis.Results nFor low-count PET restoration on the primary cohort, the mean structural similarity index (SSIM) scores for dose 6.25% were 0.898 (95% CI, 0.887-0.910) for EDSR, 0.893 (0.881-0.905) for EDSR-ViT, 0.873 (0.859-0.887) for GAN, 0.885 (0.873-0.898) for U-Net, and 0.910 (0.900-0.920) for SwinIR. In continuation, SwinIR and U-Net's performances were also discreetly evaluated at each simulated radiotracer dose levels. Using the primary Stanford cohort, the mean diagnostic image quality (DIQ; 5-point Likert scale) scores of SwinIR restoration were 5 (SD, 0) for dose 75%, 4.50 (0.535) for dose 50%, 3.75 (0.463) for dose 25%, 3.25 (0.463) for dose 12.5%, 4 (0.926) for dose 6.25%, and 2.5 (0.534) for dose 1%.Conclusion Compared to low-count PET images, with near-to or nondiagnostic images at higher dose reduction levels (up to 6.25%), both SwinIR and U-Net significantly improve the diagnostic quality of PET images. A radiotracer dose reduction to 1% of the current clinical standard radiotracer dose is out of scope for current AI techniques.","2023-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","1337-1350","","5","50","","","","","","","","","","English","","","","WOS:000913086400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;13<br/>Total Times Cited:&nbsp;&nbsp;13<br/>Cited Reference Count:&nbsp;&nbsp;51</p>","","","CHILDREN; CNN; Deep learning; IMAGE-RECONSTRUCTION; PET; PET restoration; RADIATION; Transformer model; Whole-body PET imaging","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R9L7P5AJ","journalArticle","2025","Gowthamy, J; Ramesh, SSS","Augmented histopathology: Enhancing colon cancer detection through deep learning and ensemble techniques","MICROSCOPY RESEARCH AND TECHNIQUE","","1059-910X","10.1002/jemt.24692","","Colon cancer poses a significant threat to human life with a high global mortality rate. Early and accurate detection is crucial for improving treatment quality and the survival rate. This paper presents a comprehensive approach to enhance colon cancer detection and classification. The histopathological images are gathered from the CRC-VAL-HE-7K dataset. The images undergo preprocessing to improve quality, followed by augmentation to increase dataset size and enhance model generalization. A deep learning based transformer model is designed for efficient feature extraction and enhancing classification by incorporating a convolutional neural network (CNN). A cross-transformation model captures long-range dependencies between regions, and an attention mechanism assigns weights to highlight crucial features. To boost classification accuracy, a Siamese network distinguishes colon cancer tissue classes based on probabilities. Optimization algorithms fine-tune model parameters, categorizing colon cancer tissues into different classes. The multi-class classification performance is evaluated in the experimental evaluation, which demonstrates that the proposed model provided highest accuracy rate of 98.84%. In this research article, the proposed method achieved better performance in all analyses by comparing with other existing methods.Research Highlights Deep learning-based techniques are proposed. DL methods are used to enhance colon cancer detection and classification. CRC-VAL-HE-7K dataset is utilized to enhance image quality. Hybrid particle swarm optimization (PSO) and dwarf mongoose optimization (DMO) are used. The deep learning models are tuned by implementing the PSO-DMO algorithm. Advanced techniques: explore the integration of cross transformers, attention mechanisms, and Siamese networks to enhance feature extraction capabilities, enabling the model to capture intricate patterns within histopathological images. Improved classification: by leveraging these advanced deep learning techniques, the improvement is provided to accuracy and colon cancer tissue classification that ultimately benefits both patients and healthcare professionals. Reduced interobserver variability: the proposed research endeavors to reduce the subjectivity associated with manual diagnosis by providing an automated and consistent approach to colon cancer diagnosis. Benchmarking: the quantitative analyses are conducted with diverse performance evaluation measures for evaluating performance of the proposed model and benchmarks are used to assess the effectiveness of the proposed model in clinical settings. Evaluation outcome: multiple classes are categorized from the CRC-VAL-HE-7K dataset by fine-tuning the parameters of the deep learning model.image","2025-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","298-314","","1","88","","","","","","","","","","English","","","","WOS:001321813600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;33</p>","","","attention mechanisms; clinical significance; colon cancer; computational pathology; cross transformers; deep learning models; ensemble learning; feature extraction; histopathological images; multi-class classification; siamese networks","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JPFBXJ4Z","journalArticle","2024","De, SG; Singh, A; Tiwari, V; Patel, H; Vivekananda, GN; Rajput, DS","SLiTRANet: An EEG-Based Automated Diagnosis Framework for Major Depressive Disorder Monitoring Using a Novel LGCN and Transformer-Based Hybrid Deep Learning Approach","IEEE ACCESS","","2169-3536","10.1109/ACCESS.2024.3493140","","Major depressive disorder (MDD) is a mental ailment marked by a loss of interest in activities, persistent depression, and hopelessness. MDD has been on the rise in society in recent decades for varied reasons and has spurred suicidal tendencies among individuals. Early detection, continuous monitoring, and effective treatment are crucial for its impact on quality of life and society. EEG signal models the brain's electrical activities and has emerged as a potential tool to assess the depression status of a person. Due to advancements in sensor technology, fast, convenient, and cost-effective EEG acquisition is now possible, resulting in many EEG-based healthcare monitoring applications in recent years. This work proposes an EEG-headset-based smart monitoring system for real-time diagnosis of MDD in the Internet of Medical Things (IoMT) framework. In this study, we proposed a novel Linear Graph Convolution Network-Transformer-based deep learning approach for categorizing MDD through a time-frequency analysis of EEG signals. The Stockwell transform (S-transform) is employed to exploit the spectro-temporal information from the EEG and the resulting 2D representation is then fed into customized Linear Graph Convolution Network for MDD detection. We have utilized the Weighted Focal Binary Hinge Loss function, specifically designed for customized Linear Graph Convolution Network, to improve learning and handle unbalanced input. Subsequently, a novel Transformer model is designed to refine the MDD classification further. The proposed methodology named SLiTRANet, blends spectral analysis with the S-transform, graph-based learning with Linear Graph Convolution Network, and the sequence modeling capability of the Transformer. The proposed SLiTRANet model can be further integrated within an IoMT framework for automated real-time MDD diagnosis using EEG signals. The proposed methodology is evaluated on two publicly available datasets, MODMA and HUSM datasets. The evaluation results demonstrate the superior performance of the proposed SLiTRANet framework against the existing pre-trained and hybrid deep learning models, achieving remarkable accuracy, sensitivity, specificity, and precision rates of 99.92%, 99.90%, 99.95%, and 99.97%, respectively on HUSM dataset followed by an equally good performance on MODMA dataset with an accuracy of 99.68%. The proposed comprehensive approach implemented on two varied datasets highlights significant advancements in depression detection by outperforming state-of-art approaches.","2024","2025-02-26 20:45:06","2025-02-26 20:45:06","","173109-173126","","","12","","","","","","","","","","English","","","","WOS:001362127900029","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;44</p>","","","depression classification; EEG signals; FEATURES; healthcare monitoring; hybrid deep learning network; IoMT; linear graph convolution network","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"K3HUQNSK","journalArticle","2023","Yu, L; Xiang, W; Fang, J; Chen, YPP; Chi, LH","eX-ViT: A Novel explainable vision transformer for weakly supervised semantic segmentation *","PATTERN RECOGNITION","","0031-3203","10.1016/j.patcog.2023.109666","","Recently vision transformer models have become prominent models for a multitude of vision tasks. These models, however, are usually opaque with weak feature interpretability, making their predictions inacces-sible to the users. While there has been a surge of interest in the development of post-hoc solutions that explain model decisions, these methods can not be broadly applied to different transformer architectures, as rules for interpretability have to change accordingly based on the heterogeneity of data and model structures. Moreover, there is no method currently built for an intrinsically interpretable transformer, which is able to explain its reasoning process and provide a faithful explanation. To close these crucial gaps, we propose a novel vision transformer dubbed the eXplainable Vision Transformer (eX-ViT), an in-trinsically interpretable transformer model that is able to jointly discover robust interpretable features and perform the prediction. Specifically, eX-ViT is composed of the Explainable Multi-Head Attention (E-MHA) module, the Attribute-guided Explainer (AttE) module with the self-supervised attribute-guided loss. The E-MHA tailors explainable attention weights that are able to learn semantically interpretable representations from tokens in terms of model decisions with noise robustness. Meanwhile, AttE is pro-posed to encode discriminative attribute features for the target object through diverse attribute discov-ery, which constitutes faithful evidence for the model predictions. Additionally, we have developed a self-supervised attribute-guided loss for our eX-ViT architecture, which utilizes both the attribute dis-criminability mechanism and the attribute diversity mechanism to enhance the quality of learned repre-sentations. As a result, the proposed eX-ViT model can produce faithful and robust interpretations with a variety of learned attributes. To verify and evaluate our method, we apply the eX-ViT to several weakly supervised semantic segmentation (WSSS) tasks, since these tasks typically rely on accurate visual expla-nations to extract object localization maps. Particularly, the explanation results obtained via eX-ViT are regarded as pseudo segmentation labels to train WSSS models. Comprehensive simulation results illus-trate that our proposed eX-ViT model achieves comparable performance to supervised baselines, while surpassing the accuracy and interpretability of state-of-the-art black-box methods using only image-level labels. & COPY; 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ )","2023-10","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","142","","","","","","","","","","English","","","","WOS:001015402400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;19<br/>Total Times Cited:&nbsp;&nbsp;19<br/>Cited Reference Count:&nbsp;&nbsp;40</p>","","","Attention map; Explainable; Transformer; Weakly supervised","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MI4G8N9F","journalArticle","2022","Cheatham, S; Kummervold, PE; Parisi, L; Lanfranchi, B; Croci, I; Comunello, F; Rota, MC; Filia, A; Tozzi, AE; Rizzo, C; Gesualdo, F","Understanding the vaccine stance of Italian tweets and addressing language changes through the COVID-19 pandemic: Development and validation of a machine learning model","FRONTIERS IN PUBLIC HEALTH","","2296-2565","10.3389/fpubh.2022.948880","","Social media is increasingly being used to express opinions and attitudes toward vaccines. The vaccine stance of social media posts can be classified in almost real-time using machine learning. We describe the use of a Transformer-based machine learning model for analyzing vaccine stance of Italian tweets, and demonstrate the need to address changes over time in vaccine-related language, through periodic model retraining. Vaccine-related tweets were collected through a platform developed for the European Joint Action on Vaccination. Two datasets were collected, the first between November 2019 and June 2020, the second from April to September 2021. The tweets were manually categorized by three independent annotators. After cleaning, the total dataset consisted of 1,736 tweets with 3 categories (promotional, neutral, and discouraging). The manually classified tweets were used to train and test various machine learning models. The model that classified the data most similarly to humans was XLM-Roberta-large, a multilingual version of the Transformer-based model RoBERTa. The model hyper-parameters were tuned and then the model ran five times. The fine-tuned model with the best F-score over the validation dataset was selected. Running the selected fine-tuned model on just the first test dataset resulted in an accuracy of 72.8% (F-score 0.713). Using this model on the second test dataset resulted in a 10% drop in accuracy to 62.1% (F-score 0.617), indicating that the model recognized a difference in language between the datasets. On the combined test datasets the accuracy was 70.1% (F-score 0.689). Retraining the model using data from the first and second datasets increased the accuracy over the second test dataset to 71.3% (F-score 0.713), a 9% improvement from when using just the first dataset for training. The accuracy over the first test dataset remained the same at 72.8% (F-score 0.721). The accuracy over the combined test datasets was then 72.4% (F-score 0.720), a 2% improvement. Through fine-tuning a machine-learning model on task-specific data, the accuracy achieved in categorizing tweets was close to that expected by a single human annotator. Regular training of machine-learning models with recent data is advisable to maximize accuracy.","2022-07-29","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","10","","","","","","","","","","English","","","","WOS:000844631300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;64</p>","","","artificial intelligence; machine learning; REFUSAL; Transformer model; UNITED-STATES; vaccination hesitancy; vaccines","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"53M7XKWX","journalArticle","2022","Sun, H; Depraetere, K; Meesseman, L; Silva, PC; Szymanowsky, R; Fliegenschmidt, J; Hulde, N; von Dossow, V; Vanbiervliet, M; De Baerdemaeker, J; Roccaro-Waldmeyer, DM; Stieg, J; Hidalgo, MD; Dahlweid, FM","Machine Learning-Based Prediction Models for Different Clinical Risks in Different Hospitals: Evaluation of Live Performance","JOURNAL OF MEDICAL INTERNET RESEARCH","","1438-8871","10.2196/34295","","Background: Machine learning algorithms are currently used in a wide array of clinical domains to produce models that can predict clinical risk events. Most models are developed and evaluated with retrospective data, very few are evaluated in a clinical workflow, and even fewer report performances in different hospitals. In this study, we provide detailed evaluations of clinical risk prediction models in live clinical workflows for three different use cases in three different hospitals. Objective: The main objective of this study was to evaluate clinical risk prediction models in live clinical workflows and compare their performance in these setting with their performance when using retrospective data. We also aimed at generalizing the results by applying our investigation to three different use cases in three different hospitals. Methods: We trained clinical risk prediction models for three use cases (ie, delirium, sepsis, and acute kidney injury) in three different hospitals with retrospective data. We used machine learning and, specifically, deep learning to train models that were based on the Transformer model. The models were trained using a calibration tool that is common for all hospitals and use cases. The models had a common design but were calibrated using each hospital's specific data. The models were deployed in these three hospitals and used in daily clinical practice. The predictions made by these models were logged and correlated with the diagnosis at discharge. We compared their performance with evaluations on retrospective data and conducted cross-hospital evaluations. Results: The performance of the prediction models with data from live clinical workflows was similar to the performance with retrospective data. The average value of the area under the receiver operating characteristic curve (AUROC) decreased slightly by 0.6 percentage points (from 94.8% to 94.2% at discharge). The cross-hospital evaluations exhibited severely reduced performance: the average AUROC decreased by 8 percentage points (from 94.2% to 86.3% at discharge), which indicates the importance of model calibration with data from the deployment hospital. Conclusions: Calibrating the prediction model with data from different deployment hospitals led to good performance in live settings. The performance degradation in the cross-hospital evaluation identified limitations in developing a generic model for different hospitals. Designing a generic process for model development to generate specialized prediction models for each hospital guarantees model performance in different hospitals.","2022-06-07","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","6","24","","","","","","","","","","English","","","","WOS:000966317200002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;13<br/>Total Times Cited:&nbsp;&nbsp;15<br/>Cited Reference Count:&nbsp;&nbsp;33</p>","","","acute kidney injury; algorithm; clinical risk prediction; delirium; EHR; electronic health record; HEALTH; INTEROPERABILITY; kidney; live clinical workflow; machine learning; model; model evaluation; prediction; risk; scalability; sepsis; VALIDATION; workflow","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"R47K9F3W","journalArticle","2024","Fayad, I; Ciais, P; Schwartz, M; Wigneron, JP; Baghdadi, N; de Truchis, A; d'Aspremont, A; Frappart, F; Saatchi, S; Sean, E; Pellissier-Tanon, A; Bazzi, H","Hy-TeC: a hybrid vision transformer model for high-resolution and large-scale mapping of canopy height","REMOTE SENSING OF ENVIRONMENT","","0034-4257","10.1016/j.rse.2023.113945","","Accurate and timely monitoring of forest canopy height is critical for assessing forest dynamics, biodiversity, carbon sequestration as well as forest degradation and deforestation. Recent advances in deep learning techniques, coupled with the vast amount of spaceborne remote sensing data offer an unprecedented opportunity to map canopy height at high spatial and temporal resolutions. Current techniques for wall-to-wall canopy height mapping correlate remotely sensed information from optical and radar sensors in the 2D space to the vertical structure of trees using lidar's 3D measurement abilities serving as height proxies. While studies making use of deep learning algorithms have shown promising performances for the accurate mapping of canopy height, they have limitations due to the type of architectures and loss functions employed. Moreover, mapping canopy height over tropical forests remains poorly studied, and the accurate height estimation of tall canopies is a challenge due to signal saturation from optical and radar sensors, persistent cloud cover, and sometimes limited penetration capabilities of lidar instruments. In this study, we map heights at 10 m resolution across the diverse landscape of Ghana with a new vision transformer (ViT) model, dubbed Hy-TeC, optimized concurrently with a classification (discrete) and a regression (continuous) loss function. This model achieves significantly higher accuracy than previously employed convolutional-based approaches (ConvNets) optimized with only a continuous loss function. Hy-TeC results show that our proposed discrete/continuous loss formulation significantly increases the sensitivity for very tall trees (i.e., > 35 m). Overall, Hy-TeC has significantly reduced bias (0.8 m) and higher accuracy (RMSE = 6.6 m) over tropical forests for which other approaches show poorer performance and oftentimes a saturation effect. The height maps generated by Hy-TeC also have better ground sampling distance and better sensitivity to sparse vegetation. Over these areas, Hy-TeC showed an RMSE of 3.1 m in comparison to a reference dataset while the baseline ConvNet model had an RMSE of 4.3 m. Hy-TeC, which was used to generate a height map of Ghana using free and open access remotely sensed data with Sentinel-2 and Sentinel-1 images as predictors and GEDI height measurements as calibration data, has the potential to be used globally.","2024-03-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","302","","","","","","","","","","English","","","","WOS:001155283600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;19<br/>Total Times Cited:&nbsp;&nbsp;19<br/>Cited Reference Count:&nbsp;&nbsp;97</p>","","","ABOVEGROUND BIOMASS; BACKSCATTER; Canopy height; CLIMATE; FORESTS; GEDI; IMAGES; PERFORMANCE; Sentinel-1; Sentinel-2; VEGETATION; Vision transformers, deep learning, knowledge distillation","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ABJ67EL9","journalArticle","2021","Karystianis, G; Cabral, RC; Han, SC; Poon, J; Butler, T","Utilizing Text Mining, Data Linkage and Deep Learning in Police and Health Records to Predict Future Offenses in Family and Domestic Violence","FRONTIERS IN DIGITAL HEALTH","","2673-253X","10.3389/fdgth.2021.602683","","Family and Domestic violence (FDV) is a global problem with significant social, economic, and health consequences for victims including increased health care costs, mental trauma, and social stigmatization. In Australia, the estimated annual cost of FDV is $22 billion, with one woman being murdered by a current or former partner every week. Despite this, tools that can predict future FDV based on the features of the person of interest (POI) and victim are lacking. The New South Wales Police Force attends thousands of FDV events each year and records details as fixed fields (e.g., demographic information for individuals involved in the event) and as text narratives which describe abuse types, victim injuries, threats, including the mental health status for POIs and victims. This information within the narratives is mostly untapped for research and reporting purposes. After applying a text mining methodology to extract information from 492,393 FDV event narratives (abuse types, victim injuries, mental illness mentions), we linked these characteristics with the respective fixed fields and with actual mental health diagnoses obtained from the NSW Ministry of Health for the same cohort to form a comprehensive FDV dataset. These data were input into five deep learning models (MLP, LSTM, Bi-LSTM, Bi-GRU, BERT) to predict three FDV offense types (""hands-on,"" ""hands-off,"" ""Apprehended Domestic Violence Order (ADVO) breach""). The transformer model with BERT embeddings returned the best performance (69.00% accuracy; 66.76% ROC) for ""ADVO breach"" in a multilabel classification setup while the binary classification setup generated similar results. ""Hands-off"" offenses proved the hardest offense type to predict (60.72% accuracy; 57.86% ROC using BERT) but showed potential to improve with fine-tuning of binary classification setups. ""Hands-on"" offenses benefitted least from the contextual information gained through BERT embeddings in which MLP with categorical embeddings outperformed it in three out of four metrics (65.95% accuracy; 78.03% F1-score; 70.00% precision). The encouraging results indicate that future FDV offenses can be predicted using deep learning on a large corpus of police and health data. Incorporating additional data sources will likely increase the performance which can assist those working on FDV and law enforcement to improve outcomes and better manage FDV events.","2021-02-17","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","3","","","","","","","","","","English","","","","WOS:001034069100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;53</p>","","","big data; data linkage; deep learning; family and domestic violence; health records; INTIMATE PARTNER VIOLENCE; MENTAL-ILLNESS; PEOPLE; predictive analytics; text mining; VICTIMS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UJ43L89P","journalArticle","2025","You, KS; Wang, PZ; Huang, P; Gu, YK","A sound-vibration physical-information fusion constraint-guided deep learning method for rolling bearing fault diagnosis","RELIABILITY ENGINEERING & SYSTEM SAFETY","","0951-8320","10.1016/j.ress.2024.110556","","Although current deep learning models for bearing fault diagnosis have achieved excellent accuracy, the lack of constraint-guided learning of the physical mechanisms of real bearing failures and a physically scientific training paradigm leads to low interpretability and unreliability of intelligent fault diagnosis models. In this study, a sound-vibration physical-information fusion constraint-guided (PFCG) deep learning (DL) method is proposed, aiming at weighted fusion of sound and vibration multi-physical information into a deep learning model, to guide the DL model to learn more realistic physical laws of bearing failure. Firstly, a 15-degree-of-freedom nonlinear dynamics model of multi-stage degraded bearing failure mechanism with sound-vibration response is developed, which considers the evolutionary mechanism of bearing failure from healthy state to different stages, and utilizes a particle filtering algorithm for dynamic calibration of hidden parameters. Moreover, a lightweight DL fault diagnosis model is designed to realize the deep interaction between the physical model and the DL model through the weighted fusion of the cross-entropy loss function, physical consistency loss and uncertainty loss. Moreover, the superior diagnostic performance of the proposed sound and vibration PFCG-DL model is verified by comparing the performance fluctuations and parameter attributes of different DL benchmark models before and after being guided by physical information fusion constraints (PFCG). Eventually, the proposed PFCGTransformer model achieves a diagnostic accuracy of 99.45% while keeping the number of parameters at only 0.62M, which significantly improves the accuracy and reduces the computational complexity by 81.5% compared to the CAME-Transformer model's 3.24 M number of parameters and 95.00% diagnostic accuracy. In addition, the test time of PFCG-Transformer is reduced to 1.02 s, which is 60.2% less than CAME-Transformer, demonstrating higher computational efficiency and real-time performance. Importantly, in terms of interpretability, the engineering interpretability and credibility of the models are further improved by visualizing the feature learning results of the vibration modal and multimodal fusion models and the sensitivity analyses of the sound-vibration response models with internal and external physical hyperparameters. Therefore, this study proposes a physical information-guided deep learning method with strong interpretability and superior performance, which provides an important reference for further research and application in the field of bearing fault diagnosis.","2025-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","253","","","","","","","","","","English","","","","WOS:001342291100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;11<br/>Total Times Cited:&nbsp;&nbsp;11<br/>Cited Reference Count:&nbsp;&nbsp;49</p>","","","Bearing fault diagnosis; Deep learning; Engineering interpretability; NETWORKS; Physical-information fusion constraint-guided; Sound-vibration","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4N9UU9RZ","journalArticle","2022","Mentzel, F; Kröninger, K; Lerch, M; Nackenhorst, O; Rosenfeld, A; Tsoi, AC; Weingarten, J; Hagenbuchner, M; Guatelli, S","Small beams, fast predictions: a comparison of machine learning dose prediction models for proton minibeam therapy","MEDICAL PHYSICS","","0094-2405","10.1002/mp.16066","","Background Dose calculations for novel radiotherapy cancer treatments such as proton minibeam radiation therapy is often done using full Monte Carlo (MC) simulations. As MC simulations can be very time consuming for this kind of application, deep learning models have been considered to accelerate dose estimation in cancer patients. Purpose This work systematically evaluates the dose prediction accuracy, speed and generalization performance of three selected state-of-the-art deep learning models for dose prediction applied to the proton minibeam therapy. The strengths and weaknesses of those models are thoroughly investigated, helping other researchers to decide on a viable algorithm for their own application. Methods The following recently published models are compared: first, a 3D U-Net model trained as a regression network, second, a 3D U-Net trained as a generator of a generative adversarial network (GAN) and third, a dose transformer model which interprets the dose prediction as a sequence translation task. These models are trained to emulate the result of MC simulations. The dose depositions of a proton minibeam with a diameter of 800 mu m and an energy of 20-100 MeV inside a simple head phantom calculated by full Geant4 MC simulations are used as a case study for this comparison. The spatial resolution is 0.5 mm. Special attention is put on the evaluation of the generalization performance of the investigated models. Results Dose predictions with all models are produced in the order of a second on a GPU, the 3D U-Net models being fastest with an average of 130 ms. An investigated 3D U-Net regression model is found to show the strongest performance with overall 61.0%+/-$\%\pm$0.5% of all voxels exhibiting a deviation in energy deposition prediction of less than 3% compared to full MC simulations with no spatial deviation allowed. The 3D U-Net models are observed to show better generalization performance for target geometry variations, while the transformer-based model shows better generalization with regard to the proton energy. Conclusions This paper reveals that (1) all studied deep learning models are significantly faster than non-machine learning approaches predicting the dose in the order of seconds compared to hours for MC, (2) all models provide reasonable accuracy, and (3) the regression-trained 3D U-Net provides the most accurate predictions.","2022-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","7791-7801","","12","49","","","","","","","","","","English","","","","WOS:000881274400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;5<br/>Total Times Cited:&nbsp;&nbsp;5<br/>Cited Reference Count:&nbsp;&nbsp;27</p>","","","deep learning; dose prediction; proton minibeam therapy; RADIOTHERAPY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"S6F242M3","journalArticle","2021","Ma, J; Zhang, YF; Li, YM; Zhou, L; Qin, LY; Zeng, YW; Wang, P; Lei, Y","Deep dual-side learning ensemble model for Parkinson speech recognition","BIOMEDICAL SIGNAL PROCESSING AND CONTROL","","1746-8094","10.1016/j.bspc.2021.102849","","Early diagnosis of Parkinson's disease (PD) is very important Kansara et al. (2013) and Stern (1993). In recent years, machine learning-based speech data analysis has been shown to be effective for diagnosing Parkinson's disease (PD) and automatically assessing rehabilitative speech treatment in PD Sakar et al. (2013), Tsanas et al. (2012) and Little et al. (2009). Machine learning includes feature learning and sample learning. Deep learning (deep feature learning) can generate high-level and high-quality features through deep feature transformation, improving classification accuracy. For reasons such as data collection, some samples have low quality for classification. Therefore, sample learning is necessary. Sample selection removes useless samples; therefore, deep sample learning is better, since it can generate high-level and high-quality samples through deep sample transformation. However, there are no public studies about deep sample learning. To solve the problem above, a deep dual-side learning ensemble model is designed in this paper. In this model, a deep sample learning algorithm is designed and combined with a deep network (deep feature learning), thereby realizing the deep dualside learning of PD speech data. First, an embedded stack group sparse autoencoder is designed in this paper to conduct deep feature learning to acquire new high-level deep feature data. Second, the deep features are fused with original speech features by L1 regularization feature selection methods, thereby constructing hybrid feature data. Third, an iterative mean clustering algorithm (IMC) was designed, thereby constructing a deep sample learning algorithm and conducting deep sample transformation. After that step, hierarchical sample spaces are constructed based on a deep sample learning algorithm, and the classification models are constructed on the sample spaces. Finally, the weighted fusion mechanism is designed to merge the classification models into a classification ensemble model, thereby fusing the deep feature learning algorithm and the deep sample learning algorithm together. The ensemble model is called the deep dual-side learning ensemble model. At the end of this paper, two representative speech datasets of PD were used for validation. The experimental results show that the main innovation part of the algorithm is effective. For the two datasets, the mean accuracy of the proposed algorithm reaches 98.4% and 99.6%, which are better than the state-of-art relevant algorithms. The study shows that deep dual-side learning is better for existing deep feature learning for PD speech recognition.","2021-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","69","","","","","","","","","","English","","","","WOS:000685910600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;19<br/>Total Times Cited:&nbsp;&nbsp;19<br/>Cited Reference Count:&nbsp;&nbsp;41</p>","","","Automatic assessment of rehabilitative speech treatment; CLASSIFICATION; Deep dual-side learning; Deep learning; Deep sample learning; DISEASE; EARLY-DIAGNOSIS; Feature fusion; FEATURE-SELECTION; MULTIPLE TYPES; Speech recognition of Parkinson's disease; SYSTEM; VOICE RECORDINGS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JFF4CA2W","journalArticle","2024","McFarlane, KA; Sanchez, JT","Effects of Temporal Processing on Speech-in-Noise Perception in Middle-Aged Adults","BIOLOGY-BASEL","","2079-7737","10.3390/biology13060371","","Simple Summary A significant portion of adults with clinically normal hearing sensitivity have difficulty understanding speech in background noise. Current clinical assessments fail to explain this phenomenon, prompting the exploration of auditory mechanisms beyond those covered by routine clinical testing. One mechanism important for separating sound sources-a key task for understanding speech-in-noise-is temporal processing, or the extraction and organization of acoustic timing characteristics. Here, we investigate the hypothesis that deficits in temporal processing contribute to difficulties in understanding speech-in-noise. We explore this in middle-aged adults-an under-investigated group, despite their high prevalence of speech-in-noise difficulties. In this study, we found that differences in speech-in-noise abilities were associated with deficits in two aspects of temporal processing: the neural encoding of periodic speech features, such as pitch, and perceptual sensitivity to rapid acoustic timing differences between ears. Interestingly, the use of these mechanisms was task-dependent, suggesting various aspects of temporal processing differentially contribute to speech-in-noise perception based on the characteristics of the listening environment. These findings contribute to our overall understanding of which auditory mechanisms play a role in speech-in-noise difficulties in normal hearing listeners, and can inform future clinical practice to serve this population.Abstract Auditory temporal processing is a vital component of auditory stream segregation, or the process in which complex sounds are separated and organized into perceptually meaningful objects. Temporal processing can degrade prior to hearing loss, and is suggested to be a contributing factor to difficulties with speech-in-noise perception in normal-hearing listeners. The current study tested this hypothesis in middle-aged adults-an under-investigated cohort, despite being the age group where speech-in-noise difficulties are first reported. In 76 participants, three mechanisms of temporal processing were measured: peripheral auditory nerve function using electrocochleography, subcortical encoding of periodic speech cues (i.e., fundamental frequency; F0) using the frequency following response, and binaural sensitivity to temporal fine structure (TFS) using a dichotic frequency modulation detection task. Two measures of speech-in-noise perception were administered to explore how contributions of temporal processing may be mediated by different sensory demands present in the speech perception task. This study supported the hypothesis that temporal coding deficits contribute to speech-in-noise difficulties in middle-aged listeners. Poorer speech-in-noise perception was associated with weaker subcortical F0 encoding and binaural TFS sensitivity, but in different contexts, highlighting that diverse aspects of temporal processing are differentially utilized based on speech-in-noise task characteristics.","2024-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","6","13","","","","","","","","","","English","","","","WOS:001254689600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;64</p>","","","cochlear synaptopathy; FIBERS; LISTENERS; MASKING; middle-aged adults; NORMAL-HEARING; pitch encoding; QUALITIES; SCALE; speech-in-noise perception; temporal fine structure; temporal processing; VALIDATION","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RDE4Q4CL","journalArticle","2024","Tröger, J; Dörr, F; Schwed, L; Linz, N; König, A; Thies, T; Orozco-Arroyave, JR; Rusz, J","An automatic measure for speech intelligibility in dysarthrias-validation across multiple languages and neurological disorders","FRONTIERS IN DIGITAL HEALTH","","2673-253X","10.3389/fdgth.2024.1440986","","Introduction: Dysarthria, a motor speech disorder caused by muscle weakness or paralysis, severely impacts speech intelligibility and quality of life. The condition is prevalent in motor speech disorders such as Parkinson's disease (PD), atypical parkinsonism such as progressive supranuclear palsy (PSP), Huntington's disease (HD), and amyotrophic lateral sclerosis (ALS). Improving intelligibility is not only an outcome that matters to patients but can also play a critical role as an endpoint in clinical research and drug development. This study validates a digital measure for speech intelligibility, the ki: SB-M intelligibility score, across various motor speech disorders and languages following the Digital Medicine Society (DiMe) V3 framework. Methods: The study used four datasets: healthy controls (HCs) and patients with PD, HD, PSP, and ALS from Czech, Colombian, and German populations. Participants' speech intelligibility was assessed using the ki: SB-M intelligibility score, which is derived from automatic speech recognition (ASR) systems. Verification with inter-ASR reliability and temporal consistency, analytical validation with correlations to gold standard clinical dysarthria scores in each disease, and clinical validation with group comparisons between HCs and patients were performed. Results: Verification showed good to excellent inter-rater reliability between ASR systems and fair to good consistency. Analytical validation revealed significant correlations between the SB-M intelligibility score and established clinical measures for speech impairments across all patient groups and languages. Clinical validation demonstrated significant differences in intelligibility scores between pathological groups and healthy controls, indicating the measure's discriminative capability. Discussion: The ki: SB-M intelligibility score is a reliable, valid, and clinically relevant tool for assessing speech intelligibility in motor speech disorders. It holds promise for improving clinical trials through automated, objective, and scalable assessments. Future studies should explore its utility in monitoring disease progression and therapeutic efficacy as well as add data from further dysarthrias to the validation.","2024-07-23","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","6","","","","","","","","","","English","","","","WOS:001286231000001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;55</p>","","","amyotrophic lateral sclerosis (ALS); digital biomarkers; Huntington's disease (HD); IMPAIRMENT; intelligibility; Parkinson's disease (PD); PARKINSONS-DISEASE; PATTERNS; progressive supranuclear palsy (PSP); QUALITY-OF-LIFE; RATING-SCALE; RECOGNITION; SCLEROSIS; speech analysis; SYSTEM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L8P4PPSW","journalArticle","2024","Shukla, AK; Chaturvedi, S; Ahmed, AR; Abouzeid, HL; Suleman, G; Sharif, RA; Gurumurthy, V; Cicciù, M; Minervini, G","Analysis of modified palatal surface for better speech in edentulous patients: A clinico-analytical study","TECHNOLOGY AND HEALTH CARE","","0928-7329","10.3233/THC-230477","","BACKGROUND: Phonetics with mechanics and aesthetics are considered cardinal factors contributing to the success of complete dentures. OBJECTIVE: The aim of the current study was to evaluate the changes in speech in complete denture patients with and without palatal surface changes. METHODS: The data collected for the study involved completely edentulous Patients (n = 80). Patients were divided into two groups. Group A: Up to 50 years (n = 40); Group B: Above 50 years (n = 40) (including male and female). Each group was further divided into four subgroups for speech analysis-without dentures (A1/B1); with conventional dentures (A2/B2); dentures with a palatogram (A3/B3) and dentures with palate sandblasted (A4/B4). The speech sounds of all patients were evaluated for pitch and intensity using a spectrophotogram (Praat software) and Likert scale. The data so obtained from the quantitative and qualitative analysis were statistically analysed through one-way ANOVA, unpaired student's t-test, and Pearson correlation coefficient. Qualitative analysis was done with the Mann-Whitney U test. RESULTS: The maximum mean value of the pitch and intensity in Groups A and B was subgroup A3 (Palatogram) 190.37 +/- 21.50 and 77.07 +/- 7.58, B3 (Palatogram) 190.87 +/- 24.36 and 75.33 +/- 8.12. When comparing Groups A [A1, A2, A3, A4] and B [B1, B2, B3, B4] on the basis of pitch and intensity, a statistically insignificant value was found. No changes in speech sounds (pitch and intensity) were observed in terms of age. Both quantitative and qualitative analysis results showed that speech sounds in the patients with palatogram, had the highest pitch and intensity and were most clear to the listener and there were statistically significant differences from other groups. CONCLUSION: Speech was better in palatogram-based dentures in comparison to a sandblasted denture, conventional denture and without denture in terms of qualitative and quantitative analysis. The physiologic nature of the palatogram fabrication technique produces the palatal part of the denture more compatible with the tongue, thus producing better speech.","2024","2025-02-26 20:45:06","2025-02-26 20:45:06","","1055-1065","","2","32","","","","","","","","","","English","","","","WOS:001208606200047","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;35</p>","","","COMPLETE DENTURES; dentistry; edentulous; Palatal surface; prosthodontics; speech; VERTICAL-BAR SOUND","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GK2IFFSX","journalArticle","2024","Dutta, M; Mello, TMD; Cheng, YS; Dash, NS; Nandi, R; Dutt, A; Bose, A","Universal and Language-Specific Connected Speech Characteristics of Bilingual Speakers With Alzheimer's Disease: Insights From Case Studies of Structurally Distinct Languages","JOURNAL OF SPEECH LANGUAGE AND HEARING RESEARCH","","1092-4388","10.1044/2024_JSLHR-23-00254","","Purpose: Connected speech analysis has been effectively utilized for the diagnosis and disease monitoring of individuals with Alzheimer's disease (AD). Existing research has been conducted mostly in monolingual English speakers with a noticeable lack of evidence from bilinguals and non-English speakers, particularly in non -European languages. Using a case study approach, we characterized connected speech profiles of two Bengali-English bilingual speakers with AD to determine the universal features of language impairments in both languages, identify language -specific differences between the languages, and explore language impairment characteristics of the participants with AD in relation to their bilingual language experience. Method: Participants included two Bengali-English bilingual speakers with AD and a group of age-, gender-, education-, and language -matched neurologically healthy controls. Connected speech samples were collected in first language (L1; Bengali) and second language (L2; English) using a novel storytelling task (i.e., Frog, Where Are You? ). These samples were analyzed using an augmented quantitative production analysis and correct information unit analyses for productivity, fluency, syntactic and morphosyntactic features, and lexical and semantic characteristics. Results: Irrespective of the language, AD impacted speech productivity (speech rate and fluency) and semantic characteristics in both languages. Unique language -specific differences were noted on syntactic measures (reduced sentence length in Bengali), lexical distribution (fewer pronouns and absence of reduplication in Bengali), and inflectional properties (no difficulties with noun or verb inflections in Bengali). Among the two participants with AD, the individual who showed lower proficiency and usage in L2 (English) demonstrated reduced syntactic complexity and morphosyntactic richness in English. Conclusions: Evidence from these case studies suggests that language impairment features in AD are not universal across languages, particularly in comparison to impairments typically associated with language breakdowns in English. This study underscores the importance of establishing connected speech profiles in AD for non-English-speaking populations, especially for structurally different languages. This would in turn lead to the development of language -specific markers that can facilitate early detection of language deterioration and aid in improving diagnosis of AD in individuals belonging to underserved linguistically diverse populations.","2024-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","1143-1164","","4","67","","","","","","","","","","English","","","","WOS:001244512900010","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;68</p>","","","DECLINE; DEMENTIA; DETERIORATION; IMPAIRMENT; INDIVIDUALS; MARKER; NEUROPSYCHOLOGY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B6KDTUN3","journalArticle","2023","Hasanzadeh, M; Mahmoodian, H","A novel hybrid method for feature selection based on gender analysis for early Parkinson's disease diagnosis using speech analysis","APPLIED ACOUSTICS","","0003-682X","10.1016/j.apacoust.2023.109561","","Parkinson's disease (PD) is one of the neurologic disorders for older people that, because of similar symptoms to other neurologic diseases, is difficult to diagnose, mainly in the early stages. Recently, neurologists are trying non-medical methods based on speech processing for early diagnosis of PD. Since some speech characteristics are different between women and men, we analyze them separately in this research and propose a hybrid method to classify the PD and healthy samples. The proposed method's main structure relies on the features' scores, which are based on a projection on a two-dimensional plane. In the first step, several features inherently different between women and men (healthy and PD) are removed from the features. Then, statistical-Based Feature Score (SBFS) and Classification-Based Feature Score (CBFS) are introduced in the proposed method and used as the dimensions of a two-dimensional hyperplane to rank the features. To increase the stability of the model in speech feature selection, resampling of the dataset is considered in the methodology. Different classifiers (linear and nonlinear support vector machine (LSVM and NSVM respectively), K-nearest neighborhood (KNN), naive Bayesian (NB), and random forest (RF)) are applied for sample classification. Finally, two sets of features are separately introduced for both groups of men and women. We have achieved 86% and 84% accuracy rates for men and women groups with 14 and 12 features, respectively. The results show that the number of selected features is less than the previous work that introduced and used this dataset. The results also present that although the chosen features are different for men and women, most of the selected features are in similar categories. Unlike the previous similar study where the validation method was based on Leave-one-out, in this study, the results are validated based on an independent set of the used dataset to analyze the stability of the selected features.","2023-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","211","","","","","","","","","","English","","","","WOS:001051340800001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;40</p>","","","CLASSIFICATION; Early diagnosis; Parkinson's disease; Speech processing; Statistical analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"W8YCEYRQ","journalArticle","2024","Yoon, TJ","How Much Does the Dynamic F0 Curve Affect the Expression of Emotion in Utterances?","APPLIED SCIENCES-BASEL","","2076-3417","10.3390/app142310972","","The modulation of vocal elements, such as pitch, loudness, and duration, plays a crucial role in conveying both linguistic information and the speaker's emotional state. While acoustic features like fundamental frequency (F0) variability have been widely studied in emotional speech analysis, accurately classifying emotion remains challenging due to the complex and dynamic nature of vocal expressions. Traditional analytical methods often oversimplify these dynamics, potentially overlooking intricate patterns indicative of specific emotions. This study examines the influences of emotion and temporal variation on dynamic F0 contours in the analytical framework, utilizing a dataset valuable for its diverse emotional expressions. However, the analysis is constrained by the limited variety of sentences employed, which may affect the generalizability of the findings to broader linguistic contexts. We utilized the Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS), focusing on eight distinct emotional states performed by 24 professional actors. Sonorant segments were extracted, and F0 measurements were converted into semitones relative to a 100 Hz baseline to standardize pitch variations. By employing Generalized Additive Mixed Models (GAMMs), we modeled non-linear trajectories of F0 contours over time, accounting for fixed effects (emotions) and random effects (individual speaker variability). Our analysis revealed that incorporating emotion-specific, non-linear time effects and individual speaker differences significantly improved the model's explanatory power, ultimately explaining up to 66.5% of the variance in the F0. The inclusion of random smooths for time within speakers captured individual temporal modulation patterns, providing a more accurate representation of emotional speech dynamics. The results demonstrate that dynamic modeling of F0 contours using GAMMs enhances the accuracy of emotion classification in speech. This approach captures the nuanced pitch patterns associated with different emotions and accounts for individual variability among speakers. The findings contribute to a deeper understanding of the vocal expression of emotions and offer valuable insights for advancing speech emotion recognition systems.","2024-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","23","14","","","","","","","","","","English","","","","WOS:001376284400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;41</p>","","","emotional speech recognition; fundamental frequency (F0); generalized additive mixed models (GAMMs); non-linear dynamics; PARAMETERS; pitch contours; RECOGNITION; SPEAKERS; speech processing; VOCAL EXPRESSION","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"378KA3JV","journalArticle","2023","Jenkins, T; Pouw, W","Gesture-Speech Coupling in Persons With Aphasia: A Kinematic-Acoustic Analysis","JOURNAL OF EXPERIMENTAL PSYCHOLOGY-GENERAL","","0096-3445","10.1037/xge0001346","","Aphasia is a profound language pathology hampering speech production and/or comprehension. People With Aphasia (PWA) use more manual gestures than Non-Brain Injured (NBI) individuals. This intuitively invokes the idea that gesture is compensatory in some way, but there is variable evidence of a gesture-boosting effect on speech processes. The status quo in gesture research with PWA is an emphasis on categorical analysis of gesture types, focusing on how often they are recruited, and whether more or less gesturing aids communication or speaking. However, there are increasingly louder calls for the investigation of gesture and speech as continuous entangled modes of expression. In NBI adults, expressive moments of gesture and speech are synchronized on the prosodic level. It has been neglected how this multimodal prosody is instantiated in PWA. In the current study, we perform the first acoustic-kinematic gesture-speech analysis in Persons With Aphasia (i.e., Wernicke's, Broca's, Anomic) relative to age-matched controls, where we apply several multimodal signal analysis methods. Specifically, we related the speech peaks (smoothed amplitude envelope change) with that of the nearest peaks in the gesture acceleration profile. We obtained that the magnitude of gesture versus speech peaks are positively related across the groups, though more variably for PWA, and such coupling was related to less severe Aphasia-related symptoms. No differences were found between controls and PWA in terms of temporal ordering of speech envelope versus acceleration peaks. Finally, we show that both gesture and speech have slower quasi-rhythmic structure, indicating that next to speech, gesture is slowed down too. The current results indicate that there is a basic gesture-speech coupling mechanism that is not fully reliant on core linguistic competences, as it is found relatively intact in PWA. This resonates with a recent biomechanical theory of gesture, which renders gesture-vocal coupling as fundamental and a priori to the (evolutionary) development of core linguistic competences.","2023-05","2025-02-26 20:45:06","2025-02-26 20:45:06","","1469-1483","","5","152","","","","","","","","","","English","","","","WOS:000970379900001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;81</p>","","","aphasia; AphasiaBank; gesture; gesture-speech synchrony; INFORMATION; multimodal prosody; PEOPLE; PERCEPTION; POSITION; PROSODY; speech","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DM7A259X","journalArticle","2021","Amir, O; Anker, SD; Gork, I; Abraham, WT; Pinney, SP; Burkhoff, D; Shallom, ID; Haviv, R; Edelman, ER; Lotan, C","Feasibility of remote speech analysis in evaluation of dynamic fluid overload in heart failure patients undergoing haemodialysis treatment","ESC HEART FAILURE","","2055-5822","10.1002/ehf2.13367","","Aims This study aimed to assess the ability of a voice analysis application to discriminate between wet and dry states in chronic heart failure (CHF) patients undergoing regular scheduled haemodialysis treatment due to volume overload as a result of their chronic renal failure. Methods and results In this single-centre, observational study, five patients with CHF, peripheral oedema of >= 2, and pulmonary congestion-related dyspnoea, undergoing haemodialysis three times per week, recorded five sentences into a standard smartphone/tablet before and after haemodialysis. Recordings were provided that same noon/early evening and the next morning and evening. Patient weight was measured at the hospital before and after each haemodialysis session. Recordings were analysed by a smartphone application (app) algorithm, to compare speech measures (SMs) of utterances collected over time. On average, patients provided recordings throughout 25.8 +/- 3.9 dialysis treatment cycles, resulting in a total of 472 recordings. Weight changes of 1.95 +/- 0.64 kg were documented during cycles. Median baseline SM prior to dialysis was 0.87 +/- 0.17, and rose to 1.07 +/- 0.15 following the end of the dialysis session, at noon (P = 0.0355), and remained at a similar level until the following morning (P = 0.007). By the evening of the day following dialysis, SMs returned to baseline levels (0.88 +/- 0.19). Changes in patient weight immediately after dialysis positively correlated with SM changes, with the strongest correlation measured the evening of the dialysis day [slope: -0.40 +/- 0.15 (95% confidence interval: -0.71 to -0.10), P = 0.0096]. Conclusions The fluid-controlled haemodialysis model demonstrated the ability of the app algorithm to identify cyclic changes in SMs, which reflected bodily fluid levels. The voice analysis platform bears considerable potential as a harbinger of impending fluid overload in a range of clinical scenarios, which will enhance monitoring and triage efforts, ultimately optimizing remote CHF management.","2021-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","2467-2472","","4","8","","","","","","","","","","English","","","","WOS:000647324900001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;7<br/>Total Times Cited:&nbsp;&nbsp;7<br/>Cited Reference Count:&nbsp;&nbsp;25</p>","","","Acute heart failure (AHF); Dialysis; Remote voice analysis; Speech measure (SM)","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZZP26KW4","journalArticle","2023","Pibre, L; Madrigal, F; Equoy, C; Lerasle, F; Pellegrini, T; Pinquier, J; Ferrané, I","Audio-video fusion strategies for active speaker detection in meetings","MULTIMEDIA TOOLS AND APPLICATIONS","","1380-7501","10.1007/s11042-022-13746-7","","Meetings are a common activity in professional contexts, and it remains challenging to endow vocal assistants with advanced functionalities to facilitate meeting management. In this context, a task like active speaker detection can provide useful insights to model interaction between meeting participants. Detection of the active speaker can be performed using only video based on the movements of the participants of a meeting. Depending on the assistant design and each participant position regarding the device, active speaker detection can benefit from information coming from visual and audio modalities. Motivated by our application context related to advanced meeting assistant, we want to combine audio and visual information to achieve the best possible performance. In this paper, we propose two different types of fusion (naive fusion and attention-based fusion) for the detection of the active speaker, combining two visual modalities and an audio modality through neural networks. In addition, the audio modality is mainly processed using neural networks. For comparison purpose, classical unsupervised approaches for audio feature extraction are also used. We expect visual data centered on the face of each participant to be very appropriate for detecting voice activity, based on the detection of lip and facial gestures. Thus, our baseline system uses visual data (video) and we chose a 3D Convolutional Neural Network (CNN) architecture, which is effective for simultaneously encoding appearance and movement. To improve this system, we supplemented the visual information by processing the audio stream with a CNN or an unsupervised speaker diarization system. We have further improved this system by adding visual modality information using motion through optical flow. We evaluated our proposal with a public and state-of-the-art benchmark: the AMI corpus. We analysed the contribution of each system to the merger carried out in order to determine if a given participant is currently speaking. We also discussed the results we obtained. Besides, we have shown that, for our application context, adding motion information greatly improves performance. Finally, we have shown that attention-based fusion improves performance while reducing the standard deviation.","2023-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","13667-13688","","9","82","","","","","","","","","","English","","","","WOS:000861190800003","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;44</p>","","","Active speaker detection; AREA; Audio processing; Deep learning; DIARIZATION; Multimodal fusion; SPEECH; Speech analysis; Video processing","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BKPZPEGA","journalArticle","2023","Ostermann, F; Cavalcanti, CJD; Nascimento, MM; Lima, NW","Speech analysis under a Bakhtinian approach: Contributions to research on physics education","PHYSICAL REVIEW PHYSICS EDUCATION RESEARCH","","2469-9896","10.1103/PhysRevPhysEducRes.19.010141","","[This paper is part of the Focused Collection on Qualitative Methods in PER: A Critical Examination.] Since the beginning of the 21st century, the appropriation of the sociocultural perspective by the physics education research community has represented a linguistic turn in the field, pointing out a promising path to overcome the dominance of the ""individual paradigm,"" both in terms of student learning and initial and continuing teacher training. This approach views science, science education, and research as human social activities embedded in larger sociocultural and institutional contexts, implying a significant theoretical importance to the role of social interaction and the context in which these interactions occur, viewing them as critical to a better understanding of the learning process rather than merely as a secondary role. In this theoretical framework, language plays a fundamental role as a mediator of human action, notably, it is the main system of signs used by humankind. Hence, we recognize the origin of sociocultural perspectives in Lev Vygotsky's sociohistorical psychology. The neo-Vygotskian James Wertsch proposes a ""continuity"" of Vygotsky's theory by emphasizing one of its unexplored assumptions: the characterization of human action as an activity mediated by signs and instruments. In this theoretical construction of a sociocultural approach to human action, the philosophy of language of Mikhail Bakhtin Circle becomes crucial. Data of discursive nature (oral and written speech of the instructor and their pupils, textbooks, or official documents) may be analyzed in physics education research, particularly those focusing on classroom situations (typically didactic interventions). We employed Bakhtinian analysis to avoid the text's objectivist (positivist) stance and the structural deterministic idea of ideological interpellation suggested by Althusser and endorsed by Pecheux's discourse analysis. In order to contribute to a qualitative research technique for discursive data analysis, we explain the Circle's theory and translate it into possible methodologies for research in physics education. We suggest an ""analytical trajectory"" based on this as a possible arrangement of the interpretation of discursive data under Bakhtin's metalinguistic. Finally, we exemplify the use of this analytical trajectory in our research group's works.","2023-06-20","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","19","","","","","","","","","","English","","","","WOS:001019269100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;82</p>","","","DISCOURSE ANALYSIS; IDEOLOGY; LANGUAGE; QUESTIONS; SCIENCE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PBR24DCY","journalArticle","2023","Nazih, W; Aseeri, AO; Atallah, OY; El-Sappagh, S","Vision Transformer Model for Predicting the Severity of Diabetic Retinopathy in Fundus Photography-Based Retina Images","IEEE ACCESS","","2169-3536","10.1109/ACCESS.2023.3326528","","Diabetic Retinopathy (DR) is a result of prolonged diabetes with poor blood sugar management. It causes vision problems and blindness due to the deformation of the human retina. Recently, DR has become a crucial medical problem that affects the health and life of people. Diagnosis of DR can be done manually by ophthalmologists, but this is cumbersome and time consuming especially in the current overloaded physician's environment. The early detection and prevention of DR, a severe complication of diabetes that can lead to blindness, require an automatic, accurate, and personalized machine learning-based method. Various deep learning algorithms, particularly convolutional neural networks (CNNs), have been investigated for detecting different stages of DR. Recently, transformers have proved their capabilities in natural language processing. Vision transformers (ViTs) are extensions of these models to capture long-range dependencies in images, which achieved better results than CNN models. However, ViT always needs huge datasets to learn properly, and this condition reduced its applicability in DR domain. Recently, a new real-world and large fundus image dataset called fine-grained annotated diabetic retinopathy (FGADR) has been released which supported the application of ViT in DR diagnosis domain. The literature has not explored FGADR to optimize ViT models. In this paper, we propose a novel ViT based deep learning pipeline for detecting the severity stages of DR based on fundus photography-based retina images. The model has been built using FGADR dataset. The model has been optimized using a new optimizer called AdamW to detect the global context of images. Because FGADR is an imbalanced dataset, we combine several techniques for handling this issue including the usage of F1-score as the optimization metric, data augmentation, class weights, label smoothing, and focal loss. Extensive experiments have been conducted to explore the role of ViT with different data balancing techniques to detect DR. In addition, the proposed model has been compared with the state-of-the-art CNN algorithms such as ResNet50, Incep-tionV3, and VGG19. The adopted model was able to capture the crucial features of retinal images to understand DR severity better. It achieved superior results compared to other CNN and baseline ViT models (i.e., 0.825, 0.825, 0.826, 0.964, 0.825, 0.825, and 0.956 for F1-score, accuracy, balanced accuracy, AUC, precision, recall, specificity, respectively). The results of the proposed ViT model were quite encouraging to be applied in real medical environment for assisting physicians to make accurate, personalized, and timely decisions.","2023","2025-02-26 20:45:06","2025-02-26 20:45:06","","117546-117561","","","11","","","","","","","","","","English","","","","WOS:001097528200001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;6<br/>Total Times Cited:&nbsp;&nbsp;6<br/>Cited Reference Count:&nbsp;&nbsp;84</p>","","","ARTIFICIAL-INTELLIGENCE; Blindness; Convolutional neural networks; Deep learning; diabetic retinopathy; Diabetic retinopathy; DIAGNOSIS; disease diagnosis; Feature extraction; machine learning; Machine learning; Medical services; Retina; Transformers; vision transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7C944BER","journalArticle","2022","Yang, MY; He, XP; Xu, LF; Liu, MH; Deng, JL; Cheng, X; Wei, Y; Li, Q; Wan, S; Zhang, F; Wu, L; Wang, XM; Song, B; Liu, M","CT-based transformer model for non-invasively predicting the Fuhrman nuclear grade of clear cell renal cell carcinoma","FRONTIERS IN ONCOLOGY","","2234-943X","10.3389/fonc.2022.961779","","BackgroundClear cell Renal Cell Carcinoma (ccRCC) is the most common malignant tumor in the urinary system and the predominant subtype of malignant renal tumors with high mortality. Biopsy is the main examination to determine ccRCC grade, but it can lead to unavoidable complications and sampling bias. Therefore, non-invasive technology (e.g., CT examination) for ccRCC grading is attracting more and more attention. However, noise labels on CT images containing multiple grades but only one label make prediction difficult. However, noise labels exist in CT images, which contain multiple grades but only one label, making prediction difficult. AimWe proposed a Transformer-based deep learning algorithm with CT images to improve the diagnostic accuracy of grading prediction and to improve the diagnostic accuracy of ccRCC grading. MethodsWe integrate different training models to improve robustness and predict Fuhrman nuclear grade. Then, we conducted experiments on a collected ccRCC dataset containing 759 patients and used average classification accuracy, sensitivity, specificity, and AreaUnderCurve as indicators to evaluate the quality of research. In the comparative experiments, we further performed various current deep learning algorithms to show the advantages of the proposed method. We collected patients with pathologically proven ccRCC diagnosed from April 2010 to December 2018 as the training and internal test dataset, containing 759 patients. We propose a transformer-based network architecture that efficiently employs convolutional neural networks (CNNs) and self-attention mechanisms to extract a persuasive feature automatically. And then, a nonlinear classifier is applied to classify. We integrate different training models to improve the accuracy and robustness of the model. The average classification accuracy, sensitivity, specificity, and area under curve are used as indicators to evaluate the quality of a model. ResultsThe mean accuracy, sensitivity, specificity, and Area Under Curve achieved by CNN were 82.3%, 89.4%, 83.2%, and 85.7%, respectively. In contrast, the proposed Transformer-based model obtains a mean accuracy of 87.1% with a sensitivity of 91.3%, a specificity of 85.3%, and an Area Under Curve (AUC) of 90.3%. The integrated model acquires a better performance (86.5% ACC and an AUC of 91.2%). ConclusionA transformer-based network performs better than traditional deep learning algorithms in terms of the accuracy of ccRCC prediction. Meanwhile, the transformer has a certain advantage in dealing with noise labels existing in CT images of ccRCC. This method is promising to be applied to other medical tasks (e.g., the grade of neurogliomas and meningiomas).","2022-09-28","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","12","","","","","","","","","","English","","","","WOS:000868252100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;6<br/>Total Times Cited:&nbsp;&nbsp;6<br/>Cited Reference Count:&nbsp;&nbsp;42</p>","","","clear cell renal cell carcinoma; deep learning; DIFFERENTIATION; ensemble learning; transformer network; tumor grading","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GTUSY2HP","journalArticle","2025","Chen, JB; Chen, XP; Wang, R; Le, CQ; Khalilian-Gourtani, A; Jensen, E; Dugan, P; Doyle, W; Devinsky, O; Friedman, D; Flinker, A; Wang, Y","Transformer-based neural speech decoding from surface and depth electrode signals","JOURNAL OF NEURAL ENGINEERING","","1741-2560","10.1088/1741-2552/adab21","","Objective. This study investigates speech decoding from neural signals captured by intracranial electrodes. Most prior works can only work with electrodes on a 2D grid (i.e. Electrocorticographic (ECoG) or ECoG array) and data from a single patient. We aim to design a deep-learning model architecture that can accommodate both surface ECoG and depth (stereotactic EEG or sEEG) electrodes. The architecture should allow training on data from multiple participants with large variability in electrode placements. The model should not have subject-specific layers and the trained model should perform well on participants unseen during training. Approach. We propose a novel transformer-based model architecture named SwinTW that can work with arbitrarily positioned electrodes by leveraging their 3D locations on the cortex rather than their positions on a 2D grid. We train subject-specific models using data from a single participant and multi-subject models exploiting data from multiple participants. Main results. The subject-specific models using only low-density 8 x 8 ECoG data achieved high decoding Pearson Correlation Coefficient with ground truth spectrogram (PCC = 0.817), over N = 43 participants, significantly outperforming our prior convolutional ResNet model and the 3D Swin transformer model. Incorporating additional strip, depth, and grid electrodes available in each participant (N = 39) led to further improvement (PCC = 0.838). For participants with only sEEG electrodes (N = 9), subject-specific models still enjoy comparable performance with an average PCC = 0.798. A single multi-subject model trained on ECoG data from 15 participants yielded comparable results (PCC = 0.837) as 15 models trained individually for these participants (PCC = 0.831). Furthermore, the multi-subject models achieved high performance on unseen participants, with an average PCC = 0.765 in leave-one-out cross-validation. Significance. The proposed SwinTW decoder enables future speech decoding approaches to utilize any electrode placement that is clinically optimal or feasible for a particular participant, including using only depth electrodes, which are more routinely implanted in chronic neurosurgical procedures. The success of the single multi-subject model when tested on participants within the training cohort demonstrates that the model architecture is capable of exploiting data from multiple participants with diverse electrode placements. The architecture's flexibility in training with both single-subject and multi-subject data, as well as grid and non-grid electrodes, ensures its broad applicability. Importantly, the generalizability of the multi-subject models in our study population suggests that a model trained using paired acoustic and neural data from multiple patients can potentially be applied to new patients with speech disability where acoustic-neural training data is not feasible.","2025-02-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","22","","","","","","","","","","English","","","","WOS:001408224800001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;50</p>","","","ECoG; electrocorticographic; neural speech decoding; neural speech prosthesis; speech synthesis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"QAZBYXVA","journalArticle","2024","Gomaa, A; Huang, YX; Hagag, A; Schmitter, C; Höfler, D; Weissmann, T; Breininger, K; Schmidt, M; Stritzelberger, J; Delev, D; Coras, R; Dörfler, A; Schnell, O; Frey, B; Gaipl, US; Semrau, S; Bert, C; Hau, P; Fietkau, R; Putz, F","Comprehensive multimodal deep learning survival prediction enabled by a transformer architecture: A multicenter study in glioblastoma","NEURO-ONCOLOGY ADVANCES","","2632-2498","10.1093/noajnl/vdae122","","Background This research aims to improve glioblastoma survival prediction by integrating MR images, clinical, and molecular-pathologic data in a transformer-based deep learning model, addressing data heterogeneity and performance generalizability.Methods We propose and evaluate a transformer-based nonlinear and nonproportional survival prediction model. The model employs self-supervised learning techniques to effectively encode the high-dimensional MRI input for integration with nonimaging data using cross-attention. To demonstrate model generalizability, the model is assessed with the time-dependent concordance index (Cdt) in 2 training setups using 3 independent public test sets: UPenn-GBM, UCSF-PDGM, and Rio Hortega University Hospital (RHUH)-GBM, each comprising 378, 366, and 36 cases, respectively.Results The proposed transformer model achieved a promising performance for imaging as well as nonimaging data, effectively integrating both modalities for enhanced performance (UCSF-PDGM test-set, imaging Cdt 0.578, multimodal Cdt 0.672) while outperforming state-of-the-art late-fusion 3D-CNN-based models. Consistent performance was observed across the 3 independent multicenter test sets with Cdt values of 0.707 (UPenn-GBM, internal test set), 0.672 (UCSF-PDGM, first external test set), and 0.618 (RHUH-GBM, second external test set). The model achieved significant discrimination between patients with favorable and unfavorable survival for all 3 datasets (log-rank P 1.9 x 10-8, 9.7 x 10-3, and 1.2 x 10-2). Comparable results were obtained in the second setup using UCSF-PDGM for training/internal testing and UPenn-GBM and RHUH-GBM for external testing (Cdt 0.670, 0.638, and 0.621).Conclusions The proposed transformer-based survival prediction model integrates complementary information from diverse input modalities, contributing to improved glioblastoma survival prediction compared to state-of-the-art methods. Consistent performance was observed across institutions supporting model generalizability. Glioblastoma is a common and very aggressive type of brain cancer. Predicting how long a patient with glioblastoma will live is challenging. Traditional statistical models usually rely on a single type of information and make assumptions that do not always match real-life scenarios. In this study, researchers wanted to improve the accuracy of these predictions. They combined different types of information, such as the patient's age and gender, genetic details of the tumor, and MRI data, using a special computer model that avoids the assumptions made by traditional models. This new approach was able to predict patient survival times with an accuracy of about %-71% across 3 different patient groups.","2024-08-16","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","6","","","","","","","","","","English","","","","WOS:001291399400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;61</p>","","","CANCER; deep learning; glioblastoma; GLIOMAS; MRI; MULTIFORME; multimodal data; prognosis; survival prediction; SYSTEM","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AJW4JQGQ","journalArticle","2024","Hinterwimmer, F; Serena, RS; Wilhelm, N; Breden, S; Consalvo, S; Seidl, F; Juestel, D; Burgkart, RHH; Woertler, K; von Eisenhart-Rothe, R; Neumann, J; Rueckert, D","Recommender-based bone tumour classification with radiographs-a link to the past","EUROPEAN RADIOLOGY","","0938-7994","10.1007/s00330-024-10672-0","","ObjectivesTo develop an algorithm to link undiagnosed patients to previous patient histories based on radiographs, and simultaneous classification of multiple bone tumours to enable early and specific diagnosis.Materials and methodsFor this retrospective study, data from 2000 to 2021 were curated from our database by two orthopaedic surgeons, a radiologist and a data scientist. Patients with complete clinical and pre-therapy radiographic data were eligible. To ensure feasibility, the ten most frequent primary tumour entities, confirmed histologically or by tumour board decision, were included. We implemented a ResNet and transformer model to establish baseline results. Our method extracts image features using deep learning and then clusters the k most similar images to the target image using a hash-based nearest-neighbour recommender approach that performs simultaneous classification by majority voting. The results were evaluated with precision-at-k, accuracy, precision and recall. Discrete parameters were described by incidence and percentage ratios. For continuous parameters, based on a normality test, respective statistical measures were calculated.ResultsIncluded were data from 809 patients (1792 radiographs; mean age 33.73 +/- 18.65, range 3-89 years; 443 men), with Osteochondroma (28.31%) and Ewing sarcoma (1.11%) as the most and least common entities, respectively. The dataset was split into training (80%) and test subsets (20%). For k = 3, our model achieved the highest mean accuracy, precision and recall (92.86%, 92.86% and 34.08%), significantly outperforming state-of-the-art models (54.10%, 55.57%, 19.85% and 62.80%, 61.33%, 23.05%).ConclusionOur novel approach surpasses current models in tumour classification and links to past patient data, leveraging expert insights.Clinical relevance statementThe proposed algorithm could serve as a vital support tool for clinicians and general practitioners with limited experience in bone tumour classification by identifying similar cases and classifying bone tumour entities.Key Points center dot Addressed accurate bone tumour classification using radiographic features.center dot Model achieved 92.86%, 92.86% and 34.08% mean accuracy, precision and recall, respectively, significantly surpassing state-of-the-art models.center dot Enhanced diagnosis by integrating prior expert patient assessments.Key Points center dot Addressed accurate bone tumour classification using radiographic features.center dot Model achieved 92.86%, 92.86% and 34.08% mean accuracy, precision and recall, respectively, significantly surpassing state-of-the-art models.center dot Enhanced diagnosis by integrating prior expert patient assessments.Key Points center dot Addressed accurate bone tumour classification using radiographic features.center dot Model achieved 92.86%, 92.86% and 34.08% mean accuracy, precision and recall, respectively, significantly surpassing state-of-the-art models.center dot Enhanced diagnosis by integrating prior expert patient assessments.","2024-03-15","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","","","","","","","","","","","English","","","","WOS:001185824700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;33</p>","","","Bone neoplasms; Classification; Deep learning; DIAGNOSIS; EPIDEMIOLOGY; Machine learning; PERFORMANCE; Radiography; SARCOMA","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WA3NK8GQ","journalArticle","2024","Zheng, WG; Zheng, K; Gao, LT; Zhangzhong, LL; Lan, RP; Xu, LL; Yu, JX","GRU-Transformer: A Novel Hybrid Model for Predicting Soil Moisture Content in Root Zones","AGRONOMY-BASEL","","2073-4395","10.3390/agronomy14030432","","The accurate measurement of soil moisture content emerges as a critical parameter within the ambit of agricultural irrigation management, wherein the precise prediction of this variable plays an instrumental role in enhancing the efficiency and conservation of agricultural water resources. This study introduces an innovative, cutting-edge hybrid model that ingeniously integrates Gated Recirculation Unit (GRU) and Transformer technologies, meticulously crafted to amplify the precision and reliability of soil moisture content forecasts. Leveraging meteorological and soil moisture datasets amassed from eight monitoring stations in Hebei Province, China, over the period from 2011 to 2018, this investigation thoroughly assesses the model's efficacy against a diverse array of input variables and forecast durations. This assessment is concurrently contrasted with a range of conventional machine learning and deep learning frameworks. The results demonstrate that (1) the GRU-Transformer model exhibits remarkable superiority across various aspects, particularly in short-term projections (1- to 2-day latency). The model's mean square error (MSE) for a 1-day forecast is notably low at 5.22%, reducing further to a significant 2.71%, while the mean coefficient of determination (R2) reaches a high of 89.92%. Despite a gradual increase in predictive error over extended forecast periods, the model consistently maintains robust performance. Moreover, the model shows exceptional versatility in managing different soil depths, notably excelling in predicting moisture levels at greater depths, thereby surpassing its performance in shallower soils. (2) The model's predictive error inversely correlates with the reduction in parameters. Remarkably, with a streamlined set of just six soil moisture content parameters, the model predicts an average MSE of 0.59% and an R2 of 98.86% for a three-day forecast, highlighting its resilience to varied parameter configurations. (3) In juxtaposition with prevalent models such as Support Vector Regression (SVR), K-Nearest Neighbors (KNN), Gradient Boosting Decision Tree (GBDT), XGBoost, Random Forest, and deep learning models like Deep Neural Network (DNN), Convolutional Neural Network (CNN), and standalone GRU-branch and Transformer-branch models, the GRU-Transformer framework demonstrates a significant advantage in predicting soil moisture content with enhanced precision for a five-day forecast. This underscores its exceptional capacity to navigate the intricacies of soil moisture data. This research not only provides a potent decision-support tool for agricultural irrigation planning but also makes a substantial contribution to the field of water resource conservation and optimization in agriculture, while concurrently imparting novel insights into the application of deep learning techniques in the spheres of agricultural and environmental sciences.","2024-03","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","3","14","","","","","","","","","","English","","","","WOS:001191448400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;42</p>","","","CONSUMPTION; deep learning; ELM; GRU; MAIZE; soil moisture content; transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KFMMEWNR","journalArticle","2022","Rosenfeld, N; Last, M","Using ECG signals for hypotensive episode prediction in trauma patients","COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE","","0169-2607","10.1016/j.cmpb.2022.106955","","Background and objectives: Bleeding is the leading cause of death among trauma patients both in military and civilian scenarios, and it is also the most common cause of preventable death. Identifying a casualty who suffers from an internal bleeding and may deteriorate rapidly and develop hemorrhagic shock and multiorgan failure is a profound challenge. Blood pressure and heart rate are the main vital signs used nowadays for the casualty clinical evaluation in the battlefield and in intensive care unit. However, these vitals tend to deteriorate at a relatively late stage, when the ability to prevent hazardous complications is limited. Identifying, treating, and rapidly evacuating such casualties might mitigate these complications. In this work, we try to improve a state-of-the-art method for early identification of Hypotensive Episode (HE), by adding electrocardiogram signals to several vital signs.Methods: In this research, we propose to extend the state-of-the-art HE early detection method, In Window Segmentation (InWise), by adding new types of features extracted from ECG signals. The new predictive features can be extracted from ECG signals both manually and automatically by a convolutional auto-encoder. In addition to InWise, we are trying to predict HE using a Transformer model. The Transformer is using the encoder output as an embedding of the ECG signal. The proposed approach is evaluated on trauma patients data from the MIMIC III database.Results: We evaluated the InWise prediction algorithm using four different groups of features. The first feature group contains the 93 original features extracted from vital signs. The second group contains, in addition to the original features, 24 features extracted manually from ECG signal (117 features in total). The third group contains the original features and 20 ECG features extracted by the AE (113 features in total), and the last group is the union of all three previous groups containing 137 features. The results show that each model, which has used ECG data, is outperforming the original InWise model, in terms of AUC and sensitivity with p-value < 0 . 001 (by 0.7% in AUC and up to 3.8% in sensitivity). The model which has used all three feature types (vital signs, manual ECG and AE ECG), outperforms the original model both in terms of accuracy and specificity with p-value < 0 . 001 (by 0.3% and 0.4% respectively).Conclusion: The results show an improvement in the prediction success rates as a result of using ECG based features. The importance of ECG features was confirmed by the feature importance analysis.(c) 2022 Elsevier B.V. All rights reserved.","2022-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","223","","","","","","","","","","English","","","","WOS:000822766300006","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;39</p>","","","Clinical episode prediction; Convolutional autoencoder; Feature extraction; Intensive care; Patient monitoring; Time series analysis; Transformers; Trauma patients; XGBoost classifier","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SD6RKFLM","journalArticle","2021","Xu, JF; Yang, J; Xiong, XG; Li, HF; Huang, JF; Ting, KC; Ying, YB; Lin, T","Towards interpreting multi-temporal deep learning models in crop mapping","REMOTE SENSING OF ENVIRONMENT","","0034-4257","10.1016/j.rse.2021.112599","","Multi-temporal deep learning approaches have exhibited excellent classification performance in large-scale crop mapping. These approaches efficiently and automatically transform remote sensing time series into highdimensional feature representations to identify crop types. The lack of interpretation, however, is regarded as a major drawback of these high-performance approaches. Interpreting deep learning approaches in multitemporal crop mapping is critical for verifying their reliability. This study aims to quantify the impact of multi-temporal information in input time series on classification performance and develop a multi-perspective interpretation pipeline for deep learning models. The pipeline involves three interpretation approaches: evaluating input feature importance, analyzing hidden features, and monitoring temporal changes in model's soft output. An experiment is conducted to classify corn and soybean in the U.S corn belt in 2018. The study area consists of three sites each encompassing millions of pixel-level samples at 30 m resolution. The Landsat Analysis Ready Data are used as the input remote sensing time series and Cropland Data Layer is used as the ground reference. Attention-based Long Short-Term Memory (AtLSTM) and Transformer models are built as multitemporal deep learning models, and compared to Random Forest (RF). Complete time series input in the correct order achieves a higher overall accuracy of 97.8% than using single-window or out-of-order inputs, indicating multi-temporal information facilitates crop classification. An assessment of the input feature importance demonstrates that the AtLSTM, Transformer, and RF models all consider the period from weeks 11 to 20 (earlyJuly to late-August) as a key growth period and the shortwave infrared band as the critical band for corn and soybean discrimination. Hidden feature analysis suggests that the AtLSTM model accumulates the useful information over the growth period, while the Transformer model extracts the temporal dependencies that contribute important information to high-level feature learning. The learned features contain more effective and refined information than the raw input features and thus are better suited for crop classification. The soft output analysis in the in-season classification scenario demonstrates that increased length of input time series improves the model's confidence in the classification results. The further comparison of input feature importance in different sites and years demonstrates the applicability of the interpretation approach at larger spatiotemporal extents with heterogeneous landscapes and interannual variability. This study provides a multi-perspective evaluation to identify key features in multi-spectral and multi-temporal remote sensing data, and yields a practical approach to integrate agronomy knowledge in deep learning-based crop mapping.","2021-10","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","264","","","","","","","","","","English","","","","WOS:000688282300004","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;104<br/>Total Times Cited:&nbsp;&nbsp;107<br/>Cited Reference Count:&nbsp;&nbsp;76</p>","","","AREA; Attention; AUSTRALIA; CLASSIFICATION; Corn and soybean; Crop mapping; Deep learning; Feature importance; FEATURE-SELECTION; INFORMATION; Interpretation; LAND-COVER; Long short-term memory; Multi-temporal classification; PHENOLOGY; RANDOM FOREST; RICE; TIME-SERIES","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GCUWUY8I","journalArticle","2025","Kapetas, D; Kalogeropoulou, E; Christakakis, P; Klaridopoulos, C; Pechlivani, EM","Comparative Evaluation of AI-Based Multi-Spectral Imaging and PCR-Based Assays for Early Detection of Botrytis cinerea Infection on Pepper Plants","AGRICULTURE-BASEL","","2077-0472","10.3390/agriculture15020164","","Pepper production is a critical component of the global agricultural economy, with exports reaching a remarkable $6.9B in 2023. This underscores the crop's importance as a major economic driver of export revenue for producing nations. Botrytis cinerea, the causative agent of gray mold, significantly impacts crops like fruits and vegetables, including peppers. Early detection of this pathogen is crucial for a reduction in fungicide reliance and economic loss prevention. Traditionally, visual inspection has been a primary method for detection. However, symptoms often appear after the pathogen has begun to spread. This study employs the Deep Learning algorithm YOLO for single-class segmentation on plant images to extract spatial details of pepper leaves. The dataset included hyperspectral images at discrete wavelengths (460 nm, 540 nm, 640 nm, 775 nm, and 875 nm) from derived vegetation indices (CVI, GNDVI, NDVI, NPCI, and PSRI) and from RGB. At an Intersection over Union with a 0.5 threshold, the Mean Average Precision (mAP50) achieved by the leaf-segmentation solution YOLOv11-Small was 86.4%. The extracted leaf segments were processed by multiple Transformer models, each yielding a descriptor. These descriptors were combined in ensemble and classified into three distinct classes using a K-nearest neighbor, a Long Short-Term Memory (LSTM), and a ResNet solution. The Transformer models that comprised the best ensemble classifier were as follows: the Swin-L (P:4 x 4-W:12 x 12), the ViT-L (P:16 x 16), the VOLO (D:5), and the XCIT-L (L:24-P:16 x 16), with the LSTM-based classification solution on the RGB, CVI, GNDVI, NDVI, and PSRI image sets. The classifier achieved an overall accuracy of 87.42% with an F1-Score of 81.13%. The per-class F1-Scores for the three classes were 85.25%, 66.67%, and 78.26%, respectively. Moreover, for B. cinerea detection during the initial as well as quiescent stages of infection prior to symptom development, qPCR-based methods (RT-qPCR) were used for quantification of in planta fungal biomass and integrated with the findings from the AI approach to offer a comprehensive strategy. The study demonstrates early and accurate detection of B. cinerea on pepper plants by combining segmentation techniques with Transformer model descriptors, ensembled for classification. This approach marks a significant step forward in the detection and management of crop diseases, highlighting the potential to integrate such methods into in situ systems like mobile apps or robots.","2025-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","2","15","","","","","","","","","","English","","","","WOS:001403933400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;50</p>","","","Botrytis cinerea; deep learning; descriptor classification; fungal biomass; GENES; gray mold; image classification; precision agriculture; RT-qPCR; segmentation; vision transformers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"L2VHD4K8","journalArticle","2024","Anik, BMTH; Islam, Z; Abdel-Aty, M","A time-embedded attention-based transformer for crash likelihood prediction at intersections using connected vehicle data","TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES","","0968-090X","10.1016/j.trc.2024.104831","","The real-time crash likelihood prediction model is an essential component of the proactive traffic safety management system. Over the years, numerous studies have attempted to construct a crash likelihood prediction model in order to enhance traffic safety, but mostly on freeways. In the majority of the existing studies, researchers have primarily used a deep learning-based framework to identify crash potential. Lately, Transformers have emerged as a potential deep neural network that fundamentally operates through attention-based mechanisms. Transformers exhibit distinct functional benefits over established deep learning models like Recurrent Neural Networks (RNNs), Long Short-Term Memory networks (LSTMs), and Convolutional Neural Networks (CNNs). First, they employ attention mechanisms to accurately weigh the significance of different parts of input data, a dynamic functionality that is not available in RNNs, LSTMs, and CNNs. Second, they are well-equipped to handle dependencies over long-range data sequences, a feat RNNs typically struggle with. Lastly, unlike RNNs, LSTMs, and CNNs, which process data in sequence, Transformers can parallelly process data elements during training and inference, thereby enhancing their efficiency. Apprehending the immense possibility of Transformers, this paper proposes inTersection-Transformer (inTformer), a time-embedded attention-based Transformer model that can effectively predict intersection crash likelihood in real-time. The inTformer is basically a binary prediction model that predicts the occurrence or non-occurrence of crashes at intersections in the near future (i.e., next 15 min). The proposed model was developed by employing traffic data extracted from connected vehicles. Acknowledging the complex traffic operation mechanism at intersection, this study developed zone-specific models by dividing the intersection region into two distinct zones: within-intersection and approach zones, each representing the intricate flow of traffic unique to the type of intersection (i.e., three-legged and four-legged intersections). In the 'within-intersection' zone, the inTformer models attained a sensitivity of up to 73%, while in the 'approach' zone, the sensitivity peaked at 74%. Moreover, benchmarking the optimal zone-specific inTformer models against earlier studies on crash likelihood prediction at intersections and several established deep learning models trained on the same connected vehicle dataset confirmed the superiority of the proposed inTformer. Further, to quantify the impact of features on crash likelihood at intersections, the SHAP (SHapley Additive exPlanations) method was applied on the best performing inTformer models. The most critical predictors were average and maximum approach speeds, average and maximum control delays, average and maximum travel times, split failure percentage and count, and percent arrival on green.","2024-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","169","","","","","","","","","","English","","","","WOS:001314071900001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;59</p>","","","Connected Vehicles; Intersection Safety; MACHINE; Real-Time Crash Likelihood; RISK; SAFETY; SEVERITY; SIGNALIZED INTERSECTIONS; Traffic Safety; Transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XLJGSK89","journalArticle","2024","Zhu, JH; Niu, XZ; Li, F; Wang, YX; Fournier-Viger, P; She, K","STTraj2Vec: A spatio-temporal trajectory representation learning approach","KNOWLEDGE-BASED SYSTEMS","","0950-7051","10.1016/j.knosys.2024.112207","","Computing trajectory similarity plays a critical role in various spatio-temporal applications that involve trajectory analysis. In recent years, trajectory representation learning has been extensively studied and applied for trajectory similarity calculation. However the majority of existing algorithms for trajectory representation generally have two problems. The first problem is the emphasis of spatial similarity over temporal similarity, and even to discard the temporal dimension of spatio-temporal trajectories. As a result, the outputs of these approaches cannot fully represent the similarity of spatio-temporal trajectories. The second problem is the introduction of additional information, such as the topology of the road network, which increases the uncertainty of capturing the spatio-temporal correlation of trajectories and prevents their application in scenarios where it is difficult to obtain such information. This poses a significant challenge when dealing with complex and time-varying traffic networks. This paper proposes a novel method, named STTraj2Vec (Spatiotemporal Trajectory 2 Vector), which relies only on spatio-temporal trajectories to capture their similarity without spatio-temporal separation. This takes into account the whole spatio-temporal trajectory information. In this method, an extended clustering algorithm is introduced, which maps the trajectory into a point-region quadtree, and constructs a time-varying virtual network structure based on the point-region quadtree. In this method, an extended clustering algorithm is introduced, which maps each trajectory into a point-region quadtree, and then completes density clustering through the adjacency relation of leaf nodes to construct a time-varying virtual network structure. This virtual network structure not only considers the spatial proximity, but also the time, so as to reflect the spatio-temporal characteristics of the trajectory more accurately. Then, a novel spatially and temporally integrated random walk algorithm is designed, which carries out spatiotemporal random walk on the virtual network structure, to capture the spatiotemporal characteristics of the trajectory, and thus obtains the representation of all nodes in the virtual network. Furthermore, each trajectory is converted into a sequence of vectors on the virtual road network according to the latitude, longitude, and time of the trajectory points. Finally, based on these node representations and trajectories, a transformer model with ranking loss is employed to capture the distinct contributions of the various locations and times to the similarity computation and encode each sequence of vectors into target vectors. Experiments on two public datasets show that STTraj2Vec is superior to the state-of-the-art methods in terms of effectiveness for top-k trajectory similarity search and trajectory clustering, while exhibiting low parameter sensitivity and high model robustness.","2024-09-27","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","300","","","","","","","","","","English","","","","WOS:001279710100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;2<br/>Cited Reference Count:&nbsp;&nbsp;79</p>","","","CLUSTERING-ALGORITHM; NETWORKS; Point-region quadtree; PREDICTION; Representation learning; Spatio-temporal random walk; Trajectory similarity computation; Transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"DS3NVCJL","journalArticle","2024","Gan, WX; Gu, KD; Geng, J; Qiu, CZ; Yang, RQ; Wang, HN; Hu, XD","A Novel Three-Stage Collision-Risk Pre-Warning Model for Construction Vehicles and Workers","BUILDINGS","","2075-5309","10.3390/buildings14082324","","Collision accidents involving construction vehicles and workers frequently occur at construction sites. Computer vision (CV) technology presents an efficient solution for collision-risk pre-warning. However, CV-based methods are still relatively rare and need an enhancement of their performance. Therefore, a novel three-stage collision-risk pre-warning model for construction vehicles and workers is proposed in this paper. This model consists of an object-sensing module (OSM), a trajectory prediction module (TPM), and a collision-risk assessment module (CRAM). In the OSM, the YOLOv5 algorithm is applied to identify and locate construction vehicles and workers; meanwhile, the DeepSORT algorithm is applied to the real-time tracking of the construction vehicles and workers. As a result, the historical trajectories of vehicles and workers are sensed. The original coordinates of the data are transformed to common real-world coordinate systems for convenient subsequent data acquisition, comparison, and analysis. Subsequently, the data are provided to a second stage (TPM). In the TPM, the optimized transformer algorithm is used for a real-time trajectory prediction of the construction vehicles and workers. In this paper, we enhance the reliability of the general object detection and trajectory prediction methods in the construction environments. With the assistance afforded by the optimization of the model's hyperparameters, the prediction horizon is extended, and this gives the workers more time to take preventive measures. Finally, the prediction module indicates the possible trajectories of the vehicles and workers in the future and provides these trajectories to the CRAM. In the CRAM, the worker's collision-risk level is assessed by a multi-factor-based collision-risk assessment rule, which is innovatively proposed in the present work. The multi-factor-based assessment rule is quantitatively involved in three critical risk factors, i.e., velocity, hazardous zones, and proximity. Experiments are performed within two different construction site scenarios to evaluate the effectiveness of the collision-risk pre-warning model. The research results show that the proposed collision pre-warning model can accurately predict the collision-risk level of workers at construction sites, with good tracking and predicting effect and an efficient collision-risk pre-warning strategy. Compared to the classical models, such as social-GAN and social-LSTM, the transformer-based trajectory prediction model demonstrates a superior accuracy, with an average displacement error of 0.53 m on the construction sites. Additionally, the optimized transformer model is capable of predicting six additional time steps, which equates to approximately 1.8 s. The collision pre-warning model proposed in this paper can help improve the safety of construction vehicles and workers.","2024-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","8","14","","","","","","","","","","English","","","","WOS:001305977000001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;66</p>","","","collision prediction; collision-risk factors; computer vision; construction safety; object tracking; SAFETY; STRUCK-BY; TRACKING; trajectory prediction; VISION","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UU4697JA","journalArticle","2023","Teferra, BG; Rose, J","Predicting Generalized Anxiety Disorder From Impromptu Speech Transcripts Using Context-Aware Transformer-Based Neural Networks: Model Evaluation Study","JMIR MENTAL HEALTH","","2368-7959","10.2196/44325","","Background: The ability to automatically detect anxiety disorders from speech could be useful as a screening tool for an anxiety disorder. Prior studies have shown that individual words in textual transcripts of speech have an association with anxiety severity. Transformer-based neural networks are models that have been recently shown to have powerful predictive capabilities based on the context of more than one input word. Transformers detect linguistic patterns and can be separately trained to make specific predictions based on these patterns. Objective: This study aimed to determine whether a transformer-based language model can be used to screen for generalized anxiety disorder from impromptu speech transcripts. Methods: A total of 2000 participants provided an impromptu speech sample in response to a modified version of the Trier Social Stress Test (TSST). They also completed the Generalized Anxiety Disorder 7-item (GAD-7) scale. A transformer-based neural network model (pretrained on large textual corpora) was fine-tuned on the speech transcripts and the GAD-7 to predict whether a participant was above or below a screening threshold of the GAD-7. We reported the area under the receiver operating characteristic curve (AUROC) on the test data and compared the results with a baseline logistic regression model using the Linguistic Inquiry and Word Count (LIWC) features as input. Using the integrated gradient method to determine specific words that strongly affect the predictions, we inferred specific linguistic patterns that influence the predictions. Results: The baseline LIWC-based logistic regression model had an AUROC value of 0.58. The fine-tuned transformer model achieved an AUROC value of 0.64. Specific words that were often implicated in the predictions were also dependent on the context. For example, the first-person singular pronoun ""I"" influenced toward an anxious prediction 88% of the time and a nonanxious prediction 12% of the time, depending on the context. Silent pauses in speech, also often implicated in predictions, influenced toward an anxious prediction 20% of the time and a nonanxious prediction 80% of the time. Conclusions: There is evidence that a transformer-based neural network model has increased predictive power compared with the single word-based LIWC model. We also showed that the use of specific words in a specific context-a linguistic pattern-is part of the reason for the better prediction. This suggests that such transformer-based models could play a useful role in anxiety screening systems.","2023","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","10","","","","","","","","","","English","","","","WOS:000996495400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;5<br/>Total Times Cited:&nbsp;&nbsp;5<br/>Cited Reference Count:&nbsp;&nbsp;47</p>","","","anxiety prediction; generalized anxiety disorder; impromptu speech; linguistic features; mental health; mobile phone; natural language processing; NATURAL-LANGUAGE; neural networks; RESPONSES; STRESS; TRAIT ANXIETY; transformer models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PTC3Y4B3","journalArticle","2024","Zhao, ZW; Zhang, WY; Chen, XL; Song, F; Gunasegaram, J; Huang, WY; Shi, DL; He, MG; Liu, N","Slit Lamp Report Generation and Question Answering: Development and Validation of a Multimodal Transformer Model with Large Language Model Integration","JOURNAL OF MEDICAL INTERNET RESEARCH","","1438-8871","10.2196/54047","","Background: Large language models have shown remarkable efficacy in various medical research and clinical applications. However, their skills in medical image recognition and subsequent report generation or question answering (QA) remain limited. Objective: We aim to finetune a multimodal, transformer-based model for generating medical reports from slit lamp images and develop a QA system using Llama2. We term this entire process slit lamp-GPT. Methods: Our research used a dataset of 25,051 slit lamp images from 3409 participants, paired with their corresponding physician-created medical reports. We used these data, split into training, validation, and test sets, to finetune the Bootstrapping Language-Image Pre-training framework toward report generation. The generated text reports and human-posed questions were then input into Llama2 for subsequent QA. We evaluated performance using qualitative metrics (including BLEU [bilingual evaluation understudy], CIDEr [consensus-based image description evaluation], ROUGE-L [Recall-Oriented Understudy for Gisting Evaluation-Longest Common Subsequence], SPICE [Semantic Propositional Image Caption Evaluation], accuracy, sensitivity, specificity, precision, and F 1-score) and the subjective assessments of two experienced ophthalmologists on a 1-3 scale (1 referring to high quality). Results: We identified 50 conditions related to diseases or postoperative complications through keyword matching in initial reports. The refined slit lamp-GPT model demonstrated BLEU scores (1-4) of 0.67, 0.66, 0.65, and 0.65, respectively, with a CIDEr score of 3.24, a ROUGE (Recall-Oriented Understudy for Gisting Evaluation) score of 0.61, and a Semantic Propositional Image Caption Evaluation score of 0.37. The most frequently identified conditions were cataracts (22.95%), age-related cataracts (22.03%), and conjunctival concretion (13.13%). Diseaseclassification metricsdemonstratedanoverall accuracy of 0.82 and an F 1-score of 0.64, with high accuracies (>= 0.9) observed for intraocular lens, conjunctivitis, and chronic conjunctivitis, and high F 1-scores (>= 0.9) observed for cataract and age-related cataract. For both report generation and QA components, the two evaluating ophthalmologists reached substantial agreement, with kappa scores between 0.71 and 0.84. In assessing 100 generated reports, they awarded scores of 1.36 for both completeness and correctness; 64% (64/100) wereconsidered ""entirely good,"" and 93% (93/100) were ""acceptable."" In the evaluation of 300 generated answers to questions, the scores were 1.33 for completeness, 1.14 for correctness, and 1.15 for possible harm, with 66.3% (199/300) rated as ""entirely good"" and 91.3% (274/300) as ""acceptable."" Conclusions:This study introduces the slit lamp-GPT model for report generation and subsequent QA, highlighting the potential of large language models to assist ophthalmologists and patients.","2024-12-30","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","26","","","","","","","","","","English","","","","WOS:001390714500005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;46</p>","","","ARTIFICIAL-INTELLIGENCE; large language model; medical report generation; question answering; slit lamp","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PWIAK2BW","journalArticle","2023","Alaka, B; Shibwabo, B","Models and Approaches for Comprehension of Dysarthric Speech Using Natural Language Processing: Systematic Review","JMIR REHABILITATION AND ASSISTIVE TECHNOLOGIES","","2369-2529","10.2196/44489","","Background: Speech intelligibility and speech comprehension for dysarthric speech has attracted much attention recently. Dysarthria is characterized by irregularities in the speed, strength, pitch, breath control, range, steadiness, and accuracy of muscle movements required for articulatory aspects of speech production. Objective: This study examined the contributions made by other studies involved in dysarthric speech comprehension. We focused on the modes of meaning extraction used in generalizing speaker -listener underpinnings in light of semantic ontology extraction as a desired technique, applied method types, speech representations used, and databases sourced from. Methods: This study involved a systematic literature review using 7 electronic databases: Cochrane Database of Systematic Reviews, Web of Science Core Collection, Scopus, PubMed, ACM, IEEE Xplore, and Google Scholar. The main eligibility criterion was the extraction of meaning from dysarthric speech using natural language processing or understanding approaches to improve on dysarthric speech comprehension. In total, out of 834 search results, 30 studies that matched the eligibility requirements were acquired following screening by 2 independent reviewers, with a lack of consensus being resolved through joint discussion or consultation with a third party. In order to evaluate the studies' methodological quality, the risk of bias assessment was based on the Cochrane risk -of -bias tool version 2 (RoB2) with 23 of the studies (77%) registering low risk of bias and 7 studies (33%) raising some concern over the risk of bias. The overall quality assessment of the study was done using TRIPOD (Transparent Reporting of a Multivariable Prediction Model for Individual Prognosis or Diagnosis). Results: Following a review of 30 primary studies, this study revealed that the reviewed studies focused on natural language understanding or clinical approaches, with an increase in proposed solutions from 2020 onwards. Most studies relied on speaker -dependent speech features, while others used speech patterns, semantic knowledge, or hybrid approaches. The prevalent use of vector representation aligned with natural language understanding models, while Mel -frequency cepstral coefficient representation and no representation approaches were applied in neural networks. Hybrid representation studies aimed to reconstruct dysarthric speech or improve comprehension. Comprehensive databases, like TORGO and UA-Speech, were commonly used in combination with other curated databases, while primary data was preferred for specific or unique research objectives. Conclusions: We found significant gaps in dysarthric speech comprehension characterized by the lack of inclusion of important listener or speech -independent features in the speech representations, mode of extraction, and data sources used. Further research is therefore proposed regarding the formulation of models that accommodate listener and speech -independent features through semantic ontologies that will be useful in the inclusion of key features of listener and speech -independent features for meaning extraction of dysarthric speech.","2023-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","10","","","","","","","","","","English","","","","WOS:001207666900022","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;57</p>","","","DIAGNOSIS; dysarthria; familiarity; INTELLIGIBILITY; meaning extraction; ontology extraction; RECOGNITION; speech comprehension; speech contextualization; topic knowledge","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7JVY38UE","journalArticle","2022","Cho, S; Cousins, KAQ; Shellikeri, S; Ash, S; Irwin, DJ; Liberman, MY; Grossman, M; Nevler, N","Lexical and Acoustic Speech Features Relating to Alzheimer Disease Pathology","NEUROLOGY","","0028-3878","10.1212/WNL.0000000000200581","","Background and Objectives We compared digital speech and language features of patients with amnestic Alzheimer disease (aAD) or logopenic variant primary progressive aphasia (lvPPA) in a biologically confirmed cohort and related these features to neuropsychiatric test scores and CSF analytes. Methods We included patients with aAD or lvPPA with CSF (phosphorylated tau ([p-tau]/beta-amyloid [A beta] >= 0.09, and total tau/A beta >= 0.34) or autopsy confirmation of AD pathology and age-matched healthy controls (HC) recruited at the Frontotemporal Degeneration Center of the University of Pennsylvania for a cross-sectional study. We extracted speech and language variables with automated lexical and acoustic pipelines from participants' oral picture descriptions. We compared the groups and correlated distinct features with clinical ratings and CSF p-tau levels. Results We examined patients with aAD (n = 44; age 62 +/- 8 years; 24 women; Mini-Mental State Examination [MMSE] score 21.1 +/- 4.8) or lvPPA (n = 21; age 64.1 +/- 8.2 years; 11 women; MMSE score 23.0 +/- 4.2) and HC (n = 28; age 65.9 +/- 5.9 years, 15 women; MMSE score 29 +/- 1). Patients with lvPPA produced fewer verbs (10.5 +/- 2.3; p = 0.001) and adjectives (2.7 +/- 1.3, p = 0.019) and more fillers (7.4 +/- 3.9; p = 0.022) with lower lexical diversity (0.84 +/- 0.1; p = 0.05) and higher pause rate (54.2 +/- 19.2; p = 0.015) than individuals with aAD (verbs 12.5 +/- 2; adjectives 3.8 +/- 2; fillers 4.9 +/- 4.5; lexical diversity 0.87 +/- 0.1; pause rate 45.3 +/- 12.8). Both groups showed some shared language impairments compared with HC. Word frequency (MMSE score: beta = -1.6, p = 0.009; Boston Naming Test [BNT] score: beta = -4.36, p < 0.001), adverbs (MMSE score: beta = -1.9, p = 0.003; BNT score: beta = -2.41, p = 0.041), pause rate (MMSE score: beta = -1.21, p = 0.041; BNT score: beta = -2.09, p = 0.041), and word length (MMSE score: beta = 1.75, p = 0.001; BNT score: beta = 2.94, p = 0.003) were significantly correlated with both MMSE and BNT scores, but other measures were not correlated with MMSE and/or BNT score. Prepositions (r = -0.36, p = 0.019), nouns (r = -0.31, p = 0.047), speech segment duration (r = -0.33, p = 0.032), word frequency (r = 0.33, p = 0.036), and pause rate (r = 0.34, p = 0.026) were correlated with patients' CSF p-tau levels. Discussion Our measures captured language and speech differences between the 2 phenotypes that traditional language-based clinical assessments failed to identify. This work demonstrates the potential of natural speech in reflecting underlying variants with AD pathology.","2022-07-26","2025-02-26 20:45:06","2025-02-26 20:45:06","","E313-E322","","4","99","","","","","","","","","","English","","","","WOS:000841844300011","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;21<br/>Total Times Cited:&nbsp;&nbsp;22<br/>Cited Reference Count:&nbsp;&nbsp;50</p>","","","3 VARIANTS; BIOMARKERS; DEFICITS; DEMENTIA; GUIDELINES; NATIONAL INSTITUTE; RETRIEVAL; SEMANTIC MEMORY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFFJIPUK","journalArticle","2024","Illan-Gala, I; Lorca-Puls, DL; Tee, BL; Ezzes, Z; de Leon, J; Miller, ZA; Rubio-Guerra, S; Santos-Santos, M; Gomez-Andres, D; Grinberg, LT; Spina, S; Kramer, JH; Wauters, LD; Henry, ML; Boxer, AL; Rosen, HJ; Miller, BL; Seeley, WW; Mandelli, ML; Gorno-Tempini, ML","Clinical dimensions along the non-fluent variant primary progressive aphasia spectrum","BRAIN","","0006-8950","10.1093/brain/awad396","","It is debated whether primary progressive apraxia of speech (PPAOS) and progressive agrammatic aphasia (PAA) belong to the same clinical spectrum, traditionally termed non-fluent/agrammatic variant primary progressive aphasia (nfvPPA), or exist as two completely distinct syndromic entities with specific pathologic/prognostic correlates. We analysed speech, language and disease severity features in a comprehensive cohort of patients with progressive motor speech impairment and/or agrammatism to ascertain evidence of naturally occurring, clinically meaningful non-overlapping syndromic entities (e.g. PPAOS and PAA) in our data. We also assessed if data-driven latent clinical dimensions with aetiologic/prognostic value could be identified.We included 98 participants, 43 of whom had an autopsy-confirmed neuropathological diagnosis. Speech pathologists assessed motor speech features indicative of dysarthria and apraxia of speech (AOS). Quantitative expressive/receptive agrammatism measures were obtained and compared with healthy controls. Baseline and longitudinal disease severity was evaluated using the Clinical Dementia Rating Sum of Boxes (CDR-SB). We investigated the data's clustering tendency and cluster stability to form robust symptom clusters and employed principal component analysis to extract data-driven latent clinical dimensions (LCD). The longitudinal CDR-SB change was estimated using linear mixed-effects models. Of the participants included in this study, 93 conformed to previously reported clinical profiles (75 with AOS and agrammatism, 12 PPAOS and six PAA). The remaining five participants were characterized by non-fluent speech, executive dysfunction and dysarthria without apraxia of speech or frank agrammatism. No baseline clinical features differentiated between frontotemporal lobar degeneration neuropathological subgroups. The Hopkins statistic demonstrated a low cluster tendency in the entire sample (0.45 with values near 0.5 indicating random data). Cluster stability analyses showed that only two robust subgroups (differing in agrammatism, executive dysfunction and overall disease severity) could be identified. Three data-driven components accounted for 71% of the variance [(i) severity-agrammatism; (ii) prominent AOS; and (iii) prominent dysarthria]. None of these data-driven LCDs allowed an accurate prediction of neuropathology. The severity-agrammatism component was an independent predictor of a faster CDR-SB increase in all the participants. Higher dysarthria severity, reduced words per minute and expressive and receptive agrammatism severity at baseline independently predicted accelerated disease progression.Our findings indicate that PPAOS and PAA, rather than exist as completely distinct syndromic entities, constitute a clinical continuum. In our cohort, splitting the nfvPPA spectrum into separate clinical phenotypes did not improve clinical-pathological correlations, stressing the need for new biological markers and consensus regarding updated terminology and clinical classification. Illan-Gala et al. report that primary progressive apraxia of speech and progressive agrammatic aphasia are not distinct syndromes, but a clinical continuum indicative of frontotemporal lobar degeneration. Novel clinical and biological markers are needed to improve clinical-pathological correlations.","2024-04-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","1511-1525","","4","147","","","","","","","","","","English","","","","WOS:001182662800001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;65</p>","","","APRAXIA; apraxia of speech; corticobasal degeneration; CRITERIA; DIAGNOSIS; DISEASE; dysarthria; FRONTOTEMPORAL DEMENTIA; magnetic resonance imaging; PATHOLOGY; primary progressive aphasia; progressive supranuclear palsy; SUPRANUCLEAR PALSY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"X955DFVU","journalArticle","2024","Meyer, AM; Snider, SF; Tippett, DC; Saloma, R; Faria, AV; Turkeltaub, PE; Hillis, AE; Friedman, RB","The pattern of phonological, semantic, and circumlocution naming errors for nouns and verbs in primary progressive aphasia","APHASIOLOGY","","0268-7038","10.1080/02687038.2024.2361961","","BackgroundIn the diagnostic criteria for lvPPA (Gorno-Tempini et al. 2011), ""speech (phonologic) errors in spontaneous speech and naming"" is a secondary criterion, but studies of naming error patterns in PPA have not found evidence to support this criterion. Furthermore, only a few studies have examined naming error patterns in PPA.AimsIn the current study, we examined the pattern of naming errors for nouns and verbs in all three subtypes of PPA, as well as unclassifiable PPA and typical (amnestic) Alzheimer's disease (AD). Statistical analyses focused on three common error types: phonological, semantic, and circumlocution errors.Methods & ProceduresThe final sample included 35 participants with PPA and four participants with typical AD. Participants were asked to name 284 noun pictures and 116 verb pictures. Separately for nouns and verbs, repeated-measures ANCOVA was used to examine the interaction between Error Type and Diagnostic Subtype.Twenty of the participants also completed a structural MRI scan. For these participants, we examined the relationships between naming errors and brain volume within ten left hemisphere regions of interest (ROIs).Methods & ProceduresThe final sample included 35 participants with PPA and four participants with typical AD. Participants were asked to name 284 noun pictures and 116 verb pictures. Separately for nouns and verbs, repeated-measures ANCOVA was used to examine the interaction between Error Type and Diagnostic Subtype.Twenty of the participants also completed a structural MRI scan. For these participants, we examined the relationships between naming errors and brain volume within ten left hemisphere regions of interest (ROIs).Outcomes & ResultsIn lvPPA, the proportion of phonological errors was significantly lower than the proportion of semantic errors for verbs. In svPPA, uPPA, and typical AD, semantic errors were significantly greater than phonological errors for both nouns and verbs. In between-subtype analyses, the proportion of semantic errors for nouns was significantly greater for participants with svPPA and uPPA, compared to those with nfvPPA.For nouns, the MRI analyses revealed significant negative correlations between the proportion of circumlocution errors and volume in the left inferior temporal gyrus and the left fusiform gyrus. For verbs, there were significant negative correlations between circumlocution errors and volume in the left insula, and between semantic errors and volume in the left superior temporal pole.Outcomes & ResultsIn lvPPA, the proportion of phonological errors was significantly lower than the proportion of semantic errors for verbs. In svPPA, uPPA, and typical AD, semantic errors were significantly greater than phonological errors for both nouns and verbs. In between-subtype analyses, the proportion of semantic errors for nouns was significantly greater for participants with svPPA and uPPA, compared to those with nfvPPA.For nouns, the MRI analyses revealed significant negative correlations between the proportion of circumlocution errors and volume in the left inferior temporal gyrus and the left fusiform gyrus. For verbs, there were significant negative correlations between circumlocution errors and volume in the left insula, and between semantic errors and volume in the left superior temporal pole.ConclusionsThe findings of this study indicate that semantic naming errors may be common for both nouns and verbs in typical AD and all subtypes of PPA, with the possible exception of nouns in nfvPPA. In contrast, phonological naming errors were not significantly more common than semantic errors in any diagnostic subtype. Furthermore, phonological naming errors were not significantly more common in lvPPA, compared to any other diagnostic subtype.","2024-06-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","","","","","","","","","","","English","","","","WOS:001242077200001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;83</p>","","","3 VARIANTS; Alzheimer's disease; ALZHEIMERS-DISEASE; ANTERIOR TEMPORAL-LOBE; DEMENTIA; LANGUAGE; LOGOPENIC VARIANT; MILD COGNITIVE IMPAIRMENT; naming errors; NONFLUENT APHASIA; phonology; Primary progressive aphasia; semantics; SHORT-TERM-MEMORY; SPEECH","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"UQNL3G8I","journalArticle","2023","Alighieri, C; Bettens, K; Verbeke, J; Van Lierde, K","'Sometimes I feel sad': A qualitative study on children's perceptions with cleft palate speech and language therapy","INTERNATIONAL JOURNAL OF LANGUAGE & COMMUNICATION DISORDERS","","1368-2822","10.1111/1460-6984.12879","","BackgroundEvidence-based speech therapy involves the integration of (1) the scientific evidence for therapy with (2) the perspectives of clinicians concerning therapy and (3) the perspectives of patients about therapy. The cleft literature has already paid attention to the first two cornerstones of evidence-based speech therapy. Much less is known about how children perceive cleft speech therapy. AimsThe purpose of the current qualitative study was to investigate the perceptions, emotions and expectations of (blinded for review) children with a cleft (lip and) palate (CP +/- L), aged 5-12 years, with regard to the speech therapy they receive. In this study, a focus was made on speech therapy to eliminate compensatory cleft speech errors. Methods & ProceduresSix children with a CP +/- L, aged between 5 and 12 years, were included in this study. Child-friendly semi-structured interviews were conducted using a participatory, art-based qualitative approach. This means that the 'play and puppets technique' and 'draw-write and photo-elicitation technique' were used to guide the children through the interviews. Data derived from these interviews were analysed using an inductive thematic approach. Trustworthiness of the data was achieved by applying researcher triangulation, negative case analysis and an audit trail. Outcomes & ResultsAnalyses of the interviews revealed three major themes of importance to the children: (1) treatment values, (2) treatment practices and (3) treatment outcomes. Each theme was divided into different subthemes. The theme 'treatment values' consisted of the subthemes expectations and emotions around therapy and interference with daily living. Information flow, therapy content, confirmation and rewards, parents' attendance, therapy intensity, and homework were subthemes of the major theme 'treatment practices'. The theme 'treatment outcomes' was divided into two subthemes, namely speech improvement and peers' reactions. Conclusions & ImplicationsMost children had positive attitudes towards speech therapy: it was 'something they liked' and 'something fun'. If children had negative attitudes they were related to having a fear of making mistakes during therapy. Children had clear expectations of the purpose of speech therapy. Speech therapy should 'help' improve their speech and make it more understandable to others. The children in this sample made some suggestions to decrease the experienced burden related to speech therapy. The results of this study will help to better tailor speech therapy programmes to the needs and experiences of children with a CP +/- L. WHAT THIS PAPER ADDSWhat is already known on the subjectEvidence-based speech therapy involves the integration of (1) the scientific evidence for therapy with (2) the perspectives of clinicians concerning therapy and (3) the perspectives of patients and their families about therapy. The cleft literature has already paid attention to the first two cornerstones of evidence-based speech therapy. Different studies investigated the perspectives of SLPs and parents with regard to cleft palate speech therapy. However, much less is known about the children's own experiences with and perceptions around this speech therapy. What this study adds to existing knowledgeThis study used a qualitative research design to investigate the perceptions, emotions and expectations of children with a cleft (lip and) palate, aged 5-12 years, with regard to the speech therapy they receive. Speech therapy needed to focus on the elimination of compensatory speech errors. This study provides knowledge on the speech therapy-related experiences of children with a cleft palate. What are the potential or actual clinical implications of this work?Children in this sample made some concrete suggestions to decrease the experienced burden related to cleft speech therapy, for example, integration of school work during therapy sessions and practising on the level of spontaneous speech. The results of this study help us to better tailor speech therapy programmes to the needs and experiences of children with a CP +/- L.","2023-09","2025-02-26 20:45:06","2025-02-26 20:45:06","","1526-1538","","5","58","","","","","","","","","","English","","","","WOS:000971342300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;3<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;45</p>","","","ARTICULATION; CARE; children; cleft lip and palate; EXPERIENCES; INTERVENTION; LIP; PARENT; qualitative research","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"VLSNR3KK","journalArticle","2022","Garcia-Argibay, M; Hiyoshi, A; Fall, K; Montgomery, S","Association of 5α-Reductase Inhibitors With Dementia, Depression, and Suicide","JAMA NETWORK OPEN","","2574-3805","10.1001/jamanetworkopen.2022.48135","","Importance In recent decades, there has been increased interest in the possible adverse neurological effects of 5 alpha-reductase inhibitors (5-ARIs), which have been used mainly for benign prostatic hyperplasia and androgenic alopecia. Numerous studies and reports have indicated associations of 5-ARIs with depression and suicide. However, most of these studies had methodological shortcomings, and very little is known about the potential association of 5-ARIs with dementia. Objective To investigate the association of 5-ARI use with all-cause dementia, Alzheimer disease, vascular dementia, depression, and suicide. Design, Setting, and Participants This Swedish register-based cohort study included 2236876 men aged 50 to 90 years between July 1, 2005, and December 31, 2018. Statistical analyses were performed from September 15, 2021, to May 25, 2022. Main Outcomes and Measures A diagnosis of all-cause dementia, Alzheimer disease, vascular dementia, depression, or completed suicide. Exposures A recorded prescription in the Swedish national prescription register of finasteride or dutasteride and duration of use. Results Of 2236876 men (median age at the start of follow-up, 55 years [IQR, 50-65 years] and at treatment initiation, 73 years [IQR, 66-80 years]), 70645 (3.2%) started finasteride treatment, and 8774 (0.4%) started dutasteride treatment. Men taking finasteride or dutasteride were at increased risk of all-cause dementia (finasteride: hazard ratio [HR], 1.22 [95% CI, 1.17-1.28]; dutasteride: HR, 1.10 [95% CI, 1.01-1.20]), Alzheimer disease (finasteride: HR, 1.20 [95% CI, 1.10-1.31]; dutasteride: HR, 1.28 [95% CI, 1.09-1.50]), vascular dementia (finasteride: HR, 1.44 [95% CI, 1.30-1.58]; dutasteride: HR, 1.31 [95% CI, 1.08-1.59]), and depression (finasteride: HR, 1.61 [95% CI, 1.48-1.75]; dutasteride: HR, 1.68 [95% CI, 1.43-1.96]). However, the magnitude of the association decreased over time, and the findings became statistically nonsignificant with continuous exposures over 4 years, except for depression, which showed a constant risk over time, with no differences between finasteride and dutasteride. In contrast, 5-ARIs were not associated with suicide (finasteride: HR, 1.22 [95% CI, 0.99-1.49]; dutasteride: HR, 0.98 [95% CI, 0.62-1.54]). Conclusions and Relevance This cohort study found that, while men receiving 5-ARI treatment showed a higher risk for dementia in the initial periods after starting treatment, the decreasing magnitude of the association over time suggested that the risk may be, entirely or in part, due to increased dementia detection among patients with benign prostate enlargement. Both finasteride and dutasteride were similarly associated with depression with a constant risk over time, while neither drug was associated with suicide. Prescribing clinicians and potential users should be aware of the possible risks for depression associated with 5-ARI use. This population-based cohort study investigates the association of 5 alpha-reductase inhibitor use with all-cause dementia, Alzheimer disease, vascular dementia, depression, and suicide among men in Sweden.","2022-12-22","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","12","5","","","","","","","","","","English","","","","WOS:000919597900005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;13<br/>Total Times Cited:&nbsp;&nbsp;14<br/>Cited Reference Count:&nbsp;&nbsp;39</p>","","","ATLAS; BRAIN; COGNITION; HEALTH; LOW-DOSE FINASTERIDE; NEUROSTEROIDS; OLDER MEN; RISK; SYMPTOMS; TESTOSTERONE","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"BNZV7TXK","journalArticle","2021","Rozhon, J; Rezac, F; Jalowiczor, J; Behan, L","Augmenting Speech Quality Estimation in Software-Defined Networking Using Machine Learning Algorithms","SENSORS","","1424-8220","10.3390/s21103477","","With the increased number of Software-Defined Networking (SDN) installations, the data centers of large service providers are becoming more and more agile in terms of network performance efficiency and flexibility. While SDN is an active and obvious trend in a modern data center design, the implications and possibilities it carries for effective and efficient network management are not yet fully explored and utilized. With most of the modern Internet traffic consisting of multimedia services and media-rich content sharing, the quality of multimedia communications is at the center of attention of many companies and research groups. Since SDN-enabled switches have an inherent feature of monitoring the flow statistics in terms of packets and bytes transmitted/lost, these devices can be utilized to monitor the essential statistics of the multimedia communications, allowing the provider to act in case of network failing to deliver the required service quality. The internal packet processing in the SDN switch enables the SDN controller to fetch the statistical information of the particular packet flow using the PacketIn and Multipart messages. This information, if preprocessed properly, can be used to estimate higher layer interpretation of the link quality and thus allowing to relate the provided quality of service (QoS) to the quality of user experience (QoE). This article discusses the experimental setup that can be used to estimate the quality of speech communication based on the information provided by the SDN controller. To achieve higher accuracy of the result, latency characteristics are added based on the exploiting of the dummy packet injection into the packet stream and/or RTCP packet analysis. The results of the experiment show that this innovative approach calculates the statistics of each individual RTP stream, and thus, we obtain a method for dynamic measurement of speech quality, where when quality decreases, it is possible to respond quickly by changing routing at the network level for each individual call. To improve the quality of call measurements, a Convolutional Neural Network (CNN) was also implemented. This model is based on two standard approaches to measuring the speech quality: PESQ and E-model. However, unlike PESQ/POLQA, the CNN-based model can take delay into account, and unlike the E-model, the resulting accuracy is much higher.","2021-05","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","10","21","","","","","","","","","","English","","","","WOS:000662513600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;26</p>","","","artificial neural networks; MODEL; OpenFlow; software defined networks; speech analysis; VOIP","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"2JA38CBS","journalArticle","2023","Mootassim-Billah, S; Van Nuffelen, G; Schoentgen, J; De Bodt, M; Van Gestel, D","Assessment of radio(chemo)therapy-related dysphagia in head and neck cancer patients based on cough-related acoustic features: a prospective phase II national clinical trial (ACCOUGH-P/A trial)","TRIALS","","1745-6215","10.1186/s13063-023-07660-y","","Background Radiation-associated dysphagia is defined as impaired swallowing efficiency/safety following (chemo)radiotherapy in head and neck cancer patients. In a dysphagia framework, impaired coughing may lead to lung aspiration and fatal lung infection. Although cough efficacy is a predictor of the risk of aspiration, cough investigation is minimal in patients with radiation-associated dysphagia. Because cough is a transient signal, existing software for speech analysis are not appropriate. The goal of our project is to develop an assessment method using acoustic features related to voluntary and reflexive coughs as biomarkers of the risk of penetration/aspiration in patients with radiation-associated dysphagia.Methods Healthy subjects and head and neck cancer patients with and without dysphagia will produce voluntary coughs, throat clearings and reflexive coughs. Recordings will be made using an acoustic microphone and a throat microphone. The recorded signals will be manually segmented and subsequently analysed with a software under development. Automatic final segmentation enables to measure cough duration. The first method of analysis includes temporal features: the amplitude contour, the sample entropy and the kurtosis. These features report respectively the strength, the unpredictability (turbulence noise due to the air jet) and the impulsive quality (burst) of the signal. The second method of analysis consists of a spectral decomposition of the relative cough signal energy into several frequency bands (0-400 Hz, 400-800 Hz, 800-1600 Hz, 1600-3200 Hz, > 3200 Hz). The primary outcome of this exploratory research project is the identification of a set of descriptive acoustic cough features in healthy subjects as reference data (ACCOUGH). The secondary outcome of this research in head and neck cancer patients with radiation-associated dysphagia includes the identification of (1) a set of descriptive acoustic cough features as biomarkers of penetration-aspiration (ACCOUGH-P/A), (2) swallowing scores, (3) voice features and (4) aerodynamic cough features.Discussion This study is expected to develop methods of acoustic cough analysis to enhance the assessment of radiation-associated dysphagia in head and neck cancer patients following (chemo)radiation.","2023-09-29","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","24","","","","","","","","","","English","","","","WOS:001075322500001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;55</p>","","","(Chemo)radiotherapy; Acoustic analysis; ASPIRATION PNEUMONIA; Assessment; CHEMORADIATION; CONCURRENT CHEMORADIOTHERAPY; Cough; Head and neck cancer patients; PENETRATION; RADIATION; Radiation-associated dysphagia; RADIOTHERAPY; REFLEX COUGH; SEVERITY; SWALLOWING FUNCTION; VOICE QUALITY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"7HE9M56Q","journalArticle","2024","Wang, YJ; Hattori, M; Liu, RG; Sumita, YI","Digital acoustic analysis of the first three formant frequencies in patients with a prosthesis after maxillectomy","JOURNAL OF PROSTHETIC DENTISTRY","","0022-3913","10.1016/j.prosdent.2022.10.010","","Statement of problem. Prosthetic rehabilitation with an obturator can help to restore or improve the intelligibility of speech in patients after maxillectomy. The frequency of formants 1 and 2 as well as their ranges were initially reported in patients with maxillary defects in 2002, and the evaluation method that was used is now applied in clinical evaluation. However, the details of formant 3 are not known and warrant investigation because, according to speech science, formant 3 is related to the pharyngeal volume. Clarifying the formant frequency values of formant 3 in patients after maxillectomy would enable prosthodontists to refer to these data when planning treatment and when assessing the outcome of an obturator. Purpose. The purpose of this clinical study was to determine the acoustic characteristics of formant 3, together with those of formants 1 and 2, by using a digital acoustic analysis during maxillofacial prosthetic treatment. The utility of determining formant 3 in the evaluation of speech in patients after maxillectomy was also evaluated. Material and methods. Twenty-six male participants after a maxillectomy (mean age, 63 years; range, 20 to 93 years) were included, and the 5 Japanese vowels /a/, /e/, /i/, /o/, and /u/ produced with and without a definitive obturator prosthesis were recorded. The frequencies of the 3 formants were determined, and their ranges were calculated by using a speech analysis system (Computerized Speech Lab CSL 4400). The Wilcoxon signed rank test was used to compare the formants between the 2 use conditions (alpha=0.05). Results. Significant differences were found in the frequencies and ranges of all 3 formants between the use conditions. The ranges of all 3 formants produced with the prosthesis were significantly greater than those produced without it. Conclusions. Based on the findings, both the first 2 formants and the third formant were changed by wearing an obturator prosthesis. Because formant 3 is related to the volume of the pharynx, evaluation of this formant and its range can reflect the effectiveness of the prosthesis to seal the oronasal communication and help reduce hypernasality, suggesting the utility of formant 3 analysis in prosthodontic rehabilitation.","2024-11","2025-02-26 20:45:06","2025-02-26 20:45:06","","1082-1087","","5","132","","","","","","","","","","English","","","","WOS:001356601800001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;32</p>","","","OBTURATOR PROSTHESES; QUALITY-OF-LIFE; SPEECH; SURGERY; SYSTEM; VOWELS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"AZL5YK74","journalArticle","2021","Noort, MC; Reader, TW; Gillespie, A","Sounds of silence: Data for analysing muted safety voice in speech","DATA IN BRIEF","","2352-3409","10.1016/j.dib.2021.107186","","Transcribed text from simulated hazards contains important content relevant for preventing harm. By capturing and analysing the content of speech when people raise (safety voice) or withhold safety concerns (safety silence), communication patterns may be identified for when individuals perceive risk, and safety management may be improved through identifying potential antecedents. This dataset contains transcribed speech from 404 participants (n(students) = 377; n(female) = 277, Age M-(sd) = 22.897((5.386))) engaged in a simulated hazardous scenario (walking across an unsafe plank), capturing 18,078 English words (M-(sd) = 46.117((37.559))). The data was collected through the Walking the plank paradigm (Noort et al, 2019), which provides a validated laboratory experiment designed for the direct observation of communication in response to hazardous scenarios that elicit safety concerns. Three manipulations were included in the design: hazard salience (salient vs not salient), responsibilities (clear vs diffuse) and encouragements (encouraged vs discouraged). Speech between two set timepoints in the hazardous scenario was transcribed based on video recordings and coded in terms of the extent to which speech involved safety voice or safety silence. Files contain i) a.csv containing the raw data, ii) a.csv providing variable description, iii) a Jupyter notebook (v. 3.7) providing the statistical code for the accompanying research article, iv) a.html version of the Jupyter notebook, v) a.html file providing the graph for the.html Jupyter notebook, vi) speech dictionaries, and vii) a copy of the electronic questionnaire. The data and supplemental files enable future research through providing a dataset in which participants can be distinguished in terms of the extent to which they are concerned and raise or withhold this. It enables speech and conversation analyses and the Jupyter notebook may be adapted to enable the parsing and coding of text using provided, existing and custom dictionaries. This may lead to the identification of communication patterns and potential interventions for unmuting safety voice. This data-in-brief is published alongside the research article: M. C. Noort, T.W. Reader, A. Gillespie. (2021). The sounds of safety silence: Interventions and temporal patterns unmute unique safety voice content in speech. Safety Science. (C) 2021 The Authors. Published by Elsevier Inc.","2021-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","37","","","","","","","","","","English","","","","WOS:000689356400003","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;3</p>","","","communication, hazards ,speech analysis; experiments; Safety voice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"6LAMV52Z","journalArticle","2025","Mswahili, ME; Hwang, J; Rajapakse, JC; Jo, K; Jeong, YS","Positional embeddings and zero-shot learning using BERT for molecular-property prediction","JOURNAL OF CHEMINFORMATICS","","1758-2946","10.1186/s13321-025-00959-9","","Recently, advancements in cheminformatics such as representation learning for chemical structures, deep learning (DL) for property prediction, data-driven discovery, and optimization of chemical data handling, have led to increased demands for handling chemical simplified molecular input line entry system (SMILES) data, particularly in text analysis tasks. These advancements have driven the need to optimize components like positional encoding and positional embeddings (PEs) in transformer model to better capture the sequential and contextual information embedded in molecular representations. SMILES data represent complex relationships among atoms or elements, rendering them critical for various learning tasks within the field of cheminformatics. This study addresses the critical challenge of encoding complex relationships among atoms in SMILES strings to explore various PEs within the transformer-based framework to increase the accuracy and generalization of molecular property predictions. The success of transformer-based models, such as the bidirectional encoder representations from transformer (BERT) models, in natural language processing tasks has sparked growing interest from the domain of cheminformatics. However, the performance of these models during pretraining and fine-tuning is significantly influenced by positional information such as PEs, which help in understanding the intricate relationships within sequences. Integrating position information within transformer architectures has emerged as a promising approach. This encoding mechanism provides essential supervision for modeling dependencies among elements situated at different positions within a given sequence. In this study, we first conduct pretraining experiments using various PEs to explore diverse methodologies for incorporating positional information into the BERT model for chemical text analysis using SMILES strings. Next, for each PE, we fine-tune the best-performing BERT (masked language modeling) model on downstream tasks for molecular-property prediction. Here, we use two molecular representations, SMILES and DeepSMILES, to comprehensively assess the potential and limitations of the PEs in zero-shot learning analysis, demonstrating the model's proficiency in predicting properties of unseen molecular representations in the context of newly proposed and existing datasets.Scientific contributionThis study explores the unexplored potential of PEs using BERT model for molecular property prediction. The study involved pretraining and fine-tuning the BERT model on various datasets related to COVID-19, bioassay data, and other molecular and biological properties using SMILES and DeepSMILES representations. The study details the pretraining architecture, fine-tuning datasets, and the performance of the BERT model with different PEs. It also explores zero-shot learning analysis and the model's performance on various classification and regression tasks. In this study, newly proposed datasets from different domains were introduced during fine-tuning in addition to the existing and commonly used datasets. The study highlights the robustness of the BERT model in predicting chemical properties and its potential applications in cheminformatics and bioinformatics.","2025-02-05","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","17","","","","","","","","","","English","","","","WOS:001413927800002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;66</p>","","","ALGORITHM; BERT; DATABASE; DeepSMILES; DRUG DISCOVERY; GRAPH NEURAL-NETWORK; MODELS; Molecular-property prediction; Positional embedding/encoding; SMILES; Transformers; Zero-shot learning","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GF4BSGFQ","journalArticle","2024","Chen, LT; Sun, HF; Wang, ZF; Zhang, T; Zhang, HL; Wang, W; Sun, XH; Duan, J; Gao, Y; Zhao, LN","Deep learning architecture with shunted transformer and 3D deformable convolution for voxel-level dose prediction of head and neck tumors","PHYSICAL AND ENGINEERING SCIENCES IN MEDICINE","","2662-4729","10.1007/s13246-024-01462-5","","Intensity-modulated radiation therapy (IMRT) has been widely used in treating head and neck tumors. However, due to the complex anatomical structures in the head and neck region, it is challenging for the plan optimizer to rapidly generate clinically acceptable IMRT treatment plans. A novel deep learning multi-scale Transformer (MST) model was developed in the current study aiming to accelerate the IMRT planning for head and neck tumors while generating more precise prediction of the voxel-level dose distribution. The proposed end-to-end MST model employs the shunted Transformer to capture multi-scale features and learn a global dependency, and utilizes 3D deformable convolution bottleneck blocks to extract shape-aware feature and compensate the loss of spatial information in the patch merging layers. Moreover, data augmentation and self-knowledge distillation are used to further improve the prediction performance of the model. The MST model was trained and evaluated on the OpenKBP Challenge dataset. Its prediction accuracy was compared with three previous dose prediction models: C3D, TrDosePred, and TSNet. The predicted dose distributions of our proposed MST model in the tumor region are closest to the original clinical dose distribution. The MST model achieves the dose score of 2.23 Gy and the DVH score of 1.34 Gy on the test dataset, outperforming the other three models by 8%-17%. For clinical-related DVH dosimetric metrics, the prediction accuracy in terms of mean absolute error (MAE) is 2.04% for D99\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$D_{99}$$\end{document}, 1.54% for D95\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$D_{95}$$\end{document}, 1.87% for D1\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$D_1$$\end{document}, 1.87% for Dmean\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$D_{mean}$$\end{document}, 1.89% for D0.1cc\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$D_{0.1cc}$$\end{document}, respectively, superior to the other three models. The quantitative results demonstrated that the proposed MST model achieved more accurate voxel-level dose prediction than the previous models for head and neck tumors. The MST model has a great potential to be applied to other disease sites to further improve the quality and efficiency of radiotherapy planning.","2024-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","1501-1512","","4","47","","","","","","","","","","English","","","","WOS:001285405100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;31</p>","","","Deep learning; Dose prediction; Head and neck tumors; MODULATED RADIATION-THERAPY; Multi-scale transformer model","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZY3EMLK9","journalArticle","2022","Bruneel, L; Danhieux, A; Van Lierde, K","Training speech pathology students in the perceptual evaluation of speech in patients with cleft palate: Reliability results and the students? perspective","INTERNATIONAL JOURNAL OF PEDIATRIC OTORHINOLARYNGOLOGY","","0165-5876","10.1016/j.ijporl.2022.111145","","Introduction: Listener training is necessary to achieve agreement between perceptual ratings. However, evidence is limited regarding the effect of a training in the perceptual evaluation of cleft palate speech on the reliability of students in speech pathology. The aim of this study was to evaluate the short-term and longer-term effect of a training using the Belgian Dutch outcome tool for cleft palate speech on reliability and the students' experiences.Methods: A 2-h training was provided during a specialist course for 31 first year master students in speech pathology. Information regarding listener characteristics, speech documentation and speech analysis (speech parameters and rating scales) was discussed. This last part focused on the Belgian Dutch outcome tool for speech in patients with cleft palate, including reference samples and consensus listening exercises. Pre training (T1) and post-training reliability (immediately following the training (T2) and one month later (T3)) were collected for the speech variables speech understandability, speech acceptability, hypernasality, hyponasality, nasal emission and nasal turbulence that were rated by means of ordinal scales. Inter-rater reliability was determined with interclass correlation coefficients whereas percentages of agreement were calculated for the intra-rater reliability.Results: Qualitative inter-rater ICC categories improved for hyponasality and speech acceptability at T2 and T3, whereas the inter-rater reliability of the other variables remained stable regarding their interpreted categories. All ICC values quantitatively improved at T2, except hypernasality and nasal emission. At T3, three of those variables showed a small drop one month later. Regarding intra-rater reliability, a significant decrease for hypernasality and nasal turbulence was found at T2, whereas a significant increase was observed for nasal turbulence and speech acceptability at T3. All variables, except hypernasality, had improved intra-rater reliability at T3 in comparison with T1. Students experienced fewer difficulties with most variables after training. Evaluation of nasal emission and nasal turbulence remained however difficult and required, in the students' opinion, more attention.Discussion & conclusion: Overall a positive training effect was noted, especially regarding the inter-rater reliability and the students' confidence in perceptual ratings. However, it remains a challenge to achieve a positive training effect for all target variables that is maintained over-time.","2022-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","157","","","","","","","","","","English","","","","WOS:000797836300005","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;2<br/>Total Times Cited:&nbsp;&nbsp;3<br/>Cited Reference Count:&nbsp;&nbsp;45</p>","","","AUDIT PROTOCOL; CHILDREN; Cleft palate; HYPERNASALITY; JUDGMENTS; LISTENER EXPERIENCE; NASAL AIR-FLOW; Perceptual evaluation; PHARYNGEAL FLAP SURGERY; RATINGS; Students; Training; UNDERSTANDABILITY; VOICE QUALITY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"Q7NKAJ8U","journalArticle","2024","Ali, AJ; Ahmed, AA; Abbod, MF","Groundwater level predictions in the Thames Basin, London over extended horizons using Transformers and advanced machine learning models","JOURNAL OF CLEANER PRODUCTION","","0959-6526","10.1016/j.jclepro.2024.144300","","This study breaks new ground by using the Temporal Fusion Transformer (TFT) method for groundwater level prediction, addressing the complex dynamics of the Thames Basin aquifer in England. Our research combines extensive hydrological data collected from the Thames Basin with advanced machine learning, where a complex network of rivers and streams substantially affects groundwater dynamics. Unlike previous studies, this research focuses on long-term forecasting with deep learning, offering, for the first time, a 60-day prediction horizon based on daily data. To rigorously examine the model performance and robustness on new, unseen data, we applied the walk-forward validation method and other matrices such as RMSE and R2 coupled with the Holdout technique. The models used were Long Short-Term Memory (LSTM), Attention-based LSTM, LSTM with Bayesian optimisation, Attention-based LSTM with Bayesian optimisation and TFT. They were used on the basin's Chalk, Jurassic Limestone, and Lower greensand aquifers. Whilst both LSTM models were optimised using the Bayesian technique, TFT was applied for its inherent capability in complex time series. Our methodology processed historical groundwater and rainfall data from 2001 to 2023, accounting for the potential lag in aquifer response to the proximity of the river system. The dataset served as training, validation, and holdout for each model, focusing on capturing the dynamic temporal fluctuation. The results clearly showed the superiority of the TFT model in all aquifer types compared to other models across all horizons. The Limestone had the greatest result in the 7-day projections, with an RMSE of 0.02 and R2 of 0.98; Whilst the Chalk and Lower greensand, had RMSEs of 0.03 with R2 values of 0.75 and 0.95, respectively. The Limestone aquifer performed best for the 30-day horizon again (RMSE = 0.06, R2 = 0.85), with the Chalk and Lower greensand aquifer yielding RMSE of 0.04 and 0.12 and R2 values of 0.64 and 0.74, respectively. In the 60 days predictions, the best results were observed in the limestone aquifer with RMSE of 0.09 and R2 of 0.65 in holdout validation. However, in chalk and lower greensand aquifers, the TFT showed RMSEs of 0.05 and 0.15 and R2s of 0.45 and 0.58, respectively. Traditional LSTM models demonstrated limited predictive power compared to the main model TFT, while the attention mechanism slightly improved the accuracy. This study not only sets a new benchmark in hydrological modelling accuracy but also highlights the potential of advanced machine learning in managing complex aquifers and predicting the water table.","2024-12-15","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","484","","","","","","","","","","English","","","","WOS:001373760300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;102</p>","","","AI; AQUIFER; CLIMATE-CHANGE; Deep learning; FLOW; Fractured porous media; IMPACTS; Long-term time-series predictions; LSTM; NEURAL-NETWORK; OPTIMIZATION; SIMULATION; Transformer model; VALLEY; WATER-QUALITY","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"PLQ9MZ2V","journalArticle","2025","Guo, YS; Wang, J; Zhong, Y; Wang, T; Sui, ZY","A Novel Electrical Load Forecasting Model for Extreme Weather Events Based on Improved Gated Spiking Neural P Systems and Frequency Enhanced Channel Attention Mechanism","IEEE ACCESS","","2169-3536","10.1109/ACCESS.2025.3525479","","Accurate short-term load forecasting (LF) under extreme weather is vital for the sustainable development of energy systems. This paper proposes a basic framework for future load forecasting researches of sustainable energy systems under extreme weather events and provides new direction for membrane computing model in terms of load forecasting. Inspired by nonlinear spiking mechanisms in nonlinear spiking neural P systems, the gated spiking neural P (GSNP) model is a new recurrent-like network. In this study, we develop an innovative membrane computing model, termed frequency attention temporal convolutional network-load forecasting-frequency attention gated spiking neural P (FATCN-LF-FAGSNP) model. Frequency enhanced channel attention mechanism (FECAM) is utilized to enhance the features extraction ability of temporal convolutional network (TCN) and improve prediction ability of GSNP systems. FATCN fully extracts the temporal relationship of features, the features of each channel interact with each frequency component to learn more temporal information effectively and comprehensively in frequency domain. Moreover, adding FECAM to extract features from the data fully reveals the relationship between influencing factors and the load series, which improves the quality of data features and the forecasting accuracy of the FAGSNP model. Then inspired by the interaction mechanism of impulses between biological neuronal cells, FAGSNP is able to consider the load variability and effectively predict load trends. In addition, to address load prediction challenges posed by extreme weather and promote the sustainable development of power systems, the proposed model integrates many models to solve this problem. First, optimized variational mode decomposition (VMD) is used to decompose the load series and the sub-sequences are combined with relevant features, to form the different input sequences of the prediction model. Then, FATCN-LF-FAGSNP model is developed to accurately forecast each high frequency component. Subsequently inverted Transformer model and Informer model are utilized to predict low frequency components and residual component, respectively. Finally all predicted components are reconstructed to get the final predicted results. We conducted extensive comparative experiments with ten baseline models on three real-world datasets, compared with GSNP model and TCN-GSNP model, the coefficient of determination (R-2) of the FATCN-LF-FAGSNP model increases and mean absolute percentage error (MAPE), mean absolute error (MAE) and relative absolute error (RAE) reduce, the LF accuracy (measured by R-2) of the proposed hybrid model gets 99.7% in seasonal LF task. In addition, the proposed hybrid model gets the best in MAPE, MAE, R-2 and RAE metrics in all cases, which demonstrates the effectiveness of the proposed model in LF tasks under both extreme weather scenarios and seasonal prediction scenarios.","2025","2025-02-26 20:45:06","2025-02-26 20:45:06","","4884-4911","","","13","","","","","","","","","","English","","","","WOS:001395432500011","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;43</p>","","","extreme weather events; frequency enhanced channel attention mechanism; gated spiking neural P; informer; inverted transformer; Short-term load forecasting; temporal convolutional network; variational mode decomposition","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4U7DGF9H","journalArticle","2022","Zhang, H; Zhang, JJ; Ni, WD; Jiang, YL; Liu, KJ; Sun, DY; Li, J","Transformer- and Generative Adversarial Network-Based Inpatient Traditional Chinese Medicine Prescription Recommendation: Development Study","JMIR MEDICAL INFORMATICS","","2291-9694","10.2196/35239","","Background: Traditional Chinese medicine (TCM) practitioners usually follow a 4-step evaluation process during patient diagnosis: observation, auscultation, olfaction, inquiry, pulse feeling, and palpation. The information gathered in this process, along with laboratory test results and other measurements such as vital signs, is recorded in the patient's electronic health record (EHR). In fact, all the information needed to make a treatment plan is contained in the EHR; however, only a seasoned TCM physician could use this information well to make a good treatment plan as the reasoning process is very complicated, and it takes years of practice for a medical graduate to master the reasoning skill. In this digital medicine era, with a deluge of medical data, ever-increasing computing power, and more advanced artificial neural network models, it is not only desirable but also readily possible for a computerized system to mimic the decision-making process of a TCM physician. Objective: This study aims to develop an assistive tool that can predict prescriptions for inpatients in a hospital based on patients' clinical EHRs. Methods: Clinical health records containing medical histories, as well as current symptoms and diagnosis information, were used to train a transformer-based neural network model using the corresponding physician's prescriptions as the target. This was accomplished by extracting relevant information, such as the patient's current illness, medicines taken, nursing care given, vital signs, examinations, and laboratory results from the patient's EHRs. The obtained information was then sorted chronologically to produce a sequence of data for the patient. These time sequence data were then used as input to a modified transformer network, which was chosen as a prescription prediction model. The output of the model was the prescription for the patient. The ultimate goal is for this tool to generate a prescription that matches what an expert TCM physician would prescribe. To alleviate the issue of overfitting, a generative adversarial network was used to augment the training sample data set by generating noise-added samples from the original training samples. Results: In total, 21,295 copies of inpatient electronic medical records from Guang'anmen Hospital were used in this study. These records were generated between January 2017 and December 2018, covering 6352 types of medicines. These medicines were sorted into 819 types of first-category medicines based on their class relationships. As shown by the test results, the performance of a fully trained transformer model can have an average precision rate of 80.58% and an average recall rate of 68.49%. Conclusions: As shown by the preliminary test results, the transformer-based TCM prescription recommendation model outperformed the existing conventional methods. The extra training samples generated by the generative adversarial network help to overcome the overfitting issue, leading to further improved recall and precision rates.","2022-05","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","5","10","","","","","","","","","","English","","","","WOS:000809334500002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;10<br/>Total Times Cited:&nbsp;&nbsp;11<br/>Cited Reference Count:&nbsp;&nbsp;37</p>","","","artificial intelligence; electronic health records; generative adversary networks; machine learning; natural language processing; RECORDS; traditional Chinese medicine; transformer; word2Vec","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"4SND839A","journalArticle","2022","Birnbaum, ML; Abrami, A; Heisig, S; Ali, A; Arenare, E; Agurto, C; Lu, N; Kane, JM; Cecchi, G","Acoustic and Facial Features From Clinical Interviews for Machine Lear ning-Based Psychiatric Diagnosis: Algorithm Development","JMIR MENTAL HEALTH","","2368-7959","10.2196/24699","","Background: In contrast to all other areas of medicine, psychiatry is still nearly entirely reliant on subjective assessments such as patient self-report and clinical observation. The lack of objective information on which to base clinical decisions can contribute to reduced quality of care. Behavioral health clinicians need objective and reliable patient data to support effective targeted interventions. Objective: We aimed to investigate whether reliable inferences-psychiatric signs, symptoms, and diagnoses-can be extracted from audiovisual patterns in recorded evaluation interviews of participants with schizophrenia spectrum disorders and bipolar disorder. Methods: We obtained audiovisual data from 89 participants (mean age 25.3 years; male: 48/89, 53.9%; female: 41/89, 46.1%): individuals with schizophrenia spectrum disorders (n=41), individuals with bipolar disorder (n=21), and healthy volunteers (n=27). We developed machine learning models based on acoustic and facial movement features extracted from participant interviews to predict diagnoses and detect clinician-coded neuropsychiatric symptoms, and we assessed model performance using area under the receiver operating characteristic curve (AUROC) in 5-fold cross-validation. Results: The model successfully differentiated between schizophrenia spectrum disorders and bipolar disorder (AUROC 0.73) when aggregating face and voice features. Facial action units including cheek-raising muscle (AUROC 0.64) and chin-raising muscle (AUROC 0.74) provided the strongest signal for men. Vocal features, such as energy in the frequency band 1 to 4 kHz (AUROC 0.80) and spectral harmonicity (AUROC 0.78), provided the strongest signal for women. Lip corner-pulling muscle signal discriminated between diagnoses for both men (AUROC 0.61) and women (AUROC 0.62). Several psychiatric signs and symptoms were successfully inferred: blunted affect (AUROC 0.81), avolition (AUROC 0.72), lack of vocal inflection (AUROC 0.71), asociality (AUROC 0.63), and worthlessness (AUROC 0.61). Conclusions: This study represents advancement in efforts to capitalize on digital data to improve diagnostic assessment and supports the development of a new generation of innovative clinical tools by employing acoustic and facial data analysis.","2022-01-24","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","9","","","","","","","","","","English","","","","WOS:000749560800004","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;14<br/>Total Times Cited:&nbsp;&nbsp;16<br/>Cited Reference Count:&nbsp;&nbsp;79</p>","","","audiovisual; audiovisual patterns; bipolar disorder; CODING SYSTEM; DEPRESSION; diagnostic prediction; DISORDERS; EMOTION; facial analysis; machine learning; METAANALYSIS; PREVALENCE; psychiatry; RATING-SCALE; schizophrenia; SCHIZOPHRENIA; schizophrenia spectrum disorders; SCREENING TOOLS; spectrum disorders; speech; SPEECH; speech analysis; symptom prediction","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"THKRTDBR","journalArticle","2024","Zhang, HKK; Luo, D; Li, ZB","Classifying raw irregular time series (CRIT) for large area land cover mapping by adapting transformer model","SCIENCE OF REMOTE SENSING","","2666-0172","10.1016/j.srs.2024.100123","","For Landsat land cover classification, the time series observations are typically irregular in the number of observations in a period (e.g., a year) and acquisition dates due to cloud cover variations over large areas and acquisition plan variations over long periods. Compositing or temporal percentile calculation are usually used to transform the irregular time series to regular temporal variables so that the machine and deep learning classifiers can be applied. Recognizing that the composite and percentile calculations have information loss, this study presents a method directly Classifying the Raw Irregular Time series (CRIT) ('raw' means irregular good-quality surface reflectance time series without any composite or temporal percentile derivation) by adapting Transformer. CRIT uses the acquisition day of year as classification input to align time series and also takes the Landsat satellite platform (Landsat 5, 7 and 8) as input to address the inter-sensor reflectance differences. The CRIT was demonstrated by classifying Landsat analysis ready data (ARD) surface reflectance time series acquired across one year for three years (1985, 2006 and 2018) over the Conterminous United States (CONUS) with both spatial and temporal variations in Landsat availability. 20,047 training and 4949 evaluation 30-m pixel were used where each pixel was annotated as one of seven land cover classes for each year. The CRIT was compared with classifying 16-day composite time series and temporal percentiles and compared with a 1D convolution neural network (CNN) method. Results showed that the CRIT trained with three years of samples had 1.4-1.5% higher overall accuracies with less computation time than classifying 16-day composites and 2.3-2.4% higher than classifying temporal percentiles. The CRIT advantages over 16-day composites were pronounced for developed (0.05 F1-score) and cropland (0.02 F1-score) classes and for mixed or boundary pixels. This was reasonable as the 16-day composites had only on average 7.02, 16.49 and 15.78 good quality observations for the three years, respectively, in contrast to 7.89, 27.72, and 26.60 for the raw irregular time series. The CNN was not as good as CRIT in classifying the raw irregular time series as CNN simply filling temporal positions with no observations as zeros while the CRIT used a masking mechanism to rule out their contribution. The CRIT can also take the pixel coordinates and DEM variables as input which further increased the overall accuracies by 1.1-2.6% and achieved 84.33%, 87.54% and 87.01% overall accuracies for the 1985, 2006 and 2018 classifications, respectively. The CRIT land cover maps were shown consistent with the USGS Land Change Monitoring, Assessment, and Projection (LCMAP) maps. The developed codes, training data and maps were made publicly available.","2024-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","9","","","","","","","","","","English","","","","WOS:001188202800001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;8<br/>Total Times Cited:&nbsp;&nbsp;8<br/>Cited Reference Count:&nbsp;&nbsp;129</p>","","","ATTENTION; CALIBRATION; CLASSIFICATION; CONTERMINOUS UNITED-STATES; CONUS; CROP; Irregular time series; Land cover classification; Landsat ARD; MODIS; NEURAL-NETWORKS; SEGMENTATION; SENTINEL-2; SURFACE REFLECTANCE; Transformer","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"WPXCBVGE","journalArticle","2021","Bose, A; Dash, NS; Ahmed, S; Dutta, M; Dutt, A; Nandi, R; Cheng, YS; Mello, TMD","Connected Speech Characteristics of Bengali Speakers With Alzheimer's Disease: Evidence for Language-Specific Diagnostic Markers","FRONTIERS IN AGING NEUROSCIENCE","","1663-4365","10.3389/fnagi.2021.707628","","Background and aim: Speech and language characteristics of connected speech provide a valuable tool for identifying, diagnosing and monitoring progression in Alzheimer's Disease (AD). Our knowledge of linguistic features of connected speech in AD is primarily derived from English speakers; very little is known regarding patterns of linguistic deficits in speakers of other languages, such as Bengali. Bengali is a highly inflected pro-drop language from the Indo-Aryan language family. It is the seventh most spoken language in the world, yet to date, no studies have investigated the profile of linguistic impairments in Bengali speakers with AD. The aim of this study was to characterize connected speech production and identify the linguistic features affected in Bengali speakers with AD. Methods: Participants were six Bengali speaking AD patients and eight matched controls from the urban metropolis, Kolkata, India. Narrative samples were elicited in Bengali using the Frog Story. Samples were analyzed using the Quantitative Production Analysis and the Correct Information Unit analyses to quantify six different aspects of speech production: speech rate, structural and syntactic measures, lexical measures, morphological and inflectional measures, semantic measures and measure of spontaneity and fluency disruptions. Results and conclusions: In line with the extant literature from English speakers, the Bengali AD participants demonstrated decreased speech rate, simplicity of sentence forms and structures, and reduced semantic content. Critically, differences with English speakers' literature emerged in the domains of Bengali specific linguistic features, such as the pro-drop nature of Bengali and its inflectional properties of nominal and verbal systems. Bengali AD participants produced fewer pronouns, which is in direct contrast with the overuse of pronouns by English AD participants. No obvious difficulty in producing nominal and verbal inflections was evident. However, differences in the type of noun inflections were evident; these were characterized by simpler inflectional features used by AD speakers. This study represents the first of its kind to characterize connected speech production in Bengali AD participants and is a significant step forward toward the development of language-specific clinical markers in AD. It also provides a framework for cross-linguistic comparisons across structurally distinct and under-explored languages.","2021-09-07","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","13","","","","","","","","","","English","","","","WOS:000697389600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;16<br/>Total Times Cited:&nbsp;&nbsp;16<br/>Cited Reference Count:&nbsp;&nbsp;81</p>","","","ADULTS; Alzheimer's disease; Bengali; COMPREHENSION; connected speech; DEMENTIA; DISCOURSE; micro-linguistics; NARRATIVE PRODUCTION; PICTURE DESCRIPTION; PRIMARY PROGRESSIVE APHASIA; pronoun; QUANTITATIVE-ANALYSIS; semantic; speech analysis; syntax; TEST SCORE; WORD-RETRIEVAL","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"MLTD85JQ","journalArticle","2024","Gwon, YN; Kim, JH; Chung, HS; Jung, EJ; Chun, J; Lee, SR; Shim, SR","The Use of Generative AI for Scientific Literature Searches for Systematic Reviews: ChatGPT and Microsoft Bing AI Performance Evaluation","JMIR MEDICAL INFORMATICS","","2291-9694","10.2196/51187","","Background: A large language model is a type of artificial intelligence (AI) model that opens up great possibilities for health care practice, research, and education, although scholars have emphasized the need to proactively address the issue of unvalidated and inaccurate information regarding its use. One of the best-known large language models is ChatGPT (OpenAI). It is believed to be of great help to medical research, as it facilitates more efficient data set analysis, code generation, and literature review, allowing researchers to focus on experimental design as well as drug discovery and development. Objective: This study aims to explore the potential of ChatGPT as a real-time literature search tool for systematic reviews and clinical decision support systems, to enhance their efficiency and accuracy in health care settings. Methods: The search results of a published systematic review by human experts on the treatment of Peyronie disease were selected as a benchmark, and the literature search formula of the study was applied to ChatGPT and Microsoft Bing AI as a comparison to human researchers. Peyronie disease typically presents with discomfort, curvature, or deformity of the penis in association with palpable plaques and erectile dysfunction. To evaluate the quality of individual studies derived from AI answers, we created a structured rating system based on bibliographic information related to the publications. We classified its answers into 4 grades if the title existed: A, B, C, and F. No grade was given for a fake title or no answer. Results: From ChatGPT, 7 (0.5%) out of 1287 identified studies were directly relevant, whereas Bing AI resulted in 19 (40%) relevant studies out of 48, compared to the human benchmark of 24 studies. In the qualitative evaluation, ChatGPT had 7 grade A, 18 grade B, 167 grade C, and 211 grade F studies, and Bing AI had 19 grade A and 28 grade C studies. Conclusions: This is the first study to compare AI and conventional human systematic review methods as a real-time literature collection tool for evidence -based medicine. The results suggest that the use of ChatGPT as a tool for real-time evidence generation is not yet accurate and feasible. Therefore, researchers should be cautious about using such AI. The limitations of this study using the generative pre -trained transformer model are that the search for research topics was not diverse and that it did not prevent the hallucination of generative AI. However, this study will serve as a standard for future studies by providing an index to verify the reliability and consistency of generative AI from a user's point of view. If the reliability and consistency of AI literature search services are verified, then the use of these technologies will help medical research greatly.","2024","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","12","","","","","","","","","","English","","","","WOS:001233902800001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;5<br/>Total Times Cited:&nbsp;&nbsp;5<br/>Cited Reference Count:&nbsp;&nbsp;32</p>","","","artificial intelligence; ChatGPT; clinical decision support system; decision support; education; evidence-based medicine; language model; search engine; support; systematic review; tool; treatment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"88DJLKZL","journalArticle","2024","Gwon, YN; Kim, JH; Chung, HS; Jung, EJ; Chun, J; Lee, S; Shim, SR","The Use of Generative AI for Scientific Literature Searches for Systematic Reviews: ChatGPT and Microsoft Bing AI Performance Evaluation","JMIR MEDICAL INFORMATICS","","2291-9694","10.2024/1/e51187","","Background: A large language model is a type of artificial intelligence (AI) model that opens up great possibilities for health care practice, research, and education, although scholars have emphasized the need to proactively address the issue of unvalidated and inaccurate information regarding its use. One of the best-known large language models is ChatGPT (OpenAI). It is believed to be of great help to medical research, as it facilitates more efficient data set analysis, code generation, and literature review, allowing researchers to focus on experimental design as well as drug discovery and development. Objective: This study aims to explore the potential of ChatGPT as a real -time literature search tool for systematic reviews and clinical decision support systems, to enhance their efficiency and accuracy in health care settings. Methods: The search results of a published systematic review by human experts on the treatment of Peyronie disease were selected as a benchmark, and the literature search formula of the study was applied to ChatGPT and Microsoft Bing AI as a comparison to human researchers. Peyronie disease typically presents with discomfort, curvature, or deformity of the penis in association with palpable plaques and erectile dysfunction. To evaluate the quality of individual studies derived from AI answers, we created a structured rating system based on bibliographic information related to the publications. We classified its answers into 4 grades if the title existed: A, B, C, and F. No grade was given for a fake title or no answer. Results: From ChatGPT, 7 (0.5%) out of 1287 identified studies were directly relevant, whereas Bing AI resulted in 19 (40%) relevant studies out of 48, compared to the human benchmark of 24 studies. In the qualitative evaluation, ChatGPT had 7 grade A, 18 grade B, 167 grade C, and 211 grade F studies, and Bing AI had 19 grade A and 28 grade C studies. Conclusions: This is the first study to compare AI and conventional human systematic review methods as a real -time literature collection tool for evidence-based medicine. The results suggest that the use of ChatGPT as a tool for real -time evidence generation is not yet accurate and feasible. Therefore, researchers should be cautious about using such AI. The limitations of this study using the generative pre-trained transformer model are that the search for research topics was not diverse and that it did not prevent the hallucination of generative AI. However, this study will serve as a standard for future studies by providing an index to verify the reliability and consistency of generative AI from a user's point of view. If the reliability and consistency of AI literature search services are verified, then the use of these technologies will help medical research greatly.","2024","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","12","","","","","","","","","","English","","","","WOS:001226395600002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;32</p>","","","artificial intelligence; ChatGPT; clinical decision support system; decision support; education; evidence-based medicine; language model; search engine; support; systematic review; tool; treatment","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"26K49WAF","journalArticle","2021","Ormerod, M; del Rincón, JM; Devereux, B","Predicting Semantic Similarity Between Clinical Sentence Pairs Using Transformer Models: Evaluation and Representational Analysis","JMIR MEDICAL INFORMATICS","","2291-9694","10.2196/23099","","Background: Semantic textual similarity (STS) is a natural language processing (NLP) task that involves assigning a similarity score to 2 snippets of text based on their meaning. This task is particularly difficult in the domain of clinical text, which often features specialized language and the frequent use of abbreviations. Objective: We created an NLP system to predict similarity scores for sentence pairs as part of the Clinical Semantic Textual Similarity track in the 2019 n2c2/OHNLP Shared Task on Challenges in Natural Language Processing for Clinical Data. We subsequently sought to analyze the intermediary token vectors extracted from our models while processing a pair of clinical sentences to identify where and how representations of semantic similarity are built in transformer models. Methods: Given a clinical sentence pair, we take the average predicted similarity score across several independently fine-tuned transformers. In our model analysis we investigated the relationship between the final model's loss and surface features of the sentence pairs and assessed the decodability and representational similarity of the token vectors generated by each model. Results: Our model achieved a correlation of 0.87 with the ground-truth similarity score, reaching 6th place out of 33 teams (with a first-place score of 0.90). In detailed qualitative and quantitative analyses of the model's loss, we identified the system's failure to correctly model semantic similarity when both sentence pairs contain details of medical prescriptions, as well as its general tendency to overpredict semantic similarity given significant token overlap. The token vector analysis revealed divergent representational strategies for predicting textual similarity between bidirectional encoder representations from transformers (BERT)-style models and XLNet. We also found that a large amount information relevant to predicting STS can be captured using a combination of a classification token and the cosine distance between sentence-pair representations in the first layer of a transformer model that did not produce the best predictions on the test set. Conclusions: We designed and trained a system that uses state-of-the-art NLP models to achieve very competitive results on a new clinical STS data set. As our approach uses no hand-crafted rules, it serves as a strong deep learning baseline for this task. Our key contribution is a detailed analysis of the model's outputs and an investigation of the heuristic biases learned by transformer models. We suggest future improvements based on these findings. In our representational analysis we explore how different transformer models converge or diverge in their representation of semantic signals as the tokens of the sentences are augmented by successive layers. This analysis sheds light on how these ""black box"" models integrate semantic similarity information in intermediate layers, and points to new research directions in model distillation and sentence embedding extraction for applications in clinical NLP.","2021-05","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","5","9","","","","","","","","","","English","","","","WOS:000656664300021","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;11<br/>Total Times Cited:&nbsp;&nbsp;11<br/>Cited Reference Count:&nbsp;&nbsp;36</p>","","","biomedical NLP; clinical text; natural language processing; representation learning; transformer models","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"F853R5A6","journalArticle","2023","Yang, WL; Fu, BL; Li, SZ; Lao, ZN; Deng, TF; He, W; He, HC; Chen, ZK","Monitoring multi-water quality of internationally important karst wetland through deep learning, multi-sensor and multi-platform remote sensing images: A case study of Guilin, China","ECOLOGICAL INDICATORS","","1470-160X","10.1016/j.ecolind.2023.110755","","Karst wetlands are widely distributed throughout the southwest China, and play an important role in enhancing carbon sequestration and improving water quality in karst areas. The internationally important karst wetland of Huixian is the largest karst wetland in China, but its water quality has continued to deteriorate as a result of human influences in recent years. Remote sensing technology has become an important approach to estimate water quality parameters (WQPs). However, the feasibility of combining multi-sensor remote sensing images with deep learning to estimate different WQPs in karst wetlands has not been demonstrated yet. To resolve this issue, this study constructed multiple retrieval models of WQPs (Chlorophyll-a (Chla), Phycocyanin (PC), Turbidity (Turb), Dissolved Oxygen (DO)) in karst wetlands using deep learning (Transformer and Mixture Density Network (MDN)) and optimized shallow machine learning (Random Forest (RF), XGBoost (XGB) and Gradient Boosting (GB)) based on multi-sensor images from satellite and UAV platforms. The performance of deep learning in the inversion of WQPs demonstrated to compare with shallow machine learning using multi-spectral and hyperspectral images. We further quantitatively evaluated the retrieval performance of UAV and satellite, multispectral and hyperspectral images, and presented predictive mapping of the gradient distribution of WQPs. Finally, this study adopted the SHapley Additive exPlanations (SHAP) to tackle the local and global interpretability of the input features contribution to the output of retrieval models. The results showed that (1) Transformer model presented a good prediction of PC and DO (R-2 = 0.649 -0.844), XGB and GB models achieved the highest accuracy estimation of Chla and Turb (R-2 = 0.75). (2) The estimation results of WQPs based on UAV platform (R-2 = 0.419 -0.695) was higher than that of satellite-based images. The estimation accuracy of multispectral images (R-2 = 0.338 -0.718) was slightly higher than that of Zhuhai-1 Orbita hyperspectral (OHS) images. The average accuracy of Turb estimated by UAV images (R-2 = 0.565 -0.752) was higher than that of satellite-based images. OHS hyperspectral images had the best DO estimation (R-2 = 0.314 -0.649). (3) This study found 32.66% and 23.01% of water area with the Chla and Turb concentrations exceeding 60 mu g/L and 60 NTU, respectively, which revealed that the Huixian karst wetland has suffered serious water pollution. (4) The SHAP analysis reveals that near infra-red and red band are sensitive to predict Chla and DO, red and red-edge bands are sensitive to predict PC and Turb in the karst wetland.","2023-10","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","154","","","","","","","","","","English","","","","WOS:001190755300001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;22<br/>Total Times Cited:&nbsp;&nbsp;22<br/>Cited Reference Count:&nbsp;&nbsp;61</p>","","","ALGORITHMS; CHLOROPHYLL-A; Deep learning and machine learning; DISSOLVED ORGANIC-MATTER; ECOSYSTEM SERVICES; IMPACT; INDICATORS; Karst wetlands; LANDSAT; Model inversion; Multispectral and hyperspectral images; TURBIDITY; UAV and satellite platform; Water quality parameters","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"5TXAD9FS","journalArticle","2022","Singh, P; Haimovich, J; Reeder, C; Khurshid, S; Lau, ES; Cunningham, JW; Philippakis, A; Anderson, CD; Ho, JE; Lubitz, SA; Batra, P","One Clinician Is All You Need-Cardiac Magnetic Resonance Imaging Measurement Extraction: Deep Learning Algorithm Development","JMIR MEDICAL INFORMATICS","","2291-9694","10.2196/38178","","Background: Cardiac magnetic resonance imaging (CMR) is a powerful diagnostic modality that provides detailed quantitative assessment of cardiac anatomy and function. Automated extraction of CMR measurements from clinical reports that are typically stored as unstructured text in electronic health record systems would facilitate their use in research. Existing machine learning approaches either rely on large quantities of expert annotation or require the development of engineered rules that are time-consuming and are specific to the setting in which they were developed. Objective: We hypothesize that the use of pretrained transformer-based language models may enable label-efficient numerical extraction from clinical text without the need for heuristics or large quantities of expert annotations. Here, we fine-tuned pretrained transformer-based language models on a small quantity of CMR annotations to extract 21 CMR measurements. We assessed the effect of clinical pretraining to reduce labeling needs and explored alternative representations of numerical inputs to improve performance. Methods: Our study sample comprised 99,252 patients that received longitudinal cardiology care in a multi-institutional health care system. There were 12,720 available CMR reports from 9280 patients. We adapted PRAnCER (Platform Enabling Rapid Annotation for Clinical Entity Recognition), an annotation tool for clinical text, to collect annotations from a study clinician on 370 reports. We experimented with 5 different representations of numerical quantities and several model weight initializations. We evaluated extraction performance using macroaveraged F-1-scores across the measurements of interest. We applied the best-performing model to extract measurements from the remaining CMR reports in the study sample and evaluated established associations between selected extracted measures with clinical outcomes to demonstrate validity. Results: All combinations of weight initializations and numerical representations obtained excellent performance on the gold-standard test set, suggesting that transformer models fine-tuned on a small set of annotations can effectively extract numerical quantities. Our results further indicate that custom numerical representations did not appear to have a significant impact on extraction performance. The best-performing model achieved a macroaveraged F-1-score of 0.957 across the evaluated CMR measurements (range 0.92 for the lowest-performing measure of left atrial anterior-posterior dimension to 1.0 for the highest-performing measures of left ventricular end systolic volume index and left ventricular end systolic diameter). Application of the best-performing model to the study cohort yielded 136,407 measurements from all available reports in the study sample. We observed expected associations between extracted left ventricular mass index, left ventricular ejection fraction, and right ventricular ejection fraction with clinical outcomes like atrial fibrillation, heart failure, and mortality. Conclusions: This study demonstrated that a domain-agnostic pretrained transformer model is able to effectively extract quantitative clinical measurements from diagnostic reports with a relatively small number of gold-standard annotations. The proposed workflow may serve as a roadmap for other quantitative entity extraction.","2022-09","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","9","10","","","","","","","","","","English","","","","WOS:000971308400008","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;4<br/>Total Times Cited:&nbsp;&nbsp;4<br/>Cited Reference Count:&nbsp;&nbsp;36</p>","","","ATRIAL-FIBRILLATION; cardiac MRI; clinical outcomes; deep learning; HEART-FAILURE; LEFT-VENTRICULAR MASS; machine learning; natural language processing; PREDICTORS; transformers","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IT2EXWEL","journalArticle","2023","Criss, S; Nguyen, TT; Michaels, EK; Gee, GC; Kiang, MV; Nguyen, QC; Norton, S; Titherington, E; Nguyen, L; Yardi, I; Kim, M; Thai, N; Shepherd, A; Kennedy, CJ","Solidarity and strife after the Atlanta spa shootings: A mixed methods study characterizing Twitter discussions by qualitative analysis and machine learning","FRONTIERS IN PUBLIC HEALTH","","2296-2565","10.3389/fpubh.2023.952069","","BackgroundOn March 16, 2021, a white man shot and killed eight victims, six of whom were Asian women at Atlanta-area spa and massage parlors. The aims of the study were to: (1) qualitatively summarize themes of tweets related to race, ethnicity, and racism immediately following the Atlanta spa shootings, and (2) examine temporal trends in expressions hate speech and solidarity before and after the Atlanta spa shootings using a new methodology for hate speech analysis. MethodsA random 1% sample of publicly available tweets was collected from January to April 2021. The analytic sample included 708,933 tweets using race-related keywords. This sample was analyzed for hate speech using a newly developed method for combining faceted item response theory with deep learning to measure a continuum of hate speech, from solidarity race-related speech to use of violent, racist language. A qualitative content analysis was conducted on random samples of 1,000 tweets referencing Asians before the Atlanta spa shootings from January to March 15, 2021 and 2,000 tweets referencing Asians after the shooting from March 17 to 28 to capture the immediate reactions and discussions following the shootings. ResultsQualitative themes that emerged included solidarity (4% before the shootings vs. 17% after), condemnation of the shootings (9% after), racism (10% before vs. 18% after), role of racist language during the pandemic (2 vs. 6%), intersectional vulnerabilities (4 vs. 6%), relationship between Asian and Black struggles against racism (5 vs. 7%), and discussions not related (74 vs. 37%). The quantitative hate speech model showed a decrease in the proportion of tweets referencing Asians that expressed racism (from 1.4% 7 days prior to the event from to 1.0% in the 3 days after). The percent of tweets referencing Asians that expressed solidarity speech increased by 20% (from 22.7 to 27.2% during the same time period) (p < 0.001) and returned to its earlier rate within about 2 weeks. DiscussionOur analysis highlights some complexities of discrimination and the importance of nuanced evaluation of online speech. Findings suggest the importance of tracking hate and solidarity speech. By understanding the conversations emerging from social media, we may learn about possible ways to produce solidarity promoting messages and dampen hate messages.","2023-02-07","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","11","","","","","","","","","","English","","","","WOS:000934250200001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;5<br/>Total Times Cited:&nbsp;&nbsp;9<br/>Cited Reference Count:&nbsp;&nbsp;40</p>","","","anti-Asian racism; ASIAN-AMERICANS; COVID-19; DISCRIMINATION; machine learning; qualitative content analysis; RACISM; solidarity; Twitter; WOMEN","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"JHMX8IYK","journalArticle","2023","Yoshii, K; Kimura, D; Kosugi, A; Shinkawa, K; Takase, T; Kobayashi, M; Yamada, Y; Nemoto, M; Watanabe, R; Ota, M; Higashi, S; Nemoto, K; Arai, T; Nishimura, M","Screening of Mild Cognitive Impairment Through Conversations With Humanoid Robots: Exploratory Pilot Study","JMIR FORMATIVE RESEARCH","","2561-326X","10.2196/42792","","Background: The rising number of patients with dementia has become a serious social problem worldwide. To help detect dementia at an early stage, many studies have been conducted to detect signs of cognitive decline by prosodic and acoustic features. However, many of these methods are not suitable for everyday use as they focus on cognitive function or conversational speech during the examinations. In contrast, conversational humanoid robots are expected to be used in the care of older people to help reduce the work of care and monitoring through interaction.Objective: This study focuses on early detection of mild cognitive impairment (MCI) through conversations between patients and humanoid robots without a specific examination, such as neuropsychological examination.Methods: This was an exploratory study involving patients with MCI and cognitively normal (CN) older people. We collected the conversation data during neuropsychological examination (Mini-Mental State Examination [MMSE]) and everyday conversation between a humanoid robot and 94 participants (n=47, 50%, patients with MCI and n=47, 50%, CN older people). We extracted 17 types of prosodic and acoustic features, such as the duration of response time and jitter, from these conversations. We conducted a statistical significance test for each feature to clarify the speech features that are useful when classifying people into CN people and patients with MCI. Furthermore, we conducted an automatic classification experiment using a support vector machine (SVM) to verify whether it is possible to automatically classify these 2 groups by the features identified in the statistical significance test. Results: We obtained significant differences in 5 (29%) of 17 types of features obtained from the MMSE conversational speech. The duration of response time, the duration of silent periods, and the proportion of silent periods showed a significant difference (P<.001) and met the reference value r=0.1 (small) of the effect size. Additionally, filler periods (P<.01) and the proportion of fillers (P=.02) showed a significant difference; however, these did not meet the reference value of the effect size. In contrast, we obtained significant differences in 16 (94%) of 17 types of features obtained from the everyday conversations with the humanoid robot. The duration of response time, the duration of speech periods, jitter (local, relative average perturbation [rap], 5-point period perturbation quotient [ppq5], difference of difference of periods [ddp]), shimmer (local, amplitude perturbation quotient [apq]3, apq5, apq11, average absolute differences between the amplitudes of consecutive periods [dda]), and F0cov (coefficient of variation of the fundamental frequency) showed a significant difference (P<.001). In addition, the duration of response time, the duration of silent periods, the filler period, and the proportion of fillers showed significant differences (P<.05). However, only jitter (local) met the reference value r=0.1 (small) of the effect size. In the automatic classification experiment for the classification of participants into CN and MCI groups, the results showed 66.0% accuracy in the MMSE conversational speech and 68.1% accuracy in everyday conversations with the humanoid robot.Conclusions: This study shows the possibility of early and simple screening for patients with MCI using prosodic and acoustic features from everyday conversations with a humanoid robot with the same level of accuracy as the MMSE.","2023","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","7","","","","","","","","","","English","","","","WOS:001000224400029","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;5<br/>Total Times Cited:&nbsp;&nbsp;5<br/>Cited Reference Count:&nbsp;&nbsp;41</p>","","","Alzheimer disease; humanoid robot; mild cognitive impairment; MINI-MENTAL-STATE; monitoring; neuropsychiatric symptoms; neuropsychological; neuropsychological assessment; robot; simple screening; SPEECH; symptoms","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"XFF9HFEI","journalArticle","2024","Jin, XY; Hao, Y; Hilliard, J; Zhang, ZH; Thomas, MA; Li, H; Jha, AK; Hugo, GD","A quality assurance framework for routine monitoring of deep learning cardiac substructure computed tomography segmentation models in radiotherapy","MEDICAL PHYSICS","","0094-2405","10.1002/mp.16846","","Background: For autosegmentation models, the data used to train the model (e.g., public datasets and/or vendor-collected data) and the data on which the model is deployed in the clinic are typically not the same, potentially impacting the performance of these models by a process called domain shift. Tools to routinely monitor and predict segmentation performance are needed for quality assurance. Here, we develop an approach to perform such monitoring and performance prediction for cardiac substructure segmentation.Purpose: To develop a quality assurance (QA) framework for routine or continuous monitoring of domain shift and the performance of cardiac substructure autosegmentation algorithms.Methods: A benchmark dataset consisting of computed tomography (CT) images along with manual cardiac substructure delineations of 241 breast cancer radiotherapy patients were collected, including one ""normal"" image domain of clean images and five ""abnormal"" domains containing images with artifact (metal, contrast), pathology, or quality variations due to scanner protocol differences (field of view, noise, reconstruction kernel, and slice thickness). The QA framework consisted of an image domain shift detector which operated on the input CT images and a shape quality detector on the output of an autosegmentation model, and a regression model for predicting autosegmentation model performance. The image domain shift detector was composed of a trained denoising autoencoder (DAE) and two hand-engineered image quality features to detect normal versus abnormal domains in the input CT images. The shape quality detector was a variational autoencoder (VAE) trained to estimate the shape quality of the auto-segmentation results. The output from the image domain shift and shape quality detectors was used to train a regression model to predict the per-patient segmentation accuracy, measured by Dice coefficient similarity (DSC) to physician contours. Different regression techniques were investigated including linear regression, Bagging, Gaussian process regression, random forest, and gradient boost regression. Of the 241 patients, 60 were used to train the autosegmentation models, 120 for training the QA framework, and the remaining 61 for testing the QA framework. A total of 19 autosegmentation models were used to evaluate QA framework performance, including 18 convolutional neural network (CNN)-based and one transformer-based model.Results: When tested on the benchmark dataset, all abnormal domains resulted in a significant DSC decrease relative to the normal domain for CNN models (p < 0.001), but only for some domains for the transformer model. No significant relationship was found between the performance of an autosegmentation model and scanner protocol parameters (p = 0.42) except noise (p = 0.01). CNN-based autosegmentation models demonstrated a decreased DSC ranging from 0.07 to 0.41 with added noise, while the transformer-based model was not significantly affected (ANOVA, p=0.99). For the QA framework, linear regression models with bootstrap aggregation resulted in the highest mean absolute error (MAE) of 0.041 +/- 0.002, in predicted DSC (relative to true DSC between autosegmentation and physician). MAE was lowest when combining both input (image) detectors and output (shape) detectors compared to output detectors alone.Conclusions: A QA framework was able to predict cardiac substructure autosegmentation model performance for clinically anticipated ""abnormal"" domain shifts.","2024-04","2025-02-26 20:45:06","2025-02-26 20:45:06","","2741-2758","","4","51","","","","","","","","","","English","","","","WOS:001110211100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;36</p>","","","auto-segmentation; domain shift; quality assurance","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"KNX9FV8G","journalArticle","2021","Moragrega, I; Bridler, R; Mohr, C; Possenti, M; Rochat, D; Parramon, JS; Stassen, HH","Monitoring the effects of therapeutic interventions in depression through self-assessments","RESEARCH IN PSYCHOTHERAPY-PSYCHOPATHOLOGY PROCESS AND OUTCOME","","2239-8031","10.4081/ripppo.2021.548","","The treatment of major psychiatric disorders is an arduous and thorny path for the patients concerned, characterized by polypharmacy, massive adverse side effects, modest prospects of success, and constantly declining response rates. The more important is the early detection of psychiatric disorders prior to the development of clinically relevant symptoms, so that people can benefit from early interventions. A well-proven approach to monitoring mental health relies on voice analysis. This method has been successfully used with psychiatric patients to 'objectively' document the progress of improvement or the onset of relapse. The studies with psychiatric patients over 2-4 weeks demonstrated that daily voice assessments have a notable therapeutic effect in themselves. Therefore, daily voice assessments appear to be a lowthreshold form of therapeutic means that may be realized through self-assessments. To evaluate performance and reliability of this approach, we have carried out a longitudinal study on 82 university students in 3 different countries with daily assessments over 2 weeks. The sample included 41 males (mean age 24.2 +/- 3.83 years) and 41 females (mean age 21.6 +/- 2.05 years). Unlike other research in the field, this study was not concerned with the classification of individuals in terms of diagnostic categories. The focus lay on the monitoring aspect and the extent to which the effects of therapeutic interventions or of behavioural changes are visible in the results of self-assessment voice analyses. changes analyses. The test persons showed an over-proportionally good adherence to the daily voice analysis scheme. The accumulated data were of generally high quality: sufficiently high signal levels, a very limited number of movement artifacts, and little to no interfering background noise. The method was sufficiently sensitive to detect: i) habituation effects when test persons became used to the daily procedure; and ii) short-term fluctuations that exceeded pre -specified thresholds and reached significance. Results are directly interpretable and provide information about what is going well, what is going less well, and where there is a need for action. The proposed self-assessment approach was found to be well-suited to serve as a health-monitoring tool for subjects with an elevated vulnerability to psychiatric disorders or to stress-induced mental health problems. Daily voice assessments are in fact a low-thresh-old form of therapeutic means that can be realized through self -assessments, that requires only little effort, can be carried out in the test person's own home, and has the potential to strengthen resilience and to induce positive behavioural changes.","2021","2025-02-26 20:45:06","2025-02-26 20:45:06","","250-262","","3","24","","","","","","","","","","English","","","","WOS:000734175900002","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;52</p>","","","biofeedback; CORTISOL RESPONSES; depression; DISORDERS; early detection; MENTAL-HEALTH; ONSET; Psychiatric disorders; psychosomatic disturbances; RISK; self-monitoring; SPEECH ANALYSIS; STRESS; stress-related health problems; university students; UNIVERSITY-STUDENTS; voice analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"IAA3ST7V","journalArticle","2024","D'Angelo, CM; Rajarathinam, RJ","Speech analysis of teaching assistant interventions in small group collaborative problem solving with undergraduate engineering students","BRITISH JOURNAL OF EDUCATIONAL TECHNOLOGY","","0007-1013","10.1111/bjet.13449","","This descriptive study focuses on using voice activity detection (VAD) algorithms to extract student speech data in order to better understand the collaboration of small group work and the impact of teaching assistant (TA) interventions in undergraduate engineering discussion sections. Audio data were recorded from individual students wearing head-mounted noise-cancelling microphones. Video data of each student group were manually coded for collaborative behaviours (eg, group task relatedness, group verbal interaction and group talk content) of students and TA-student interactions. The analysis includes information about the turn taking, overall speech duration patterns and amounts of overlapping speech observed both when TAs were intervening with groups and when they were not. We found that TAs very rarely provided explicit support regarding collaboration. Key speech metrics, such as amount of turn overlap and maximum turn duration, revealed important information about the nature of student small group discussions and TA interventions. TA interactions during small group collaboration are complex and require nuanced treatments when considering the design of supportive tools.Practitioner notes What is already known about this topic Student turn taking can provide information about the nature of student discussions and collaboration. Real classroom audio data of small groups typically have lots of background noise and present challenges for audio analysis. TAs have little training in how to productively intervene with students about collaborative skills. What this paper adds TA interaction with groups primarily focused on task progress and understanding of concepts with negligible explicit support on building collaborative skills. TAs intervened with the groups often which gave the students little time for uptake of their suggestions or deeper discussion. Student turn overlap was higher without the presence of TAs. Maximum turn duration can be an important real-time turn metric to identify the least verbally active student participant in a group. Implications for practice and/or policy TA training should include information about how to monitor groups for collaborative behaviours and when and how they should intervene to provide feedback and support. TA feedback systems should keep track of previous interventions by TAs (especially in contexts where there are multiple TAs facilitating) and the duration since previous intervention to ensure that TAs do not intervene with a group too frequently with little time for student uptake. Maximum turn duration could be used as a real-time metric to identify the least verbally active student in a group so that support could be provided to them by the TAs.","2024-06","2025-02-26 20:45:06","2025-02-26 20:45:06","","1583-1601","","4","55","","","","","","","","","","English","","","","WOS:001175878100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;64</p>","","","ANALYTICS; audio analytics; CLASSROOM; collaborative problem solving; computer-supported collaborative learning; DISCOURSE; FRAMEWORK; higher education; multimodal learning analytics; SCIENCE; teacher orchestration; THINKING; WORK","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"SEC3684T","journalArticle","2024","Noffs, G; Cobler-Lichter, M; Perera, T; Kolbe, SC; Butzkueven, H; Boonstra, FMC; van der Walt, A; Vogel, AP","Plug-and-play microphones for recording speech and voice with smart devices","FOLIA PHONIATRICA ET LOGOPAEDICA","","1021-7762","10.1159/000535152","","INTRODUCTION Smart devices are widely available and capable of quickly recording and uploading speech segments for health-related analysis. The switch from laboratory recordings with professional-grade microphone set ups to remote, smart device-based recordings offers immense potential for the scalability of voice assessment. Yet, a growing body of literature points to a wide heterogeneity among acoustic metrics for their robustness to variation in recording devices. The addition of consumer-grade plug-and-play microphones has been proposed as a possible solution. Our aim was to assess if the addition of consumer-grade plug-and-play microphones increase the acoustic measurement agreement between ultra-portable devices and a reference microphone. METHODS Speech was simultaneously recorded by a reference high-quality microphone commonly used in research, and by two configurations with plug-and-play microphones. Twelve speech-acoustic features were calculated using recordings from each microphone to determine the agreement intervals in measurements between microphones. Agreement intervals were then compared to expected deviations in speech in various neurological conditions. Each microphone's response to speech and to silence were characterized through acoustic analysis to explore possible reasons for differences in acoustic measurements between microphones. The statistical differentiation of two groups, neurotypical and people with Multiple Sclerosis, using metrics from each tested microphone was compared to that of the reference microphone. RESULTS The two consumer-grade plug-and-play microphones favoured high frequencies (mean centre of gravity difference >= +175.3Hz) and recorded more noise (mean difference in signal-to-noise <= -4.2dB) when compared to the reference microphone. Between consumer-grade microphones, differences in relative noise were closely related to distance between the microphone and the speaker's mouth. Agreement intervals between the reference and consumer-grade microphones remained under disease-expected deviations only for fundamental frequency (f0, agreement interval <= 0.06Hz), f0 instability (f0 CoV, agreement interval <= 0.05%) and for tracking of second formant movement (agreement interval <= 1.4Hz/millisecond). Agreement between microphones was poor for other metrics, particularly for fine timing metrics (mean pause length and pause length variability for various tasks). The statistical difference between the two groups of speakers was smaller with the plug-and-play than with the reference microphone. CONCLUSION Measurement of f0 and F2 slope were robust to variation in recording equipment while other acoustic metrics were not. Thus, the tested plug-and-play microphones should not be used interchangeably with professional-grade microphones for speech analysis. Plug-and-play microphones may assist in equipment standardization within speech studies, including remote or self-recording, possibly with small loss in accuracy and statistical power as observed in this study.","2024-08","2025-02-26 20:45:06","2025-02-26 20:45:06","","372-385","","4","76","","","","","","","","","","English","","","","WOS:001108588400001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;52</p>","","","Acoustic analysis; ACOUSTIC ANALYSIS; APATHY; ARTICULATION RATE; BIOMARKERS; DISORDERS; IMPAIRMENT; microphone; MULTIPLE-SCLEROSIS; PLACEMENT; QUALITY; remote assessment; speech; voice","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"B3LNJUFM","journalArticle","2021","Rahman, W; Lee, S; Islam, MS; Antony, VN; Ratnu, H; Ali, MR; Al Mamun, A; Wagner, E; Jensen-Roberts, S; Waddell, E; Myers, T; Pawlik, M; Soto, J; Coffey, M; Sarkar, A; Schneider, R; Tarolli, C; Lizarraga, K; Adams, J; Little, MA; Dorsey, ER; Hoque, E","Detecting Parkinson Disease Using a Web-Based Speech Task: Observational Study","JOURNAL OF MEDICAL INTERNET RESEARCH","","1438-8871","10.2196/26305","","Background: Access to neurological care for Parkinson disease (PD) is a rare privilege for millions of people worldwide, especially in resource-limited countries. In 2013, there were just 1200 neurologists in India for a population of 1.3 billion people; in Africa, the average population per neurologist exceeds 3.3 million people. In contrast, 60,000 people receive a diagnosis of PD every year in the United States alone, and similar patterns of rising PD cases-fueled mostly by environmental pollution and an aging population-can be seen worldwide. The current projection of more than 12 million patients with PD worldwide by 2040 is only part of the picture given that more than 20% of patients with PD remain undiagnosed. Timely diagnosis and frequent assessment are key to ensure timely and appropriate medical intervention, thus improving the quality of life of patients with PD. Objective: In this paper, we propose a web-based framework that can help anyone anywhere around the world record a short speech task and analyze the recorded data to screen for PD. Methods: We collected data from 726 unique participants (PD: 262/726, 36.1% were women; non-PD: 464/726, 63.9% were women; average age 61 years) from all over the United States and beyond. A small portion of the data (approximately 54/726, 7.4%) was collected in a laboratory setting to compare the performance of the models trained with noisy home environment data against high-quality laboratory-environment data. The participants were instructed to utter a popular pangram containing all the letters in the English alphabet, ""the quick brown fox jumps over the lazy dog."" We extracted both standard acoustic features (mel-frequency cepstral coefficients and jitter and shimmer variants) and deep learning-based embedding features from the speech data. Using these features, we trained several machine learning algorithms. We also applied model interpretation techniques such as Shapley additive explanations to ascertain the importance of each feature in determining the model's output. Results: We achieved an area under the curve of 0.753 for determining the presence of self-reported PD by modeling the standard acoustic features through the XGBoost-a gradient-boosted decision tree model. Further analysis revealed that the widely used mel-frequency cepstral coefficient features and a subset of previously validated dysphonia features designed for detecting PD from a verbal phonation task (pronouncing ""ahh"") influence the model's decision the most. Conclusions: Our model performed equally well on data collected in a controlled laboratory environment and in the wild across different gender and age groups. Using this tool, we can collect data from almost anyone anywhere with an audio-enabled device and help the participants screen for PD remotely, contributing to equity and access in neurological care.","2021-10-19","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","10","23","","","","","","","","","","English","","","","WOS:000708587400003","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;17<br/>Total Times Cited:&nbsp;&nbsp;19<br/>Cited Reference Count:&nbsp;&nbsp;54</p>","","","ALGORITHMS; GENDER; improving access and equity in health care; LARGE-SAMPLE; mobile phone; NEUROLOGY; Parkinson's disease; speech analysis","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"GDL7I2CK","journalArticle","2025","van Eeden, S; Mckean, C; Stringer, H","Rethinking speech sound disorder (SSD) in non-syndromic cleft lip and palate: The importance of recognizing phonological and language difficulties","INTERNATIONAL JOURNAL OF LANGUAGE & COMMUNICATION DISORDERS","","1368-2822","10.1111/1460-6984.13151","","BackgroundChildren born with cleft palate +/- lip (CP +/- L) are at risk of speech sound disorder (SSD). Up to 40% continue to have SSD at age 5-6 years. These difficulties are typically described as articulatory in nature and often include cleft speech characteristics (CSC) hypothesized to result from structural differences. In non-CP +/- L SSD comorbidity with language difficulties is often reported. There is growing evidence of concomitant language difficulties in children with CP +/- L and of a higher prevalence of developmental speech errors in children compared with non-CP +/- L peers. The impact of underlying phonological and language skills on speech production in children with CP +/- L is poorly understood.AimsTo investigate language outcomes in children with CP +/- L and the relationship to speech production, by answering the following research questions: (1) Does the profile of language skills in children with CP +/- L differ from normative samples? (2) Do children with CP +/- L and SSD have poorer language skills than those with typically developing speech? (3) Is there an association between language skills and speech profile in children with CP +/- L at age 5-8 years?Methods & ProceduresIn this prospective cross-sectional, observational study, 95 participants were recruited from regional cleft lip and palate services in the UK. They were aged 5;0-7;11 with non-syndromic CP +/- L. Those with a syndromic diagnosis, global learning disability, sensorineural hearing loss and first language other than English were excluded. Assessments of speech (Diagnostic Evaluation of Articulation and Phonology-DEAP) and language (Clinical Evaluation of Language Fundamentals-5th UK edition-CELF) were completed. Language outcomes were analysed and compared with normative samples and according to speech error analysis.Outcomes & ResultsAverage language scores were within the expected range. For those presenting with SSD, language scores were significantly lower than those with typically developing speech. Analysis of speech errors showed four distinct speech profiles: typical speech, CSC only, developmental speech characteristics (DSC), and combined CSC + DSC. Language scores were lower for participants with DSC (+/- CSC). A significant association was found between the presence of CSC + DSC and expressive language outcomes (odds ratio (OR) = 10.82; 95% confidence interval (CI) = 2.42, 48.32, p = 0.002).Conclusions & ImplicationsAn association between language skills and speech production was observed. The distribution of speech errors in children with CP +/- L varied with a high level of DSC as well as CSC. Those with CSC + DSC had significantly lower language scores than those with typically developing speech or CSC only. Speech and language therapists working with this caseload should be alerted to potential ongoing phonological and language difficulties in children presenting with this profile.","2025-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","1","60","","","","","","","","","","English","","","","WOS:001397477600001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;72</p>","","","5-YEAR-OLDS; AGE; CHILDREN; cleft palate; COHORT; DEVELOPMENTAL DISORDERS; language; LEXICAL SELECTIVITY; phonology; PREVALENCE; RISK; speech sound disorder; SPOKEN; TODDLERS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"A5C2QMD9","journalArticle","2025","Wu, Y; Wu, SY; Yu, YH; Hu, XJ; Zhao, T; Jiang, Y; Ke, BL","Blinking characteristics analyzed by a deep learning model and the relationship with tear film stability in children with long-term use of orthokeratology","FRONTIERS IN CELL AND DEVELOPMENTAL BIOLOGY","","2296-634X","10.3389/fcell.2024.1517240","","Purpose: Using deep learning model to observe the blinking characteristics and evaluate the changes and their correlation with tear film characteristics in children with long-term use of orthokeratology (ortho-K). Methods: 31 children (58 eyes) who had used ortho-K for more than 1 year and 31 age and gender-matched controls were selected for follow-up in our ophthalmology clinic from 2021/09 to 2023/10 in this retrospective case-control study. Both groups underwent comprehensive ophthalmological examinations, including Ocular Surface Disease Index (OSDI) scoring, Keratograph 5M, and LipiView. A deep learning system based on U-Net and Swim-Transformer was proposed for the observation of blinking characteristics. The frequency of incomplete blinks (IB), complete blinks (CB) and incomplete blinking rate (IBR) within 20 s, as well as the duration of the closing, closed, and opening phases in the blink wave were calculated by our deep learning system. Relative IPH% was proposed and defined as the ratio of the mean of IPH% within 20 s to the maximum value of IPH% to indicate the extent of incomplete blinking. Furthermore, the accuracy, precision, sensitivity, specificity, F1 score of the overall U-Net-Swin-Transformer model, and its consistency with built-in algorithm were evaluated as well. Independent t-test and Mann-Whitney test was used to analyze the blinking patterns and tear film characteristics between the long-term ortho-K wearer group and the control group. Spearman's rank correlation was used to analyze the relationship between blinking patterns and tear film stability. Results: Our deep learning system demonstrated high performance (accuracy = 98.13%, precision = 96.46%, sensitivity = 98.10%, specificity = 98.10%, F1 score = 0.9727) in the observation of blinking patterns. The OSDI scores, conjunctival redness, lipid layer thickness (LLT), and tear meniscus height did not change significantly between two groups. Notably, the ortho-K group exhibited shorter first (11.75 +/- 7.42 s vs. 14.87 +/- 7.93 s, p = 0.030) and average non-invasive tear break-up times (NIBUT) (13.67 +/- 7.0 s vs. 16.60 +/- 7.24 s, p = 0.029) compared to the control group. They demonstrated a higher IB (4.26 +/- 2.98 vs. 2.36 +/- 2.55, p < 0.001), IBR (0.81 +/- 0.28 vs. 0.46 +/- 0.39, p < 0.001), relative IPH% (0.3229 +/- 0.1539 vs. 0.2233 +/- 0.1960, p = 0.004) and prolonged eye-closing phase (0.18 +/- 0.08 s vs. 0.15 +/- 0.07 s, p = 0.032) and opening phase (0.35 +/- 0.12 s vs. 0.28 +/- 0.14 s, p = 0.015) compared to controls. In addition, Spearman's correlation analysis revealed a negative correlation between incomplete blinks and NIBUT (for first-NIBUT, r = -0.292, p = 0.004; for avg-NIBUT, r = -0.3512, p < 0.001) in children with long-term use of ortho-K. Conclusion: The deep learning system based on U-net and Swim-Transformer achieved optimal performance in the observation of blinking characteristics. Children with long-term use of ortho-K presented an increase in the frequency and rate of incomplete blinks and prolonged eye closing phase and opening phase. The increased frequency of incomplete blinks was associated with decreased tear film stability, indicating the importance of monitoring children's blinking patterns as well as tear film status in clinical follow-up.","2025-01-28","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","12","","","","","","","","","","English","","","","WOS:001417943700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;46</p>","","","blinking pattern; children; deep learning system; DRY-EYE; INCOMPLETE BLINKING; OCULAR SURFACE; orthokeratology; OVERNIGHT ORTHOKERATOLOGY; PATTERNS; tear film","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RW5AKD54","journalArticle","2023","Diaz-Ramos, RE; Noriega, I; Trejo, LA; Stroulia, E; Cao, B","Using Wearable Devices and Speech Data for Personalized Machine Learning in Early Detection of Mental Disorders: Protocol for a Participatory Research Study","JMIR RESEARCH PROTOCOLS","","1929-0748","10.2196/48210","","Background: Early identification of mental disorder symptoms is crucial for timely treatment and reduction of recurring symptoms and disabilities. A tool to help individuals recognize warning signs is important. We posit that such a tool would have to rely on longitudinal analysis of patterns and trends in the individual's daily activities and mood, which can now be captured through data from wearable activity trackers, speech recordings from mobile devices, and the individual's own description of their mental state. In this paper, we describe such a tool developed by our team to detect early signs of depression, anxiety, and stress.Objective: This study aims to examine three questions about the effectiveness of machine learning models constructed based on multimodal data from wearables, speech, and self-reports: (1) How does speech about issues of personal context differ from speech while reading a neutral text, what type of speech data are more helpful in detecting mental health indicators, and how is the quality of the machine learning models influenced by multilanguage data? (2) Does accuracy improve with longitudinal data collection and how, and what are the most important features? and (3) How do personalized machine learning models compare against population-level models? Methods: We collect longitudinal data to aid machine learning in accurately identifying patterns of mental disorder symptoms. We developed an app that collects voice, physiological, and activity data. Physiological and activity data are provided by a variety of off-the-shelf fitness trackers, that record steps, active minutes, duration of sleeping stages (rapid eye movement, deep, and light sleep), calories consumed, distance walked, heart rate, and speed. We also collect voice recordings of users reading specific texts and answering open-ended questions chosen randomly from a set of questions without repetition. Finally, the app collects users' answers to the Depression, Anxiety, and Stress Scale. The collected data from wearable devices and voice recordings will be used to train machine learning models to predict the levels of anxiety, stress, and depression in participants. Results: The study is ongoing, and data collection will be completed by November 2023. We expect to recruit at least 50 participants attending 2 major universities (in Canada and Mexico) fluent in English or Spanish. The study will include participants aged between 18 and 35 years, with no communication disorders, acute neurological diseases, or history of brain damage. Data collection complied with ethical and privacy requirements.Conclusions: The study aims to advance personalized machine learning for mental health; generate a data set to predict Depression, Anxiety, and Stress Scale results; and deploy a framework for early detection of depression, anxiety, and stress. Our long-term goal is to develop a noninvasive and objective method for collecting mental health data and promptly detecting mental disorder symptoms.","2023","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","","12","","","","","","","","","","English","","","","WOS:001114833900001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;1<br/>Total Times Cited:&nbsp;&nbsp;1<br/>Cited Reference Count:&nbsp;&nbsp;36</p>","","","anxiety; BEHAVIOR; DASS21; depression; DEPRESSION; Depression, Anxiety, and Stress Scale; machine learning; mental health; mood disorders; PREDICTION; SENSORS; smartwatches; speech analysis; stress; voice; wearables","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"8ASJTJXK","journalArticle","2022","Iyer, R; Nedeljkovic, M; Meyer, D","Using Vocal Characteristics To Classify Psychological Distress in Adult Helpline Callers: Retrospective Observational Study","JMIR FORMATIVE RESEARCH","","2561-326X","10.2196/42249","","Background: Elevated psychological distress has demonstrated impacts on individuals' health. Reliable and efficient ways to detect distress are key to early intervention. Artificial intelligence has the potential to detect states of emotional distress in an accurate, efficient, and timely manner. Objective: The aim of this study was to automatically classify short segments of speech obtained from callers to national suicide prevention helpline services according to high versus low psychological distress and using a range of vocal characteristics in combination with machine learning approaches. Methods: A total of 120 telephone call recordings were initially converted to 16-bit pulse code modulation format. Short variable-length segments of each call were rated on psychological distress using the distress thermometer by the responding counselor and a second team of psychologists (n=6) blinded to the initial ratings. Following this, 24 vocal characteristics were initially extracted from 40-ms speech frames nested within segments within calls. After highly correlated variables were eliminated, 19 remained. Of 19 vocal characteristics, 7 were identified and validated as predictors of psychological distress using a penalized generalized additive mixed effects regression model, accounting for nonlinearity, autocorrelation, and moderation by sex. Speech frames were then grouped using k-means clustering based on the selected vocal characteristics. Finally, component-wise gradient boosting incorporating these clusters was used to classify each speech frame according to high versus low psychological distress. Classification accuracy was confirmed via leave-one-caller-out cross-validation, ensuring that speech segments from individual callers were not used in both the training and test data. Results: The sample comprised 87 female and 33 male callers. From an initial pool of 19 characteristics, 7 vocal characteristics were identified. After grouping speech frames into 2 separate clusters (correlation with sex of caller, Cramer's V =0.02), the component-wise gradient boosting algorithm successfully classified psychological distress to a high level of accuracy, with an area under the receiver operating characteristic curve of 97.39% (95% CI 96.20-98.45) and an area under the precision-recall curve of 97.52 (95% CI 95.71-99.12). Thus, 39,282 of 41,883 (93.39%) speech frames nested within 728 of 754 segments (96.6%) were classified as exhibiting low psychological distress, and 71455 of 75503 (94.64%) speech frames nested within 382 of 423 (90.3%) segments were classified as exhibiting high psychological distress. As the probability of high psychological distress increases, male callers spoke louder, with greater vowel articulation but with greater roughness (subharmonic depth). In contrast, female callers exhibited decreased vocal clarity (entropy), greater proportion of signal noise, higher frequencies, increased breathiness (spectral slope), and increased roughness of speech with increasing psychological distress. Individual caller random effects contributed 68% to risk reduction in the classification algorithm, followed by cluster configuration (23.4%), spectral slope (4.4%), and the 50th percentile frequency (4.2%). Conclusions: The high level of accuracy achieved suggests possibilities for real-time detection of psychological distress in helpline settings and has potential uses in pre-emptive triage and evaluations of counseling outcomes.","2022-12","2025-02-26 20:45:06","2025-02-26 20:45:06","","","","12","6","","","","","","","","","","English","","","","WOS:000968623100040","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;5<br/>Total Times Cited:&nbsp;&nbsp;5<br/>Cited Reference Count:&nbsp;&nbsp;21</p>","","","artificial intelligence; biomarker; digital health intervention; distress; emotional distress; machine learning; mental distress; mental health; mental health intervention; psychological stress; psychological well being; SPEECH; speech analysis; voice; voice biomarker","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"RE8QG22S","journalArticle","2023","Barberis, M; Vandermosten, M","The role of education, concept knowledge, work setting and clinical experience in communication partner training: A survey of Flemish speech and language therapists","INTERNATIONAL JOURNAL OF LANGUAGE & COMMUNICATION DISORDERS","","1368-2822","10.1111/1460-6984.12928","","BackgroundAphasia can affect the communication between the person with aphasia (PWA) and the communication partner (CP). It is therefore necessary to support both the PWA and their CPs. Communication partner training (CPT) focuses on training communication between dyads of whom one person has aphasia. Although there is increasing evidence supporting CPT as an effective intervention to improve communication and reduce the psychosocial consequences of stroke, implementation in clinical practice remains limited. AimTo understand the mechanisms behind the practice-evidence gap currently hindering CPT implementation, this study investigated the role of (1) education, (2) concept knowledge, (3) work setting and (4) clinical experience in CPT. Methods & ProceduresFlemish speech and language therapists (SLTs) clinically involved in aphasia rehabilitation were surveyed online regarding CPT. Statistical analyses include descriptive statistics to report survey results and non-parametric group comparisons to investigate the role of the four variables on CPT. Outcomes & ResultsIn this study 72 SLTs were included, of whom 73.61% indicated they deliver CPT but of whom only 43.10% indicated CP presence during therapy. The most frequently identified barriers to CPT delivery were lack of time and CPT-specific knowledge. Other barriers were lack of resources, work setting dependent factors, PWA or CP dependent factors, individual therapy to the PWA being of higher priority, existing CPT methods and interventions being perceived as unclear and feeling uncertain about CPT delivery. Concerning the role of the four variables on CPT delivery, neither education nor concept knowledge had a significant effect on CPT delivery. Work setting and clinical experience did, however, influence CPT delivery. More specifically, CPT delivery and CP presence were higher in the private practice (chronic phase) compared to the other three settings and experienced SLTs deliver CPT more often compared with less experienced SLTs. Conclusions & ImplicationsTo reduce the practice-evidence gap, we suggest prioritising the two most frequently identified barriers, that is, lack of time and CPT-specific knowledge. To overcome the time barrier in CPT, we propose implementing automated natural speech analysis to reduce the workload. To enhance CPT-specific knowledge, speech and language therapy curricula should provide more in-depth theory and hands-on practice for CPT. In addition, increased awareness about CPT-specific methods is needed to further support clinical practice. WHAT THIS PAPER ADDSWhat is already known on the subjectCommunication partner training (CPT) is an effective intervention to improve communication and reduce the psychosocial consequences of stroke. Despite this evidence base, a current practice-evidence gap exists. What this study addsThis is the first study to characterise CPT delivery in a Flemish cohort of speech and language therapists (SLTs). In addition, on a more international perspective, few studies have investigated the role of education, concept knowledge, work setting and clinical experience in CPT. We found that neither education nor concept knowledge has a significant effect on CPT delivery. CPT delivery and communication partner presence are significantly higher in the private practice compared to the hospital, rehabilitation centre or nursing home settings. Experienced SLTs deliver CPT more often compared with less-experienced SLTs. The two most prominent reported barriers include lack of time and CPT-specific knowledge. What are the clinical implications of this work?This study suggests reducing the practice-evidence gap by alleviating the main barriers identified, that is, lack of time and CPT-specific knowledge. Time-barriers can be addressed by implementing automated natural speech analyses. We additionally advocate for more in-depth theory and hands-on practice for CPT in speech and language therapy curricula.","2023-11","2025-02-26 20:45:06","2025-02-26 20:45:06","","2117-2130","","6","58","","","","","","","","","","English","","","","WOS:001019806700001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;58</p>","","","aphasia; APHASIA; communication partner training; CONVERSATION THERAPY; PATHOLOGY; PEOPLE; rehabilitation; SERVICES; SIGNIFICANT OTHERS","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
"ZC86CNTI","journalArticle","2025","Yuan, SJ; Wang, GS; Mu, B; Zhou, FF","TianXing: A Linear Complexity Transformer Model with Explicit Attention Decay for Global Weather Forecasting","ADVANCES IN ATMOSPHERIC SCIENCES","","0256-1530","10.1007/s00376-024-3313-9","","In this paper, we introduce TianXing, a transformer-based data-driven model designed with physical augmentation for skillful and efficient global weather forecasting. Previous data-driven transformer models such as Pangu-Weather, FengWu, and FuXi have emerged as promising alternatives for numerical weather prediction in weather forecasting. However, these models have been characterized by their substantial computational resource consumption during training and limited incorporation of explicit physical guidance in their modeling frameworks. In contrast, TianXing applies a linear complexity mechanism that ensures proportional scalability with input data size while significantly diminishing GPU resource demands, with only a marginal compromise in accuracy. Furthermore, TianXing proposes an explicit attention decay mechanism in the linear attention derived from physical insights to enhance its forecasting skill. The mechanism can re-weight attention based on Earth's spherical distances and learned sparse multivariate coupling relationships, promptingTianXing to prioritize dynamically relevant neighboring features. Finally, to enhance its performance in mediumrange forecasting, TianXing employs a stacked autoregressive forecast algorithm. Validation of the model's architecture is conducted using ERA5 reanalysis data at a 5.625 degrees latitude-longitude resolution, while a high-resolution dataset at 0.25 degrees is utilized for training the actual forecasting model. Notably, the TianXing exhibits excellent performance, particularly in the Z500 (geopotential height) and T850 (temperature) fields, surpassing previous data-driven models and operational full-resolution models such as NCEP GFS and ECMWF IFS, as evidenced by latitude-weighted RMSE and ACC metrics. Moreover, the TianXing has demonstrated remarkable capabilities in predicting extreme weather events, such as typhoons. (sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)""(sic)(sic)""(TianXing)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic)(sic)(sic)Transformer(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)Transformer(sic)(sic)(sic), (sic)(sic)(sic),(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)GPU(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)5.625 degrees(sic)ERA5(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)0.25 degrees(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic). (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)RMSE(sic)ACC(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)Z500((sic)(sic)(sic)(sic))(sic)T850((sic)(sic))(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic), (sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)NCEP GFS(sic)ECMWF IFS. (sic)(sic),(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic)(sic).","2025-01","2025-02-26 20:45:06","2025-02-26 20:45:06","","9-25","","1","42","","","","","","","","","","English","","","","WOS:001374429100001","","","","<p>Times Cited in Web of Science Core Collection:&nbsp;&nbsp;0<br/>Total Times Cited:&nbsp;&nbsp;0<br/>Cited Reference Count:&nbsp;&nbsp;27</p>","","","(sic)(sic)(sic)(sic); (sic)(sic)(sic)(sic)(sic); deep learning; linear attention; physics augmentation; weather forecast","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a63f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard library \n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import math\n",
    "import zlib\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# third-party libraries\n",
    "from typing import List \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.tokens import Doc, Token\n",
    "import nltk\n",
    "from tqdm.notebook import tqdm\n",
    "from natsort import natsorted\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# local module\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "from project_config import from_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd477f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp resources\n",
    "# nltk corpora\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "# spacy model \n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "STOP_WORDS = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e28480e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "PITT_DEMENTIA_PATH = from_root(\"Pitt\", \"Dementia\", \"cookie\")\n",
    "PITT_CONTROL_PATH = from_root(\"Pitt\", \"Control\", \"cookie\")\n",
    "ADRESS_TRAIN_CD_PATH = from_root(\"ADReSS-IS2020-data\", \"train\", \"transcription\", \"cd\")\n",
    "ADRESS_TRAIN_CC_PATH = from_root(\"ADReSS-IS2020-data\", \"train\", \"transcription\", \"cc\")\n",
    "ADRESS_TEST_PATH = from_root(\"ADReSS-IS2020-data\", \"test\", \"transcription\")\n",
    "\n",
    "OUT_DATA_DIR = from_root(\"data\")\n",
    "\n",
    "SAVE_PATHS = {\n",
    "    \"adress_train\": from_root(\"data\", \"adress_train.tsv\"),\n",
    "    \"adress_test\": from_root(\"data\", \"adress_test.tsv\"),\n",
    "    \"pitt_control\": from_root(\"data\", \"pitt_control.tsv\"),\n",
    "    \"pitt_ad\": from_root(\"data\", \"pitt_ad.tsv\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3142d1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0865a7ffb14359a6bf93865703fdff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning cookie:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cf7d203872b42d4a053dee8d475750a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scanning cookie:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 30 educated guess age entries (AD)\n",
      "Found 60 educated guess age entries (Control)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# NOTE:\n",
    "# The following code is used to collect the age of the patients from the \n",
    "# transcripts. The age is collected from the @ID line in the transcript files \n",
    "# where the age is in the format of \"XX;00.\"\n",
    "# This step is inteded for reporting and analysis purposes only, thus you can \n",
    "# skip this step. \n",
    "# =============================================================================\n",
    "def collect_exact_educated_guesses(base_dir: str) -> pd.DataFrame:\n",
    "    results = []\n",
    "    cha_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        cha_files.extend([\n",
    "            os.path.join(root, file)\n",
    "            for file in files if file.endswith(\".cha\")\n",
    "        ])\n",
    "\n",
    "    for file_path in tqdm(cha_files, desc=f\"Scanning {os.path.basename(base_dir)}\"):\n",
    "        file_id = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    if line.startswith(\"@ID\") and \"|PAR|\" in line:\n",
    "                        parts = line.strip().split(\"|\")\n",
    "                        if len(parts) > 4:\n",
    "                            raw_age = parts[3].strip()\n",
    "                            if re.fullmatch(r\"\\d{2};00\\.\", raw_age):\n",
    "                                results.append({\n",
    "                                    \"ID\": file_id,\n",
    "                                    \"raw_age\": raw_age\n",
    "                                })\n",
    "                        break  # only need to inspect one @ID line\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def save_sorted_educated_guesses(df: pd.DataFrame, out_path: str, label: str) -> None:\n",
    "    df = df.set_index(\"ID\")\n",
    "    df = df.loc[natsorted(df.index)].reset_index()\n",
    "    df.to_csv(out_path, sep=\"\\t\", index=False)\n",
    "    print(f\"Found {len(df)} educated guess age entries ({label})\")\n",
    "\n",
    "df_ad = collect_exact_educated_guesses(PITT_DEMENTIA_PATH)\n",
    "df_control = collect_exact_educated_guesses(PITT_CONTROL_PATH)\n",
    "\n",
    "save_sorted_educated_guesses(\n",
    "    df_ad,\n",
    "    os.path.join(OUT_DATA_DIR, \"pitt_educated_guess_ages_ad.tsv\"),\n",
    "    label=\"AD\"\n",
    ")\n",
    "\n",
    "save_sorted_educated_guesses(\n",
    "    df_control,\n",
    "    os.path.join(OUT_DATA_DIR, \"pitt_educated_guess_ages_control.tsv\"),\n",
    "    label=\"Control\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae01f8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fc89c234cfd4128a454c497cc13a63f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ADReSS-train:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e1541e980e84dcf9495e731491de487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ADReSS-train:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98244df09c174047baa8ec5e41e75e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ADReSS-train:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feacb6c060a9482ca99903f3eba3a0a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ADReSS-train:   0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a0934b14fbd4cad97ad36534c95b82f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ADReSS-test:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc1ba634d8b42e5ac030f6f04c00957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing ADReSS-test:   0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13c71ab7ba914f3dad5c1a9f82c91ed2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Pitt-Control:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ad96628f8e4f9d89afebdd6e75e8b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Pitt-Dementia:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe0abf96f58944238b0cbffac2e99867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Pitt-Control:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6a452d8e0640be966898f42b5766ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Pitt-Dementia:   0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Title: ADReSS Challenge Analysis Notebook\n",
    "# Author: Thomas Searle\n",
    "# Date: 2020\n",
    "# Code version: N/A (not versioned)\n",
    "# Type: Source code (Jupyter Notebook)\n",
    "# Availability: https://github.com/tomolopolis/ADReSS_Challenge/blob/master/Analysis.ipynb\n",
    "# Note: The following code was inspired by the above source.\n",
    "# =============================================================================\n",
    "\n",
    "def clean_cha_line(text: str, roberta: bool = False) -> str:\n",
    "    # remove timestamps (e.g., \u001500000432_00000550\u0015)\n",
    "    text = re.sub(r'\u0015.*?\u0015', '', text)\n",
    "    # remove annotations like [//], [*], etc.\n",
    "    text = re.sub(r'\\[.*?\\]', '', text)\n",
    "    # remove non-verbal info in parentheses\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)\n",
    "    if roberta:\n",
    "        # remove all non-alphanumeric characters except whitespace\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    else:\n",
    "        # remove all non-alphanumeric characters but keep punctuation like ., !, ?\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s\\.\\!\\?]', '', text)\n",
    "    # normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "            \n",
    "    return text\n",
    "\n",
    "def remove_duplicate_words(text: str) -> str:\n",
    "    # used code from: https://stackoverflow.com/questions/17238587/python-regular-expression-to-remove-repeated-words\n",
    "    # remove consecutive duplicate words (e.g., \"the the\", \"from from\")\n",
    "    cleaned_text = re.sub(r'\\b(\\w+)(?:\\s+\\1\\b)+', r'\\1', text, flags=re.IGNORECASE)\n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def extract_transcript_only(\n",
    "    file_path: str, \n",
    "    label: int | None = None, \n",
    "    dataset: str | None = None,\n",
    "    remove_duplicates: bool = False\n",
    ") -> dict:\n",
    "    file_name = os.path.basename(file_path)\n",
    "    file_id = file_name.split(\".cha\")[0]\n",
    "\n",
    "    record = {\"ID\": file_id, \"dataset\": dataset, \"label\": label}\n",
    "    transcript = []\n",
    "    disfluency_count = 0\n",
    "    pause_count = 0\n",
    "    utterance_count = 0\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        i = 0\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "\n",
    "            # extract metadata\n",
    "            if line.startswith(\"@ID\"):\n",
    "                parts = line.split(\"|\")\n",
    "                if len(parts) > 4 and parts[2].strip() == \"PAR\":\n",
    "                    if parts[4].strip():\n",
    "                        record[\"gender\"] = parts[4].strip()[0]\n",
    "                    else:\n",
    "                        record[\"gender\"] = None\n",
    "                    raw_age = parts[3].strip()\n",
    "                    try:\n",
    "                        if raw_age:\n",
    "                            # extract digits before semicolon\n",
    "                            fixed_age = int(raw_age.split(\";\")[0])\n",
    "                            record[\"age\"] = fixed_age\n",
    "                    except:\n",
    "                        record[\"age\"] = None\n",
    "                    try:\n",
    "                        if len(parts) > 8 and parts[8].strip():\n",
    "                            # print(\"MMSE score found:\", parts[8].strip())\n",
    "                            record[\"mmse\"] = float(parts[8].strip())\n",
    "                    except:\n",
    "                        record[\"mmse\"] = None\n",
    "\n",
    "            # participant lines\n",
    "            if line.startswith(\"*PAR:\"):\n",
    "                utterance_count += 1\n",
    "                raw_text = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "                #  if participant has more than one line of text continue reading\n",
    "                i += 1\n",
    "                while i < len(lines):\n",
    "                    next_line = lines[i].strip()\n",
    "                    if next_line.startswith(\"*\") or next_line.startswith(\"@\") or next_line.startswith(\"%\"):\n",
    "                        break  # not a continuation\n",
    "                    raw_text += \" \" + next_line\n",
    "                    i += 1\n",
    "                #  count disfluency and pause features \n",
    "                for tag in [\"[//]\", \"[/]\", \"[/-]\", \"[*]\", \"[+...\", \"+//.\", \"[+\"]:\n",
    "                    disfluency_count += raw_text.count(tag)\n",
    "                pause_count += len(re.findall(r\"\\(\\.\\)\", raw_text))           # short pause\n",
    "                pause_count += len(re.findall(r\"\\(\\.\\.\\)\", raw_text))         # medium pause\n",
    "                pause_count += len(re.findall(r\"\\(\\.\\.\\.\\)\", raw_text))       # long pause\n",
    "                pause_count += len(re.findall(r\"\\(\\d+\\.\\d+\\)\", raw_text))     # timed pause\n",
    "\n",
    "                cleaned_text = clean_cha_line(raw_text, roberta=remove_duplicates)\n",
    "                if remove_duplicates: # for RoBERTa models\n",
    "                    cleaned_text = remove_duplicate_words(cleaned_text)\n",
    "                transcript.append(cleaned_text)\n",
    "\n",
    "                continue \n",
    "            i += 1\n",
    "\n",
    "    full_text = \" \".join(transcript)\n",
    "    record[\"transcription\"] = full_text\n",
    "    record[\"disfluencies\"] = disfluency_count\n",
    "    record[\"pause_count\"] = pause_count\n",
    "    record[\"utterance_count\"] = utterance_count\n",
    "    return record\n",
    "\n",
    "\n",
    "def collect_data_from_directory(\n",
    "    base_dir: str, \n",
    "    label: int | None, \n",
    "    dataset_tag: str,\n",
    "    remove_duplicates: bool = False\n",
    ") -> list[dict]:\n",
    "    all_records = []\n",
    "    cha_files = []\n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        cha_files.extend([\n",
    "            os.path.join(root, file)\n",
    "            for file in files if file.endswith(\".cha\")\n",
    "        ])\n",
    "\n",
    "    for file_path in tqdm(cha_files, desc=f\"Processing {dataset_tag}\"):\n",
    "        try:\n",
    "            data = extract_transcript_only(file_path, label=label, dataset=dataset_tag, remove_duplicates=remove_duplicates)\n",
    "            all_records.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "    return all_records\n",
    "\n",
    "\n",
    "# ADReSS datasets (RF = keep duplicates, RoBERTa = remove them)\n",
    "adress_train_cd = collect_data_from_directory(ADRESS_TRAIN_CD_PATH, 1, \"ADReSS-train\", remove_duplicates=False)\n",
    "adress_train_cc = collect_data_from_directory(ADRESS_TRAIN_CC_PATH, 0, \"ADReSS-train\", remove_duplicates=False)\n",
    "adress_train_cd_roberta = collect_data_from_directory(ADRESS_TRAIN_CD_PATH, 1, \"ADReSS-train\", remove_duplicates=True)\n",
    "adress_train_cc_roberta = collect_data_from_directory(ADRESS_TRAIN_CC_PATH, 0, \"ADReSS-train\", remove_duplicates=True)\n",
    "\n",
    "adress_test_rf = collect_data_from_directory(ADRESS_TEST_PATH, None, \"ADReSS-test\", remove_duplicates=False)\n",
    "adress_test_roberta = collect_data_from_directory(ADRESS_TEST_PATH, None, \"ADReSS-test\", remove_duplicates=True)\n",
    "\n",
    "# Pitt datasets (do the same)\n",
    "pitt_control_rf = collect_data_from_directory(PITT_CONTROL_PATH, 0, \"Pitt-Control\", remove_duplicates=False)\n",
    "pitt_dementia_rf = collect_data_from_directory(PITT_DEMENTIA_PATH, 1, \"Pitt-Dementia\", remove_duplicates=False)\n",
    "pitt_control_roberta = collect_data_from_directory(PITT_CONTROL_PATH, 0, \"Pitt-Control\", remove_duplicates=True)\n",
    "pitt_dementia_roberta = collect_data_from_directory(PITT_DEMENTIA_PATH, 1, \"Pitt-Dementia\", remove_duplicates=True)\n",
    "\n",
    "# combine and save datasets\n",
    "df_adress_train_rf = pd.DataFrame(adress_train_cd + adress_train_cc)\n",
    "df_adress_train_roberta = pd.DataFrame(adress_train_cd_roberta + adress_train_cc_roberta)\n",
    "\n",
    "df_adress_test = pd.DataFrame(adress_test_rf)\n",
    "df_adress_test_roberta = pd.DataFrame(adress_test_roberta)\n",
    "\n",
    "df_pitt_control = pd.DataFrame(pitt_control_rf)\n",
    "df_pitt_control_roberta = pd.DataFrame(pitt_control_roberta)\n",
    "\n",
    "df_pitt_ad = pd.DataFrame(pitt_dementia_rf)\n",
    "df_pitt_ad_roberta = pd.DataFrame(pitt_dementia_roberta)\n",
    "\n",
    "#  for RF\n",
    "df_adress_train_rf.to_csv(SAVE_PATHS[\"adress_train\"], sep=\"\\t\", index=False)\n",
    "df_adress_test.to_csv(SAVE_PATHS[\"adress_test\"], sep=\"\\t\", index=False)\n",
    "df_pitt_control.to_csv(SAVE_PATHS[\"pitt_control\"], sep=\"\\t\", index=False)\n",
    "df_pitt_ad.to_csv(SAVE_PATHS[\"pitt_ad\"], sep=\"\\t\", index=False)\n",
    "\n",
    "# for RoBERTa -> drop duplicates and the features (only needs raw text)\n",
    "df_adress_train_roberta.drop(columns=[\"disfluencies\", \"pause_count\", \"utterance_count\"], errors=\"ignore\")\\\n",
    "    .to_csv(from_root(\"data\", \"adress_train_roberta.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "df_adress_test_roberta.drop(columns=[\"disfluencies\", \"pause_count\", \"utterance_count\"], errors=\"ignore\")\\\n",
    "    .to_csv(from_root(\"data\", \"adress_test_roberta.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "df_pitt_control_roberta.drop(columns=[\"disfluencies\", \"pause_count\", \"utterance_count\"], errors=\"ignore\")\\\n",
    "    .to_csv(from_root(\"data\", \"pitt_control_roberta.tsv\"), sep=\"\\t\", index=False)\n",
    "\n",
    "df_pitt_ad_roberta.drop(columns=[\"disfluencies\", \"pause_count\", \"utterance_count\"], errors=\"ignore\")\\\n",
    "    .to_csv(from_root(\"data\", \"pitt_ad_roberta.tsv\"), sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c4f8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# NOTE:\n",
    "# The files `manual_age_fix_pitt_ad.tsv` and `manual_age_fix_pitt_control.tsv`\n",
    "# were manually created based on the demographic spreadsheet available from \n",
    "# the Pitt Corpus (TalkBank). These files contain corrected age values for \n",
    "# participants where the transcript data was missing.\n",
    "#\n",
    "# These files are not required to run the pipeline. You may skip this cell\n",
    "# if you do not have these files or prefer not to apply manual age corrections.\n",
    "# =============================================================================\n",
    "\n",
    "MANUAL_AGE_AD_PATH = from_root(\"data\", \"raw\", \"manual_age_fix_pitt_ad.tsv\")\n",
    "MANUAL_AGE_CONTROL_PATH = from_root(\"data\", \"raw\", \"manual_age_fix_pitt_control.tsv\")\n",
    "PITT_AD_SAVE_PATH = from_root(\"data\", \"pitt_ad.tsv\")\n",
    "PITT_CONTROL_SAVE_PATH = from_root(\"data\", \"pitt_control.tsv\")\n",
    "\n",
    "\n",
    "def apply_manual_age_fix(\n",
    "    df: pd.DataFrame, \n",
    "    manual_path: str, \n",
    "    save_path: str\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Changes age values in df with manually corrected ones from official spreadsheet.\n",
    "    \"\"\"\n",
    "    manual_ages = pd.read_csv(manual_path, sep=\"\\t\")\n",
    "    df = df.merge(manual_ages, on=\"ID\", how=\"left\", suffixes=(\"\", \"_manual\"))\n",
    "    df.loc[df[\"age_manual\"].notna(), \"age\"] = df[\"age_manual\"]\n",
    "    df.drop(columns=\"age_manual\", inplace=True)\n",
    "    df.to_csv(save_path, sep=\"\\t\", index=False)\n",
    "    return df\n",
    "\n",
    "df_pitt_ad = apply_manual_age_fix(df_pitt_ad, MANUAL_AGE_AD_PATH, PITT_AD_SAVE_PATH)\n",
    "df_pitt_control = apply_manual_age_fix(df_pitt_control, MANUAL_AGE_CONTROL_PATH, PITT_CONTROL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c79fc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IU_KEYWORDS = [\"cookie\", \"girl\", \"boy\", \"mother\", \"chair\", \"jar\", \"sink\", \"kitchen\",\n",
    "               \"water\", \"hand\", \"cupboard\", \"stool\", \"plate\", \"overflowing\"]\n",
    "\n",
    "FIRST_PERSON_PRONOUNS = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\"}\n",
    "THIRD_PERSON_PRONOUNS = {\"he\", \"him\", \"his\", \"she\", \"her\", \"hers\", \"they\", \"them\", \"their\", \"theirs\"}\n",
    "\n",
    "\n",
    "def type_token_ratio(tokens : List[str]) -> float:\n",
    "    # calculates the ratio of unique words to total words (lexical diversity)\n",
    "    types = set(tokens)\n",
    "    if tokens: \n",
    "        return len(types) / len(tokens)\n",
    "    else: \n",
    "        return 0\n",
    "\n",
    "\n",
    "def honore_statistic(tokens : List[str]) -> float:\n",
    "    # calculates lexical richness, giving more weight to words appearing once\n",
    "    N = len(tokens)\n",
    "    V = len(set(tokens))\n",
    "    if V == 0:\n",
    "        return 0\n",
    "    V1 = 0 # count of unique words (only appear once)\n",
    "    word_counts = Counter(tokens)\n",
    "    for word, freq in word_counts.items():\n",
    "        if freq == 1:\n",
    "            V1 += 1\n",
    "    if V > 0 and V1 < V:\n",
    "        return 100 * math.log(N) / (1 - V1 / V)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def idea_density(tokens : List[str]) -> float:\n",
    "    # estimates the density of content words (nouns, verbs, adjectives, adverbs)\n",
    "    # how much info is packed into the text -> proportion of content words to total tokens\n",
    "    content_pos = {\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"}\n",
    "    if not tokens:\n",
    "        return 0\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    count = 0\n",
    "    for token in doc:\n",
    "        if token.pos_ in content_pos:\n",
    "            count += 1\n",
    "    return count/len(tokens)\n",
    "\n",
    "\n",
    "def compression_ratio(text : str) -> float:\n",
    "    # measures text complexity by calculating the ratio of compressed to uncompressed text size\n",
    "    raw = text.encode(\"utf-8\")\n",
    "    compressed = zlib.compress(raw)\n",
    "    return len(compressed) / len(raw) if raw else 0\n",
    "\n",
    "\n",
    "def count_iu_keywords(tokens : List[str]) -> int:\n",
    "    # counts the number of IU keywords in the text\n",
    "    count = 0\n",
    "    for token in tokens:\n",
    "        if token.lower() in IU_KEYWORDS:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "def moving_average_ttr(tokens: List[str], window_size: int = 50) -> float:\n",
    "    # MATTR Calculation (Moving-Average TTR)\n",
    "    if len(tokens) < window_size:\n",
    "        return len(set(tokens)) / len(tokens) if tokens else 0\n",
    "    ttr_values = []\n",
    "    for i in range(len(tokens) - window_size + 1):\n",
    "        window = tokens[i:i + window_size]\n",
    "        ttr_values.append(len(set(window)) / window_size)\n",
    "    return np.mean(ttr_values)\n",
    "\n",
    "\n",
    "def count_cfg_rules(doc : Doc) -> Counter:\n",
    "    # count CFG Production Rules\n",
    "    production_counter = Counter()\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            rule = f\"{token.pos_} -> {' '.join(child.pos_ for child in token.children)}\"\n",
    "            production_counter[rule] += 1\n",
    "    return production_counter\n",
    "\n",
    "\n",
    "def compute_repetition_score(sentences: List[str]) -> float:\n",
    "    # measure Repetition via Cosine Distance\n",
    "    if len(sentences) < 2:\n",
    "        return 0\n",
    "    vectorizer = CountVectorizer().fit(sentences)\n",
    "    vectors = vectorizer.transform(sentences).toarray()\n",
    "    similarities = []\n",
    "    for i in range(len(vectors)):\n",
    "        for j in range(i+1, len(vectors)):\n",
    "            norm_i = np.linalg.norm(vectors[i])\n",
    "            norm_j = np.linalg.norm(vectors[j])\n",
    "            if norm_i != 0 and norm_j != 0:\n",
    "                sim = np.dot(vectors[i], vectors[j]) / (norm_i * norm_j)\n",
    "                similarities.append(sim)\n",
    "    if similarities:\n",
    "        return np.mean(similarities)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def brunet_index(tokens: List[str]) -> float:\n",
    "    # Brunet Index -> measures lexical richness\n",
    "    n = len(tokens)\n",
    "    v = len(set(tokens))\n",
    "    if n > 0:\n",
    "        return (n / v) * math.log(n / v)\n",
    "    return 0\n",
    "\n",
    "\n",
    "def named_entity_count(doc : Doc) -> int:\n",
    "    entity_count = 0\n",
    "    for ent in doc.ents: \n",
    "        entity_count += 1\n",
    "    return entity_count\n",
    "\n",
    "\n",
    "def noun_verb_ratio(noun_ratio: float, verb_ratio: float) -> float:\n",
    "    if verb_ratio != 0:\n",
    "        return noun_ratio / verb_ratio\n",
    "    return 0\n",
    "\n",
    "\n",
    "def get_tree_depth(token : Token) -> int:\n",
    "    children = list(token.children)\n",
    "    if not children:\n",
    "        return 1\n",
    "    depths = []\n",
    "    for child in children:\n",
    "        depths.append(get_tree_depth(child))\n",
    "    return 1 + max(depths) \n",
    "\n",
    "\n",
    "def extract_linguistic_features(row : pd.Series) -> pd.Series:\n",
    "    text = row[\"transcription\"]\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    # lemmatized tokens (for all lexical metrics, e.g., running, ran, run -> run)\n",
    "    for token in doc:\n",
    "        if token.is_alpha:\n",
    "            tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    mattr_value = moving_average_ttr(tokens)\n",
    "    cfg_counter = count_cfg_rules(doc)\n",
    "    repetition_score = compute_repetition_score([sent.text for sent in doc.sents])\n",
    "    brunet = brunet_index(tokens)\n",
    "    named_ents = named_entity_count(doc)\n",
    "\n",
    "    # POS tagging -> tags like grammatical roles (noun, verb, etc.) \n",
    "    pos_counts = Counter()\n",
    "    for token in doc:\n",
    "        pos_counts[token.pos_] += 1\n",
    "\n",
    "    num_words = len(tokens)  \n",
    "    sents = list(doc.sents) # sentence segmentation\n",
    "    num_sents = len(sents) # num of sentences\n",
    "    num_cius = count_iu_keywords(tokens) \n",
    "\n",
    "    # prononun counts\n",
    "    first_pron_count = 0\n",
    "    third_pron_count = 0 \n",
    "    stopword_count = 0\n",
    "    for t in tokens:\n",
    "        if t in FIRST_PERSON_PRONOUNS:\n",
    "            first_pron_count += 1\n",
    "        if t in THIRD_PERSON_PRONOUNS:\n",
    "            third_pron_count += 1\n",
    "        if t in STOP_WORDS:\n",
    "            stopword_count += 1\n",
    "    \n",
    "    # POS ratios\n",
    "    pronoun_ratio = pos_counts.get(\"PRON\", 0) / num_words if num_words else 0\n",
    "    first_person_ratio = first_pron_count / num_words if num_words else 0\n",
    "    third_person_ratio = third_pron_count / num_words if num_words else 0\n",
    "    stopword_ratio = stopword_count / num_words if num_words else 0\n",
    "    noun_ratio = pos_counts.get(\"NOUN\", 0) / num_words if num_words else 0\n",
    "    verb_ratio = pos_counts.get(\"VERB\", 0) / num_words if num_words else 0\n",
    "    prep_ratio = pos_counts.get(\"ADP\", 0) / num_words if num_words else 0\n",
    "    adv_ratio = pos_counts.get(\"ADV\", 0) / num_words if num_words else 0\n",
    "    auxiliary_ratio = pos_counts.get(\"AUX\", 0) / num_words if num_words else 0\n",
    "\n",
    "    noun_verb = noun_verb_ratio(noun_ratio, verb_ratio)\n",
    "    mean_sentence_length = num_words / num_sents if num_sents else 0\n",
    "\n",
    "    # parse tree depth\n",
    "    sentence_tree_depths = []\n",
    "    for sent in sents:\n",
    "        sentence_tree_depths.append(get_tree_depth(sent.root))\n",
    "    parse_tree_depth = max(sentence_tree_depths) if sentence_tree_depths else 0\n",
    "\n",
    "    # clause count\n",
    "    clause_labels = {\"ccomp\", \"advcl\", \"relcl\"}\n",
    "    clause_count = 0\n",
    "    for token in doc:\n",
    "        if token.dep_ in clause_labels:\n",
    "            clause_count += 1\n",
    "    clauses_per_sentence = clause_count / num_sents if num_sents else 0\n",
    "    \n",
    "    # content density --> proportion of content words (nouns, verbs, adjectives, adverbs) to total tokens\n",
    "    content_density_sum = 0\n",
    "    for tag in [\"NOUN\", \"VERB\", \"ADJ\", \"ADV\"]:\n",
    "        content_density_sum += pos_counts.get(tag, 0)\n",
    "    content_density = content_density_sum / num_words if num_words else 0\n",
    "\n",
    "    features = {\n",
    "        \"word_count\": num_words,\n",
    "        \"unique_words\": len(set(tokens)),\n",
    "        \"type_token_ratio\": type_token_ratio(tokens),\n",
    "        \"honore_statistic\": honore_statistic(tokens),\n",
    "        \"idea_density\": idea_density(tokens),\n",
    "        \"compression_ratio\": compression_ratio(text),\n",
    "        \"content_density\": content_density,\n",
    "        \"pronoun_ratio\": pronoun_ratio,\n",
    "        \"first_person_ratio\": first_person_ratio,\n",
    "        \"third_person_ratio\": third_person_ratio,\n",
    "        \"stopword_ratio\": stopword_ratio,\n",
    "        \"noun_ratio\": noun_ratio,\n",
    "        \"verb_ratio\": verb_ratio,\n",
    "        \"prep_ratio\": prep_ratio,\n",
    "        \"adv_ratio\": adv_ratio,\n",
    "        \"auxiliary_ratio\": auxiliary_ratio,\n",
    "        \"mean_sentence_length\": mean_sentence_length,\n",
    "        \"num_sentences\": num_sents,\n",
    "        \"IU_count\": num_cius,\n",
    "        \"IU_density\": num_cius / num_words if num_words else 0,\n",
    "        \"parse_tree_depth\": parse_tree_depth,\n",
    "        \"clauses_per_sentence\": clauses_per_sentence,\n",
    "        \"disfluency_count\": row.get(\"disfluencies\", 0),\n",
    "        \"pause_count\": row.get(\"pause_count\", 0),\n",
    "        \"utterance_count\": row.get(\"utterance_count\", 0),\n",
    "        \"mattr\": mattr_value,\n",
    "        \"repetition_score\": repetition_score,\n",
    "        \"cfg_np_to_nn\": cfg_counter.get(\"NOUN -> NOUN\", 0),\n",
    "        \"cfg_np_to_det_noun\": cfg_counter.get(\"NOUN -> DET NOUN\", 0),\n",
    "        \"brunet_index\": brunet,\n",
    "        \"named_entity_count\": named_ents,\n",
    "        \"noun_verb_ratio\": noun_verb,\n",
    "    }\n",
    "    return pd.Series(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5459c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Adress Train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d5ce177b748435c9179f42b27ec9db5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/108 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Adress Test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f11137134734700a9a1d97f79819d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/48 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Pitt Control\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ffc1af493664305a691480aaf38ac47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: Pitt Ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfcfc590df714b06a915902240d58905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/306 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "\n",
    "INPUT_FILES = {\n",
    "    \"adress_train\": from_root(\"data\", \"adress_train.tsv\"),\n",
    "    \"adress_test\": from_root(\"data\", \"adress_test.tsv\"),\n",
    "    \"pitt_control\": from_root(\"data\", \"pitt_control.tsv\"),\n",
    "    \"pitt_ad\": from_root(\"data\", \"pitt_ad.tsv\"),\n",
    "}\n",
    "\n",
    "def process_and_save(input_path: str, output_path: str) -> None:\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Input file not found: {input_path}\")\n",
    "        return \n",
    "    \n",
    "    df = pd.read_csv(input_path, sep=\"\\t\")\n",
    "    features_df = df.progress_apply(extract_linguistic_features, axis=1)\n",
    "    metadata_cols = [\"ID\", \"label\", \"gender\", \"age\"]\n",
    "    available_cols = []\n",
    "    for col in metadata_cols: \n",
    "        if col in df.columns: \n",
    "            available_cols.append(col)\n",
    "\n",
    "    final_df = pd.concat([df[available_cols], features_df], axis=1)\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)  \n",
    "    final_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "for name, input_path in INPUT_FILES.items():\n",
    "    print(f\"Processing dataset: {name.replace('_', ' ').title()}\")\n",
    "    filename = os.path.basename(input_path).replace(\".tsv\", \"_features.tsv\")\n",
    "    output_path = from_root(\"data\", \"features\", filename)\n",
    "    process_and_save(input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
